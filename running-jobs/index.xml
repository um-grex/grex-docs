<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Running jobs on Grex on Grex</title><link>https://um-grex.github.io/grex-doc/running-jobs/</link><description>Recent content in Running jobs on Grex on Grex</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>The MIT License (MIT) Copyright Â© 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-doc/running-jobs/index.xml" rel="self" type="application/rss+xml"/><item><title>Slurm partitions</title><link>https://um-grex.github.io/grex-doc/running-jobs/slurm-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-doc/running-jobs/slurm-partitions/</guid><description>Partitions# The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a &amp;ldquo;partition&amp;rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who &amp;ldquo;owns&amp;rdquo; the hardware as well.
There is no fully automatic selection of partitions, other than the default skylake for most of the users, and compute for the short jobs.</description></item><item><title>How to run interactive jobs on Grex?</title><link>https://um-grex.github.io/grex-doc/running-jobs/interactive-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-doc/running-jobs/interactive-jobs/</guid><description>Interactive work# The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:
edit files compile codes and run short interactive calculations. configure and build programs (limit the number of threads to 4: make -j4) submit and monitor jobs transfer and/or download data run short tests, &amp;hellip; etc. In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].</description></item><item><title>Running batch jobs on Grex</title><link>https://um-grex.github.io/grex-doc/running-jobs/batch-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-doc/running-jobs/batch-jobs/</guid><description>Batch jobs# HPC systems usually are clusters of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users&amp;rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called &amp;ldquo;jobs&amp;rdquo; to a &amp;ldquo;batch queue&amp;rdquo;.</description></item><item><title>Using local disks: $SLURM_TMPDIR</title><link>https://um-grex.github.io/grex-doc/running-jobs/using-localdisks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-doc/running-jobs/using-localdisks/</guid><description>Introduction#</description></item><item><title>Scheduling policies and running jobs on Contributed nodes</title><link>https://um-grex.github.io/grex-doc/running-jobs/contributed-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-doc/running-jobs/contributed-systems/</guid><description>Scheduling policies for contributed systems# Grex has a few user-contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the &amp;ldquo;preferred access&amp;rdquo; is preemption.
On the definition of preferential access to HPC systems# Preferential access is when you have non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have.</description></item></channel></rss>