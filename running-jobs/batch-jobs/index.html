<!doctype html><html><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo 0.136.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Everything you need to know for running batch jobs on Grex."><title>Running batch jobs on Grex | Grex</title>
<link rel=icon href=/grex-docs/logo/um_logo_vertical.png type=image/x-icon><link rel=stylesheet rel=preload type=text/css href=/grex-docs/css/main.css><link rel=stylesheet type=text/css href=/grex-docs/css/external/fontawesome.all.min.css><link rel=stylesheet type=text/css href></head><body onload=browserCompatibility()><div id=navbarInfo class="is-hidden modal-container"><div class="modal-content-box modal-text-box"><div class="card modal-title is-radiusless"><div class=card-header><p class="card-header-title is-marginless">About</p></div></div><div class=modal-content-container><div class="modal-content-block is-block"><div class='columns is-marginless is-multiline is-centered'><div class=column><a class=is-itext-link href=https://github.com/um-grex/grex-docs target=_blank><div><i class='fa-brands fa-github fa-2x'></i><p>GitHub repository</p></div></a></div><div class=column><a class=is-itext-link href=https://github.com/um-grex/grex-docs/archive/refs/heads/main.zip target=_blank><div><i class='fa-solid fa-download fa-2x'></i><p>Download</p></div></a></div><div class=column><a class=is-itext-link href=https://github.com/um-grex/grex-docs/issues target=_blank><div><i class='fa-solid fa-circle-info fa-2x'></i><p>Report an issue</p></div></a></div><div class=column><a class=is-itext-link href=https://github.com/um-grex/grex-docs/forks target=_blank><div><i class='fa-solid fa-code-branch fa-2x'></i><p>Fork</p></div></a></div></div><p>Website built with <a href=https://gohugo.io/ class=is-pretty-link>Hugo</a> (0.136.0)</p><p>The MIT License (MIT) Copyright © 2023 UM-Grex ()</p></div></div></div></div><div id=navbarShortcuts class="is-hidden modal-container"><div class="modal-content-box modal-text-box"><div class="card modal-title is-radiusless"><div class=card-header><p class="card-header-title is-marginless">Keyboard shortcuts</p></div></div><div class=modal-content-container><div class=modal-content-block><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>q</kbd></div><div class=modal-content><p class=has-text-weight-normal>Show current page QR code</p></div><div class=modal-content><kbd>Escape</kbd></div><div class=modal-content><p class=has-text-weight-normal>Close modals</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>i</kbd></div><div class=modal-content><p class=has-text-weight-normal>Show website information</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>k</kbd></div><div class=modal-content><p class=has-text-weight-normal>Show shortcuts</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>h</kbd></div><div class=modal-content><p class=has-text-weight-normal>Go to homepage</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>f</kbd></div><div class=modal-content><p class=has-text-weight-normal>Find on website</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>m</kbd></div><div class=modal-content><p class=has-text-weight-normal>Collapse/Uncollapse sidebar</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>t</kbd></div><div class=modal-content><p class=has-text-weight-normal>Collapse/Uncollapse table of contents</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>↑</kbd></div><div class=modal-content><p class=has-text-weight-normal>Go to the top of the page</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>↓</kbd></div><div class=modal-content><p class=has-text-weight-normal>Go to the bottom of the page</p></div><div class=modal-content><kbd>Shift</kbd><p>+</p><kbd>p</kbd></div><div class=modal-content><p class=has-text-weight-normal>Print current page</p></div></div></div></div></div><div id=modalContainer class="is-hidden modal-container"><div id=modal class=modal-content-box></div></div><nav id=navbar class=navbar role=navigation aria-label="main navigation"><div id=navbarItemsContainer class=navbar-brand><div id=globalLogoContainer class=is-flex><a id=globalLogo class="navbar-item navbar-item-standard is-paddingless" href=/grex-docs/><img alt=Homepage src=/grex-docs/logo/um_logo_email_signature.png>
</a><a id=globalTouchLogo class="navbar-item navbar-item-standard is-paddingless" href=/grex-docs/><img alt=Homepage src=/grex-docs/logo/um_logo_email_signature.png></a></div><div id=navbarItems class=is-flex><div id=navbarItemsStart class=is-flex><div id=searchContainer class=navbar-item><div class="control has-icons-left"><span id=searchInput class=autocomplete><input id=search autocomplete=off class=input type=search placeholder=Search>
</span><span class="icon is-left"><i class="fa-solid fa-magnifying-glass"></i></span><ul id=searchList class="is-paddingless is-hidden"></ul></div></div></div><div id=navbarItemsEnd class="is-flex is-invisible"><div class=navbar-item><a id=printButton class="button navbar-item-standard"><span class=icon-text>Print</span>
<span class=icon><i class="fa-solid fa-print fa-lg"></i></span></a></div><div class=navbar-item><a id=qrCodeButton class="button navbar-item-standard"><span class=icon-text>QR code</span>
<span class=icon><i class="fa-solid fa-qrcode fa-lg"></i></span></a></div><div class=navbar-item><a id=shortcutsInfo trigger=navbarShortcuts class="button navbar-item-standard navbar-trigger" title="Keyboard shortcuts"><span class=icon-text>Shortcuts</span>
<span class=icon><i class="fa-solid fa-keyboard fa-lg"></i></span></a></div><div class=navbar-item><div id=taxonomiesSelectorContainer class="dropdown is-right"><div class=dropdown-trigger><button id=taxonomiesSelector class="button navbar-item-standard" aria-haspopup=true aria-controls=dropdown-menu>
<span class=icon-text>Taxonomies</span>
<span class=icon><i class="fa-solid fa-tags fa-lg"></i></span></button></div><div id=dropdown-menu-taxonomies class=dropdown-menu role=menu><div class="dropdown-content is-paddingless"><a href=/grex-docs/categories/ class="dropdown-item has-text-weight-bold">Categories</a></div></div></div></div><div class=navbar-item><a id=siteInfo trigger=navbarInfo class="button navbar-item-standard navbar-trigger" title=About><span class=icon-text>About</span>
<span class=icon><i class="fa-solid fa-circle-question fa-lg"></i></span></a></div><div id=navbarExtend class=navbar-item><div id=navbarExtendWrapper><a id=navbarExtendButton class="button navbar-item-standard"><span class=icon><i class="fa-solid fa-ellipsis fa-lg"></i></span></a><div id=navbarExtendItemsContainer><div id=navbarExtendItemsWrapper><div class=navbar-item><a id=printButtonExtend class="button navbar-item-standard"><span class=icon-text>Print</span>
<span class=icon><i class="fa-solid fa-print fa-lg"></i></span></a></div><div class=navbar-item><a id=qrCodeButtonExtend class="button navbar-item-standard"><span class=icon-text>QR code</span>
<span class=icon><i class="fa-solid fa-qrcode fa-lg"></i></span></a></div><div class=navbar-item><a id=shortcutsInfoExtend trigger=navbarShortcuts class="button navbar-item-standard navbar-trigger" title="Keyboard shortcuts"><span class=icon-text>Shortcuts</span>
<span class=icon><i class="fa-solid fa-keyboard fa-lg"></i></span></a></div><div class=navbar-item><div id=taxonomiesSelectorContainerExtend class="dropdown is-right"><div class=dropdown-trigger><button id=taxonomiesSelectorExtend class="button navbar-item-standard" aria-haspopup=true aria-controls=dropdown-menu>
<span class=icon-text>Taxonomies</span>
<span class=icon><i class="fa-solid fa-tags fa-lg"></i></span></button></div><div id=dropdown-menu-taxonomiesExtend class=dropdown-menu role=menu><div class="dropdown-content is-paddingless"><a href=/grex-docs/categories/ class="dropdown-item has-text-weight-bold">Categories</a></div></div></div></div><div class=navbar-item><a id=siteInfoExtend trigger=navbarInfo class="button navbar-item-standard navbar-trigger" title=About><span class=icon-text>About</span>
<span class=icon><i class="fa-solid fa-circle-question fa-lg"></i></span></a></div></div></div></div></div></div></div></div></nav><div id=navbarOverlay></div><div class="columns is-mobile is-paddingless is-marginless" id=mainContainer><div id=sidebarContainer class="column is-narrow is-paddingless is-marginless"><div id=sidebarWrapper class=is-fixed><div id=sidebar class=is-marginless><aside class="menu is-paddingless is-marginless"><ul class="menu-list is-marginless is-paddingless"><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/updates/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/updates/ class=card-header-title title='Grex Updates'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Grex Updates</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/grex/ class=card-header-title><i class='fa-solid fa-user-gear fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/grex/ class=card-header-title title=Home><i class='fa-solid fa-user-gear fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Home</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/start-guide/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/start-guide/ class=card-header-title title='Quick Start Guide'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Quick Start Guide</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/access/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/access/ class=card-header-title title='Access and usage conditions'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Access and usage conditions</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/connecting/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/connecting/ class=card-header-title title='Connect / Transfer data'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Connect / Transfer data</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/connecting/mfa/ class=card-header-title title='Multi-Factor Authentication'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Multi-Factor Authentication</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/connecting/ssh/ class=card-header-title title='Connecting with SSH'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Connecting with SSH</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/connecting/ood/ class=card-header-title title='Connect with OOD'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Connect with OOD</p></a></div></div></li></div></div><div><div><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/connecting/data-transfer/ class=card-header-title title='Data transfer'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Data transfer</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/connecting/data-transfer/one-drive/ class=card-header-title title='Rclone and OneDrive'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Rclone and OneDrive</p></a></div></div></li></div></div></ul></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/storage/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/storage/ class=card-header-title title='Storage and Data'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Storage and Data</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/storage/data-sizes-and-quota/ class=card-header-title title='Data sizes and Quotas'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Data sizes and Quotas</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/storage/data-backup/ class=card-header-title title='Data Backup'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Data Backup</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/storage/data-sharing/ class=card-header-title title='Data sharing'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Data sharing</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon is-sidebar-active"><div class=card-header><a href=/grex-docs/running-jobs/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-tree-active is-entries-expandable is-entries-expanded"><div class=card><div class=card-header><a href=/grex-docs/running-jobs/ class=card-header-title title='Running jobs'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Running jobs</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-expanded"><span class=icon></span></a></div></div><ul class="is-marginless is-tree-active"><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/running-jobs/slurm-partitions/ class=card-header-title title='Slurm partitions'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Slurm partitions</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/running-jobs/interactive-jobs/ class=card-header-title title='Interactive jobs'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Interactive jobs</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class="card is-sidebar-active" id=sidebarActiveEntry><div class="card-header is-single-link"><a href=/grex-docs/running-jobs/batch-jobs/ class=card-header-title title='Batch jobs'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Batch jobs</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/running-jobs/using-localdisks/ class=card-header-title title='Using local disks'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Using local disks</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/running-jobs/contributed-systems/ class=card-header-title title='Contributed nodes'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Contributed nodes</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/software/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/software/ class=card-header-title title='Software / Applications'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Software / Applications</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/software/general-linux/ class=card-header-title title='Linux tools'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Linux tools</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/software/using-modules/ class=card-header-title title='Using modules'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Using modules</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/software/code-development/ class=card-header-title title='Code development'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Code development</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/software/containers/ class=card-header-title title=Containers><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Containers</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/software/cern-vmfs/ class=card-header-title title=CVMFS><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">CVMFS</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/software/jupyter-notebook/ class=card-header-title title='Jupyter notebooks'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Jupyter notebooks</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/specific-soft/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/specific-soft/ class=card-header-title title='Software Specific Notes'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Software Specific Notes</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/gaussian/ class=card-header-title title=Gaussian><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Gaussian</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/julia/ class=card-header-title title=Julia><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Julia</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/lammps/ class=card-header-title title=LAMMPS><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">LAMMPS</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/matlab/ class=card-header-title title=MATLAB><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">MATLAB</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/nwchem/ class=card-header-title title=NWChem><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">NWChem</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/orca/ class=card-header-title title=ORCA><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">ORCA</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/vasp/ class=card-header-title title=VASP><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">VASP</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/specific-soft/python-ai/ class=card-header-title title='Python for ML'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Python for ML</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/ood/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/ood/ class=card-header-title title=OpenOnDemand><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">OpenOnDemand</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/changes/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/changes/ class=card-header-title title='Grex changes'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Grex changes</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/changes/linux-slurm-update/ class=card-header-title title='Dec 10-11, 2019'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Dec 10-11, 2019</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/changes/changes-before-2020/ class=card-header-title title='Before Dec, 2019'><i class='fa-solid fa-turn-up fa-rotate-90 fa-xs'></i><p class="pt-0 pb-0 pr-2 pl-0">Before Dec, 2019</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/support/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/support/ class=card-header-title title='Getting Help'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Getting Help</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/training/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/training/ class=card-header-title title=Workshops><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Workshops</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/training/workshops-2023/ class=card-header-title title=2023><i class='fa-solid fa-cubes'></i><p class="pt-0 pb-0 pr-2 pl-0">2023</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/training/workshops-2022/ class=card-header-title title=2022><i class='fa-solid fa-cubes'></i><p class="pt-0 pb-0 pr-2 pl-0">2022</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/training/workshops-2021/ class=card-header-title title=2021><i class='fa-solid fa-cubes'></i><p class="pt-0 pb-0 pr-2 pl-0">2021</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class=card-header><a href=/grex-docs/friends/ class=card-header-title><i class='fa-solid fa-house-chimney fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class="is-marginless is-entries-expandable is-entries-shrinked"><div class=card><div class=card-header><a href=/grex-docs/friends/ class=card-header-title title='Friendly Organizations'><i class='fa-solid fa-house-chimney fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Friendly Organizations</p></a><a class="card-header-icon is-icon-expandable is-paddingless is-icon-shrinked"><span class=icon></span></a></div></div><ul class=is-marginless><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/friends/alliancecan/ class=card-header-title title='DRAC (Alliance)'><i class='fa-solid fa-house-chimney'></i><p class="pt-0 pb-0 pr-2 pl-0">DRAC (Alliance)</p></a></div></div></li></div></div><div><div><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/friends/localit/ class=card-header-title title='UManitoba IT resources'><i class='fa-solid fa-house-chimney'></i><p class="pt-0 pb-0 pr-2 pl-0">UManitoba IT resources</p></a></div></div></li></div></div></ul></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/faq/ class=card-header-title><i class='fa-solid fa-circle-question fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/faq/ class=card-header-title title=FAQ><i class='fa-solid fa-circle-question fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">FAQ</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/deprecated/ class=card-header-title><i class='fa-solid fa-ban fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/deprecated/ class=card-header-title title='Deprecated pages'><i class='fa-solid fa-ban fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Deprecated pages</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/disclaimer/ class=card-header-title><i class='fa-solid fa-ban fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/disclaimer/ class=card-header-title title=Disclaimer><i class='fa-solid fa-ban fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Disclaimer</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/glossary/ class=card-header-title><i class='fa-solid fa-sitemap fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/glossary/ class=card-header-title title=Glossary><i class='fa-solid fa-sitemap fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Glossary</p></a></div></div></li></div></div><div class="card is-fs-expandable-icon"><div class="card-header is-single-link"><a href=/grex-docs/sitemap/ class=card-header-title><i class='fa-solid fa-sitemap fa-lg'></i></a></div></div><div class=is-expandable><div class=is-sidebar-list-wrapper><li class=is-marginless><div class=card><div class="card-header is-single-link"><a href=/grex-docs/sitemap/ class=card-header-title title=Sitemap><i class='fa-solid fa-sitemap fa-lg'></i><p class="pt-0 pb-0 pr-2 pl-1">Sitemap</p></a></div></div></li></div></div></ul></aside></div><div id=sidebarCollapsible class=is-marginless><div class="card is-uncollapse-action"><div id=sidebarUncollapse class="card-header is-single-link" title="Collapse/Uncollapse sidebar"><a class=card-header-title><i class="fa-solid fa-angles-right fa-lg"></i></a></div></div><div class="card is-collapse-action"><div id=sidebarCollapse class="card-header is-single-link" title="Collapse/Uncollapse sidebar"><a class=card-header-title><i class="fa-solid fa-angles-left fa-lg"></i><p>Collapse sidebar</p></a></div></div></div></div></div><div id=sidebarMobileWrapper class="column is-narrow is-hidden-desktop"></div><div class="columns is-mobile is-marginless is-scroll-smooth has-toc" id=contentContainer><div class=column id=content><div id=contentTitle>Running batch jobs on Grex</div><h2 id=batch-jobs>Batch jobs<a href=#batch-jobs class=anchor>#</a></h2><hr><p>HPC systems usually are <strong>clusters</strong> of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users&rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called &ldquo;jobs&rdquo; to a &ldquo;batch queue&rdquo;. A subset of the available resources of the HPC machine is allocated to each of the users&rsquo; batch jobs, and they run without any need for user intervention as soon as the resources become available.</p><p>The job placement, usage monitoring and job accounting are done via a special software, the HPC scheduler. This is an often-under-appreciated automation that makes usage efficient and saves a lot of work on part of the user. However, using HPC is hard in a sense that users have to make an effort in order to figure out what are the available resources on an HPC cluster, and what is the efficient way of requesting the resources for their jobs. Asking for too many resources might be wasteful both in preventing others from using them and in making for a longer queuing time.</p><p>The resources (&ldquo;tractable resources&rdquo; in SLURM speak) are CPU time, memory, and GPU time. Generic resources can be software licenses, &mldr; etc. Requesting resources is done via command line options to job submission commands <strong>sbatch</strong> and <strong>salloc</strong>, or via special comment lines or SLURM directives (starting with #SBATCH) in job scripts. There are also options to control job placement such as partitions.</p><p>There are default values for the resources which are taken when you do not specify the resource limit. Note that the default values are, as a rule, quite small. On Grex, the default values are set as follow: <strong>3 hours</strong> of wall time, <strong>256mb</strong> of memory per CPU. In most of the cases, it is better to have an explicit request of an appropriate resource limit rather than using the default.</p><blockquote><p>We ask our users to be fair and considerate and do not allow for deliberate waste of resources (such as running serial jobs on more than one CPU core, or running CPU-only calculations on GPU nodes).</p></blockquote><p>There are certain scheduling policies in place to prevent the cluster from being swamped by a single user. In particular, the MAXPS / GrpRunMins limit disfavors asking for many CPU cores for long wall time, a MaxCPU limits restricts number of CPU cores used, and there are limits on number of user&rsquo;s jobs in the system and number of array job elements, as described below.</p><h2 id=scheduling-policies>Scheduling policies<a href=#scheduling-policies class=anchor>#</a></h2><hr><p>The following policies are implemented on Grex:</p><blockquote><ul><li>The default wall time is 3 hours (equivalent to: <strong>--time=3:00:00</strong> or <strong>--time=0-3:00:00</strong>).</li><li>The default amount of memory per processor (<strong>--mem-per-cpu=</strong>) is 256 mb. Memory limits are enforced, so an accurate estimate of memory resource (either in the form of <strong>--mem=</strong> or <strong>--mem-per-cpu=</strong>) should be provided.</li><li>The maximum wall time is 21 days on <strong>genoa</strong> and <strong>skylake</strong> partitions, 14 days on <strong>largemem</strong> and <strong>genlm</strong> partition.</li><li>The maximum wall time is 3 days on the <strong>gpu</strong> partition.</li><li>The maximum wall time is 7 days on the <strong>preempted</strong> partitions: <strong>stamps-b</strong>, <strong>livi-b</strong> and <strong>agro-b</strong>.</li><li>The maximum number of processor-minutes for all currently running jobs of a group without a RAC is 4 M.</li><li>The maximum number of jobs that a user may have queued to run is 4000. The maximum size of an array job is 2000.</li><li>Users without a RAC award are allowed to simultaneously use up to 400 CPU cores per accounting group.</li><li>There are limits on the number of GPUs that can be used on contributed hardware (1 GPU per job).</li></ul></blockquote><p>Note that you can see some information about the partitions by running the custom script <strong>partition-list</strong> from your terminal:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>partition-list</span></span></code></pre></div><h2 id=typical-batch-job-cases>Typical batch job cases<a href=#typical-batch-job-cases class=anchor>#</a></h2><hr><p>Any batch job is submitted with <strong>sbatch</strong> command. Batch jobs are usually shell (BASH, etc.) scripts wrapping around the invocation of a code. The comments on top of the script that start with <strong>#SBATCH</strong> are interpreted by the SLURM scheduler as options for resource requests:</p><table><thead><tr><th style=text-align:center>Directive</th><th style=text-align:center>Example</th><th style=text-align:center>Description</th></tr></thead><tbody><tr><td style=text-align:center><strong>--ntasks=</strong></td><td style=text-align:center>--ntasks=4</td><td style=text-align:center>Number of tasks (MPI processes) per job.</td></tr><tr><td style=text-align:center><strong>--nodes=</strong></td><td style=text-align:center>--nodes=2</td><td style=text-align:center>Number of nodes (servers) per job.</td></tr><tr><td style=text-align:center><strong>--ntasks-per-node=</strong></td><td style=text-align:center>--ntasks-per-node=4</td><td style=text-align:center>Number of tasks (MPI processes) per node.</td></tr><tr><td style=text-align:center><strong>--cpus-per-task=</strong></td><td style=text-align:center>--cpus-per-task=8</td><td style=text-align:center>Number of threads per task (should not exceed the number of physical cores.</td></tr><tr><td style=text-align:center><strong>--mem-per-cpu=</strong></td><td style=text-align:center>--mem-per-cpu=1500M</td><td style=text-align:center>Memory per task (or thread).</td></tr><tr><td style=text-align:center><strong>--mem=</strong></td><td style=text-align:center>--mem=16000M</td><td style=text-align:center>Memory per node.</td></tr><tr><td style=text-align:center><strong>--gpus=</strong></td><td style=text-align:center>--gpus=1</td><td style=text-align:center>Number of GPUs per job.</td></tr><tr><td style=text-align:center><strong>--time-</strong></td><td style=text-align:center>--time=0-8:00:00</td><td style=text-align:center>wall time in format DD-HH:MM:SS</td></tr><tr><td style=text-align:center><strong>--qos=</strong></td><td style=text-align:center>*</td><td style=text-align:center>QOS by name (Not to be used on Grex!).</td></tr><tr><td style=text-align:center><strong>--partition=</strong></td><td style=text-align:center>--partition=skylake</td><td style=text-align:center>Partition name: <strong>skylake</strong>, &mldr; etc (<strong>very much used on Grex!</strong>).</td></tr></tbody></table><p>Assuming the name of <strong>myfile.slurm</strong> (the name or the extension does not matter, it can be called <em>afile.job</em>, <em>otherjob.sh</em>, &mldr; etc.), a job is submitted with the command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sbatch myfile.slurm</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sbatch <span style=color:#f92672>[</span>+some options<span style=color:#f92672>]</span> myfile.slurm</span></span></code></pre></div><p>Some options like <strong>--partition=skylake</strong> could be invoked at submission time.</p><p>Refer to the official SLURM <a href=https://slurm.schedmd.com/documentation.html class=is-pretty-link>documentation</a>
and/or <strong>man sbatch</strong> for the available options. Below we provide examples for typical cases of SLURM jobs.</p><h2 id=serial-jobs>Serial jobs<a href=#serial-jobs class=anchor>#</a></h2><hr><p>The simplest kind of job is a serial job when one compute process runs in a sequential fashion. Naturally, such job can utilize only a single CPU core: even large parallel supercomputers as a rule do not parallelize binary codes automatically. So, the CPU request for a serial job is always 1, which is the default; the other resources can be wall time and memory. SLURM has two ways of specifying the memory: memory per core (<strong>--mem-per-cpu=</strong>) and total memory per node (<strong>--mem=</strong>). It is more logical to use per-core memory always; except in case of the whole-node jobs when special value <strong>--mem=0</strong> gives all the available memory for the allocated node. An example script (for 1 CPU, wall time of 30 minutes and a memory of 2500M) is provided below.</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for serial job</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-serial-job-template.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=0-0:30:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem=2500M</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;Serial-Job-Test&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Script for running serial program: your_program</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Current working directory is `pwd`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load modules if needed:</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>./your_program &lt;+options or arguments <span style=color:#66d9ef>if</span> any&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><p>An important special case of serial jobs is high-throughput computing: jobs are serial because they are too short to parallelize them, however there are very many such jobs per research project. The case of embarrassingly parallel computations like some of the Monte Carlo simulations are often High Throughput Computing (HTC).</p><blockquote><ul><li>Serial jobs that have regularly named inputs and run more than a few minutes each best be specified as a Job array (see below).</li><li>Serial jobs that are great in numbers, and run less than a few minutes each, better be joined into a task farm running within a single larger job using tools like GLOST, GNU Parallel or a workflow engine like QDO.</li></ul></blockquote><p>An example of <a href=https://docs.alliancecan.ca/wiki/GLOST class=is-pretty-link>GLOST</a>
job is under the MPI jobs section (see below).</p><h2 id=smp--threaded--single-node-jobs>SMP / threaded / single node jobs<a href=#smp--threaded--single-node-jobs class=anchor>#</a></h2><hr><p>The next kind of job is multi-threaded, shared memory or single-node parallel jobs. Often these jobs are for Symmetric Multiprocessing (SMP) codes that can use more than one CPU on a given node to speed up the calculations. However, SMP/multithreaded jobs rely on some form of inter-process communication (shared memory, &mldr; etc.) that limits them to the CPU cores within just a single server. They cannot scale across multiple compute nodes. Examples are OpenMP, pthreads, Java codes, etc. Gaussian and PSI4 are SMP codes; threaded BLAS/LAPACK routines from MKL (inside NumPY) can utilize multiple threads, &mldr; etc. Note that this kind of programs do not scale very well when increasing the number of threads. We recommend to our users to run a benchmark to see how their programs <a href=https://docs.alliancecan.ca/wiki/Scalability/en class=is-pretty-link>scale</a>
with the number of threads to define the combination or a set of threads for better performance.</p><p>Thus, from the point of view of the SMP/threaded jobs resources request, the following considerations are important:</p><blockquote><ul><li>asking always only a single compute node and one task (<strong>--nodes=1 --ntasks=1</strong>) job.</li><li>asking for several CPU cores on it per job, up to the maximum number of CPU cores per node (<strong>--cpus-per-task=N</strong>) where N should not exceed the total physical cores available on the node. Depending on the partition, you may choose N up to 52 on the <strong>skylake</strong> partition, up to 40 on the <strong>largemem</strong> partition, &mldr; etc.</li><li>making sure that the total memory asked for does not exceed the memory available on the node (refer to the section about node characteristics, <a href=/grex-docs/grex/#hardware class=is-pretty-link>hardware</a>
for more information).</li><li>making sure that the code would use exactly the number of CPU cores allocated to the job, to prevent waste or congestion of the resources.</li></ul></blockquote><p>In SLURM, it makes a difference whether you ask for <strong>parallel tasks (--ntasks)</strong> or <strong>threads (--cpus-per-task)</strong> ; the threads should not be isolated from each other (because they might need to use shared memory!) but the tasks are isolated to each own &ldquo;cgroup&rdquo;.</p><p>An environment variable <strong>${SLURM_CPUS_PER_TASK}</strong> is set in the job, so you can set an appropriate parameter of your code to the same value.</p><p>For OpenMP, it would be done like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export OMP_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>SLURM_CPUS_PER_TASK<span style=color:#e6db74>}</span></span></span></code></pre></div><p>For MKL it is <strong>MKL_NUM_THREADS</strong>, for Julia <strong>--JULIA_NUM_THREADS</strong>, for Java <strong>-Xfixme</strong> parameter.</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running a job on **skylake** partition: using full node</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-smp-job-node-template.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=0-8:00:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --nodes=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --ntask-per-node=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --cpus-per-task=52</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem=0</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --partition=skylake</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;OMP-Job-Test&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># An example of an OpenMP threaded job that </span>
</span></span><span style=display:flex><span><span style=color:#75715e># takes a whole &#34;skylake&#34; node for 8 hours. </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export OMP_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>SLURM_CPUS_PER_TASK<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>./your-openmp.x input.dat &gt; output.log
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><p>Note that the above example requests the whole node&rsquo;s memory with <strong>--mem=0</strong> because the node is allocated to the job fully due to all the CPUs anyways. It is easier to use the <strong>--mem</strong> syntax for SMP jobs because typically the memory is shared between threads (i.e., the amount of memory used does not change with the number of SMP threads). Note, however, that the memory request should be reasonably &ldquo;efficient&rdquo; if possible.</p><p>It is also possible to use a fraction of the node for running OpenMP jobs. Here is an example asking for 1 task with 4 threads on compute partition:</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running a job on **skylake** partition: using a fraction of the node</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-smp-job-partial-node-template.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=0-8:00:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --nodes=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --ntask-per-node=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --cpus-per-task=4</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem=8000M</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --partition=skylake</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;OMP-Job-Test&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># An example of an OpenMP threaded job that </span>
</span></span><span style=display:flex><span><span style=color:#75715e># takes 4 threads for 8 hours. </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export OMP_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>SLURM_CPUS_PER_TASK<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>./your-openmp.x input.dat &gt; output.log
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><h2 id=gpu-jobs>GPU jobs<a href=#gpu-jobs class=anchor>#</a></h2><hr><p>The GPU jobs would usually be similar to SMP/threaded jobs, with the following differences:</p><ul><li>The GPU jobs should run on the nodes that have GPU hardware, which means you&rsquo;d want always to specify one of the following options:</li></ul><table><thead><tr><th>Directive</th><th>Description</th></tr></thead><tbody><tr><td><strong>--partition=gpu</strong></td><td>to use the <strong>gpu</strong> partition.</td></tr><tr><td><strong>--partition=stamps-b</strong></td><td>to use the <strong>stamps-b</strong> partition.</td></tr><tr><td><strong>--partition=livi-b</strong></td><td>to use the <strong>livi-b</strong> partition.</td></tr><tr><td><strong>--partition=agro-b</strong></td><td>to use the <strong>agro-b</strong> partition.</td></tr></tbody></table><ul><li>SLURM on Grex uses the so-called &ldquo;GTRES&rdquo; plugin for scheduling GPU jobs, which means that the request syntax in the form <strong>--gpus=N</strong> or <strong>--gpus-per-node=N</strong> or <strong>--gpus-per-task=N</strong> is used.</li></ul><h3 id=how-many-gpus-to-ask-for>How many GPUs to ask for?<a href=#how-many-gpus-to-ask-for class=anchor>#</a></h3><hr><p>Grex, at the moment, does not have GPU-direct MPI enabled, which means that most of the jobs would be single-node. The GPU nodes in either <strong>gpu</strong> (two nodes, 32GB V100s) or <strong>stamps-b</strong> (three nodes, 16GB V100s) partition have 4 V100 GPUs, 32 Intel 52xx CPUs and 192GB of CPU memory. There is also the <strong>livi-b</strong> partition with a large single 16x v100 GPU server. So, asking 1 to 4 GPUs, one node, and 6-8 CPUs per GPU with an appropriate amount of RAM (4-8 Gb) per job would be a good starting point.</p><p>Note that V100 is a fairly large GPU for most of the jobs, and for good utilization of the GPU resources available on Grex, it is a good idea to start with a single GPU, and then try if the code actually is able to saturate it with load. Many codes cannot scale to utilize more than one GPU, and few codes can utilize more than two of them.</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running ont-guppy on GPU</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-guppy-gpu.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --gpus=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --partition=stamps-b</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --ntasks=1 </span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --cpus-per-task=6</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem-per-cpu=6000M</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=0-12:00:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=genomics-test</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Adjust the resource requests above to your needs.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example of loading modules, CUDA:</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module load cuda/12.4.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export OMP_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>SLURM_CPUS_PER_TASK<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nvidia-smi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>guppy_basecaller -x auto --gpu_runners_per_device <span style=color:#ae81ff>6</span> -i Fast5 -s GuppyFast5 -c dna_r9.4.1_450bps_hac.cfg
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><p>The above script (if called say, gpu.job) can be submitted with the usual command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sbatch gpu.job</span></span></code></pre></div><h2 id=distributed-massively-parallel-jobs>Distributed, massively parallel jobs<a href=#distributed-massively-parallel-jobs class=anchor>#</a></h2><hr><p>Parallel jobs that can spawn multiple servers are the most scalable ones, because they are not limited by the number of CPUs or memory per node. Running many parallel tasks across more than one node requires some inter-node communication (which as a rule is slower than shared memory within one server). In HPC, high speed interconnect and specialized RDMA-aware communication libraries make distributed parallel computation very scalable. Grex uses InfiniBand interconnect (IB).</p><p>Most often (but not always), parallel programs are built upon a low-level message passing library called MPI. Refer to the Software section for more information about parallel libraries on Grex. Examples of distributed parallel codes are GAMESS-US, <a href=/grex-docs/specific-soft/orca/ class=is-pretty-link>ORCA</a>
, <a href=/grex-docs/specific-soft/lammps/ class=is-pretty-link>LAMMPS</a>
, <a href=/grex-docs/specific-soft/vasp/ class=is-pretty-link>VASP</a>
, &mldr; etc.</p><p>The distributed parallel jobs can be placed across their compute nodes in several ways (i.e., how many parallel tasks per compute node?). Thus, SLURM resource request syntax allows to specify the required layout of nodes/tasks (or nodes/tasks/threads, or even nodes/tasks/GPUs since hybrid MPI+OpenMP and MPI+GPU programs exist). A consideration about the layout is a tradeoff between making the program work faster (and sometimes to work correctly at all) and making the scheduler&rsquo;s work easier.</p><p>A well written MPI software theoretically should not care how the tasks are distributed across how many physical compute nodes. Thus, SLURM&rsquo;s <strong>--ntasks=</strong> request (similar to the old Torque <strong>procs=</strong>) specified without <strong>--nodes</strong> would work and make the scheduling easier.</p><p><strong>A note on process starting:</strong></p><p>Since MPI jobs are distributed, there should be a mechanism to start the compute processes across all of the nodes (or CPUs) allocated for it. The mechanism should know which nodes to use, and how many. Most modern MPI implementations &ldquo;tightly integrate&rdquo; with SLURM, so they will get this information automatically via a Process Management Interface (PMI). SLURM provides its own job starting command called <strong><a href=https://slurm.schedmd.com/srun.html class=is-pretty-link>srun</a>
</strong>. Most MPI implementations also provide their own job spawned commands, usually called <strong>mpiexec</strong> or <strong>mpirun</strong>. These are specific to each MPI vendor/kind and not well standardized, and differ in the support of SLURM.</p><p>For example, OpenMPI (the default, supported MPI implementation) on Grex is compiled against PMIx (3.x, 4.x) or PMI1 (1.6.5). So, it is preferable to use <strong>srun</strong> instead of <strong>mpiexec</strong> to kick start the MPI processes, because <strong>srun</strong> would use PMI.</p><p>For Intel MPI (another MPI, also available on Grex and required by some of the binary codes, like ANSYS or ADF), <strong>srun</strong> sometimes may not work, but PMI1 can be used with <strong>mpiexec.hydra</strong> by setting the following environment variable:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export I_PMI_LIBRARY<span style=color:#f92672>=</span>/opt/slurm/lib/libpmi.so</span></span></code></pre></div><h3 id=some-examples>Some examples<a href=#some-examples class=anchor>#</a></h3><hr><p>Here is an example for running MPI job (in this case, Quantum ESPRESSO) using 32 cores:</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running QE on 32 CPUs</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-qe-mpi-template.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=0-8:00:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem-per-cpu=1500M</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --ntasks=32</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;QE-Job&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># A example of an MPI parallel that </span>
</span></span><span style=display:flex><span><span style=color:#75715e># takes 32 cores on Grex for 8 hours. </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the modules:</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module load arch/avx512  intel/2023.2  openmpi/4.1.6 espresso/7.3.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export OMP_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>srun pw.x -in MyFile.scf.in  &gt; Myfile.scf.log
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><p>However, in practice there are cases when layout should be more restrictive. If the software code assumes equal distribution of processes per node, the request should be <strong>--nodes=N --ntasks-per-node=M</strong>. A similar case is MPMD codes (Like <a href=/grex-docs/specific-soft/nwchem/ class=is-pretty-link>NWCHem</a>
or GAMESS-US or OpenMolcas) that have some of the processes doing computation and some communication functions, and therefore requires at least two tasks running per each node.</p><p>For some codes, especially for large parallel jobs with intensive communication between tasks there can be performance differences due to memory and interconnect bandwidths, depending on whether the same number of parallel tasks is compacted on few nodes or spread across many of them. Find an example of the job below.</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running NWChem on 32 cores distributed on 4 nodes</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-nwchem-template.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --nodes=4</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --ntasks-per-node=8 </span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem-per-cpu=4000M</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;NWchem-Job&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=NWchem-dft-test</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Adjust the number of tasks, time and memory required.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># the above spec is for 32 compute tasks over 4 nodes.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the modules:</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>which nwchem
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Uncomment/Change these in case you want to use custom basis sets</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NWCHEMROOT<span style=color:#f92672>=</span>/global/software/cent7/nwchem/6.8.1-intel15-ompi314
</span></span><span style=display:flex><span>export NWCHEM_NWPW_LIBRARY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NWCHEMROOT<span style=color:#e6db74>}</span>/data/libraryps
</span></span><span style=display:flex><span>export NWCHEM_BASIS_LIBRARY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NWCHEMROOT<span style=color:#e6db74>}</span>/data/libraries
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># In most cases SCRATCH_DIR would  be on local nodes scratch</span>
</span></span><span style=display:flex><span><span style=color:#75715e># While results are in the same directory</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export NWCHEM_SCRATCH_DIR<span style=color:#f92672>=</span>$TMPDIR
</span></span><span style=display:flex><span>export NWCHEM_PERMANENT_DIR<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>pwd<span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Optional memory setting; note that this one or the one in your code</span>
</span></span><span style=display:flex><span><span style=color:#75715e># must match the #SBATCH --mem-per-cpu times compute tasks  !</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export NWCHEM_MEMORY_TOTAL<span style=color:#f92672>=</span><span style=color:#ae81ff>2000000000</span> <span style=color:#75715e># 24000 MB, double precision words only</span>
</span></span><span style=display:flex><span>export MKL_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>srun nwchem  dft_feco5.nw &gt; dft_feco5.$SLURM_JOBID.log
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Program finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><h3 id=openmpi>OpenMPI<a href=#openmpi class=anchor>#</a></h3><hr><p>OpenMPI is the default MPI implementation for Grex (and Compute Canada, now the Alliance). The modules for it on Grex are called <strong>ompi</strong> . The MPI example scripts above are all OpenMPI based. The old version 1.6.5 is there for compatibility reasons with older software; most users should use 3.1.x or 4.x.x versions. Using <strong>srun</strong> is recommended in all cases.</p><h3 id=intel-mpi>Intel MPI<a href=#intel-mpi class=anchor>#</a></h3><hr><p>For applications using IntelMPI (<strong>impi</strong> modules on Grex, or Intel-MPI based software from Compute Canada CVMFS software stack), a few environment variables have to be set. The following link explains it: <a href=https://software.intel.com/en-us/articles/how-to-use-slurm-pmi-with-the-intel-mpi-library-for-linux class=is-pretty-link>Using SLURM with PMI</a>
.</p><p>The JLab documentation example shows an <a href=https://scicomp.jlab.org/docs/intelMPIJobs class=is-pretty-link>example of SLURM script with IntelMPI</a>
.</p><h3 id=other-mpis>Other MPIs<a href=#other-mpis class=anchor>#</a></h3><hr><p>Finally, some canned codes like ANSYS or StatCCM+ would use a vendor-specific MPI implementation that would not tightly integrate with our scheduler&rsquo;s process to CPU core placement. In that case, several whole nodes (that is, with the number of tasks equal to the node&rsquo;s number of CPU cores) should be requested to prevent the impact on other jobs with resource congestion.</p><p>Such codes will also require a nodelist (machinefile) file obtained from SLURM and provided to them in their own format.</p><p>A custom script <strong>slurm_hl2hl.py</strong> makes this easier (see <a href=https://docs.alliancecan.ca/wiki/Star-CCM%2B class=is-pretty-link>CC StarCCM+</a>
or <a href=https://docs.alliancecan.ca/wiki/ANSYS#Cluster_Batch_Job_Submission class=is-pretty-link>CC ANSYS documentation</a>
). The script <strong>slurm_hl2hl.py</strong> is already available on Grex.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>slurm_hl2hl.py --format STAR-CCM+ &gt; machinefile</span></span></code></pre></div><h2 id=job-arrays>Job arrays<a href=#job-arrays class=anchor>#</a></h2><hr><p>Job arrays allow for submitting many similar jobs &ldquo;in one blow&rdquo;. It saves users work on job submission, and also makes SLURM scheduler more efficient in scheduling the array jobs because it would know they are the same with respect to size, expected wall time etc.</p><p>Array jobs work most naturally when a single code has to be applied for parameter sweep and/or to a large number of input files that are regularly named, for example as: test1.in, test2.in, &mldr; test99.in</p><p>Then, a single job script with <strong>#SBATCH --array=1,99</strong> can be used to submit the 99 jobs.</p><p>In order to distinguish between the input files, within each of the jobs at run time, you would have to obtain a value for the array index. Which is set by SLURM as <strong>${SLURM_ARRAY_TASK_ID}</strong> environment variable. The call to the code on a particular input will then be like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./my_code test<span style=color:#e6db74>${</span>SLURM_ARRAY_TASK_ID<span style=color:#e6db74>}</span>.in</span></span></code></pre></div><p>This way each of the array element jobs can distinguish their own portion of the work to do. A real life example is below; it attempts to run all of the <strong>Gaussian</strong> standard tests which have names of the format test0001.com, test0002.com, .. test1204.com, etc. Note the <strong>printf</strong> trick to deal with trailing zeroes in the input names.</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running job array: case of Gaussian standard tests</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-array-job-gauss-tests.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --cpus-per-task=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem=1000MB</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;G16-tests&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --array=1-1204</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Current working directory is `pwd`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Running on `hostname`&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set up the Gaussian environment using the module command:</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module load gaussian/g16.c01
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run g16 on an array job element</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>id<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>printf <span style=color:#e6db74>&#34;%04d&#34;</span> $SLURM_ARRAY_TASK_ID<span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>v<span style=color:#f92672>=</span>test<span style=color:#e6db74>${</span>id<span style=color:#e6db74>}</span>.com
</span></span><span style=display:flex><span>w<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>basename $v .com<span style=color:#e6db74>`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g16 &lt; $v &gt; <span style=color:#e6db74>${</span>w<span style=color:#e6db74>}</span>.<span style=color:#e6db74>${</span>SLURM_JOBID<span style=color:#e6db74>}</span>.out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><p>There are limits on how large array jobs can be (see our scheduling policies): the maximal number of elements in job array, as well as the maximal number of jobs that can be submitted by a user.</p><h2 id=using-cc-cvmfs-software>Using CC CVMFS software<a href=#using-cc-cvmfs-software class=anchor>#</a></h2><hr><p>As explained in more detail in the software/Modules documentation, we provide Compute Canada&rsquo;s software environment. Most of it can run out of the box by just specifying the corresponding module.</p><p>There are some <strong>caveats</strong>:</p><ul><li><p>Some of the Compute Canada software might have hardcoded environment variables that exist only on these systems. An example is <strong>SLURM_TMPDIR</strong>. On Grex, add <strong>export SLURM_TMPDIR=$TMPDIR</strong> to your job scripts.</p></li><li><p>In general, it is hard to containerize HPC. So the software that requires low-level hardware/device drivers access (OpenMPI, CUDA) may have problems when running on non-CC systems. Newer version of OpenMPI (3.1.x) seems to be more portable for using the PMIx job starting mechanism.</p></li><li><p>&ldquo;Restricted&rdquo; (commercial) software&rsquo;s binaries are not distributed by Compute Canada CVMFS due to the obvious licensing issues. It has to be installed locally on Grex.</p></li></ul><p>Having said that, <strong>module load CCEnv</strong> gives the right software environment to be run on Grex for a vast majority of threaded and serial software items from CC software stack. See discussion about MPI-parallel jobs below.</p><p>Because of the distributed nature of CVMFS, it might take time to download a program or library or data file. It would probably make sense to first access it interactively or from an interactive job to warm the CVMFS local cache, to avoid job failures due to the delay.</p><p>Below is an example of an R serial job that uses quite a few packages from Compute Canada software stack.</p><div class=sc-collapsible-container><div class=sc-collapsible-header>Script template for running R using a module from Compute Canada software stack.</div><div class=sc-collapsible-content><div class=sc-snippet-wrapper><div class=sc-snippet-container><div class=sc-snippet><div class=sc-snippet-caption>run-r-cc-cvmfs-template.sh</div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --ntasks=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem-per-cpu=4000M</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=0-72:00:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --job-name=&#34;R-gdal-jags-bench&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cd <span style=color:#e6db74>${</span>SLURM_SUBMIT_DIR<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the modules:           </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>module load CCEnv
</span></span><span style=display:flex><span>module load arch/avx512
</span></span><span style=display:flex><span>module load StdEnv/2023
</span></span><span style=display:flex><span>module load gcc/12.3  r/4.4.0 jags/4.3.2 geos/3.12.0  gdal/3.9.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export MKL_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>R --vanilla &lt; Benchmark.R &amp;&gt; benchmark.<span style=color:#e6db74>${</span>SLURM_JOBID<span style=color:#e6db74>}</span>.txt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Program finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span>
</span></span></code></pre></div></div></div></div></div></div><p>Users of contributed systems which are newer than the original Grex nodes might want to switch to <strong>arch/avx2</strong> or <strong>arch/avx512</strong> from the default <strong>arch/sse3</strong>.</p><h3 id=using-cc-cvmfs-software-that-is-mpi-based>Using CC CVMFS software that is MPI-based.<a href=#using-cc-cvmfs-software-that-is-mpi-based class=anchor>#</a></h3><hr><p>We have found that the recent Compute Canada toolchains that use OpenMPI 3.1.x work on Grex without any changes (that is, with <strong>srun</strong>). Therefore, for OpenMPI based applications, we recommend to load Compute Canada&rsquo;s software that depends on the recent toolchains, 2018.3 or later (Intel 2018 compilers, GCC 7.3 compilers and openmpi/3.1.2).</p><p>For example, the module commands below would load the Intel/OpenMPI 3.1.2 toolchain-based environment:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>module load CCEnv
</span></span><span style=display:flex><span>module load StdEnv/2018.3</span></span></code></pre></div><p>Below is an arbitrarily chosen IMB benchmark result for MPI1 on Grex, the <em>sendrecv</em> tests using two processes on two nodes with several MPI implementations (CC means MPI coming from the Compute Canada (now, the Alliance) stack, Grex means compiled locally on Grex).</p><p><img src=/grex-docs/benchmarks/mpis-on-grex.png alt></p><p>You can see that differences in performance between OpenMPI 3.1.x from CC stack and Grex are minor for this benchmark, even without attempting any local tuning for the CC OpenMPI.</p><h3 id=using-new-2020-cc-cvmfs-stack>Using New 2020 CC CVMFS Stack<a href=#using-new-2020-cc-cvmfs-stack class=anchor>#</a></h3><hr><p>Since Spring 2021, Compute Canada has updated the default software stack on their CVMFS distribution to StdEnv/2020 and gentoo. This version will not run on legacy Grex partitions (<strong>compute</strong>) at all, because it requires AVX2 CPU architecture. It will work as expected on all new GPU and CPU nodes (<strong>skylake</strong>, <strong>largemem</strong>, <strong>gpu</strong> and contributed systems).</p><h2 id=related-links>Related links<a href=#related-links class=anchor>#</a></h2><hr><ul><li><a href=https://slurm.schedmd.com/documentation.html class=is-pretty-link>SLURM</a></li></ul></div><div class="column is-sticky is-paddingless" id=tocWrapper><div class=is-paddingless id=tocContainer><div class=toc-header><h6 class="toc-title is-marginless">On this page:</h6><i title="Back to top" id=tocBackToTop class="fa-solid fa-circle-arrow-up"></i></div><div id=tocContent><div id=toc><nav id=TableOfContents><ul><li><a href=#batch-jobs>Batch jobs</a></li><li><a href=#scheduling-policies>Scheduling policies</a></li><li><a href=#typical-batch-job-cases>Typical batch job cases</a></li><li><a href=#serial-jobs>Serial jobs</a></li><li><a href=#smp--threaded--single-node-jobs>SMP / threaded / single node jobs</a></li><li><a href=#gpu-jobs>GPU jobs</a><ul><li><a href=#how-many-gpus-to-ask-for>How many GPUs to ask for?</a></li></ul></li><li><a href=#distributed-massively-parallel-jobs>Distributed, massively parallel jobs</a><ul><li><a href=#some-examples>Some examples</a></li><li><a href=#openmpi>OpenMPI</a></li><li><a href=#intel-mpi>Intel MPI</a></li><li><a href=#other-mpis>Other MPIs</a></li></ul></li><li><a href=#job-arrays>Job arrays</a></li><li><a href=#using-cc-cvmfs-software>Using CC CVMFS software</a><ul><li><a href=#using-cc-cvmfs-software-that-is-mpi-based>Using CC CVMFS software that is MPI-based.</a></li><li><a href=#using-new-2020-cc-cvmfs-stack>Using New 2020 CC CVMFS Stack</a></li></ul></li><li><a href=#related-links>Related links</a></li></ul></nav></div><div id=taxonomies><div class=taxonomy-wrapper><div class=toc-header><h6 class="toc-title is-marginless">Categories</h6></div><a href=/grex-docs/categories/scheduler/ class=taxonomy><i class="fa-solid fa-circle-dot"></i>Scheduler</a></div><div class=taxonomy-wrapper></div></div></div></div><div class=is-paddingless id=tocCollapsible title="Collapse/Uncollapse table of contents"><i class="fa-solid fa-angles-right fa-lg"></i></div></div></div></div><script type=text/javascript>const baseUrl="https://um-grex.github.io/grex-docs/",codeCopyBefore="Copy to clipboard",codeCopyAfter="Copied to clipboard",svgDownloadLabel="Download as SVG",helperLoadingLabel="Loading",searchNoResults="No result found",introNextLabel="Next",introPrevLabel="Previous",introSkipLabel="✗",introDoneLabel="Done",printLabel="Print current page",qrCodeLabel="Show current page QR code",naCommonLabel="Not available"</script><script type=module src=/grex-docs/js/theme/modules/const.min.js></script><script type=module src=/grex-docs/js/theme/modules/helpers.min.js></script><script type=module src=/grex-docs/js/theme/modules/helpersGlobal.min.js></script><script type=module src=/grex-docs/js/theme/init.min.js></script><script type=module src=/grex-docs/js/theme/navbar.min.js></script><script type=module src=/grex-docs/js/theme/print.min.js></script><script type=module src=/grex-docs/js/theme/qrcode.min.js></script><script type=module src=/grex-docs/js/theme/search.min.js></script><script type=module src=/grex-docs/js/theme/shortcuts.min.js></script><script type=module src=/grex-docs/js/theme/sidebar.min.js></script><script type=module src=/grex-docs/js/theme/toc.min.js></script><script type=module src=/grex-docs/js/shortcodes/collapsible.min.js></script><script type=module src=/grex-docs/js/shortcodes/treeview.min.js></script><script type=text/javascript src=/grex-docs/js/theme/browserCompatibility.min.js></script><script type=text/javascript src=/grex-docs/js/external/flexsearch/flexsearch.bundle.min.js></script><script type=text/javascript src=/grex-docs/js/external/qrious/qrious.min.js></script><script type=text/javascript src=/grex-docs/js/external/overlay-scrollbars/overlayscrollbars.min.js></script></body></html>