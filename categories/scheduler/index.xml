<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scheduler on Grex</title><link>https://um-grex.github.io/grex-docs/categories/scheduler/</link><description>Recent content in Scheduler on Grex</description><generator>Hugo</generator><language>en</language><copyright>The MIT License (MIT) Copyright © 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-docs/categories/scheduler/index.xml" rel="self" type="application/rss+xml"/><item><title>Running Quantum Espresso on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/espresso/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/espresso/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://www.quantum-espresso.org"
 class="is-pretty-link">Quantum ESPRESSO&lt;/a
>
 is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft).&lt;/p>
&lt;h2 id='system-specific-notes'>System specific notes&lt;a href='#system-specific-notes' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>On the Grex&amp;rsquo;s default software stack (&lt;em>SBEnv&lt;/em>), Espresso is built using a variety of compilers and Open MPI 4.1&lt;/p>
&lt;p>To find out which versions are available, use &lt;strong>module spider espresso&lt;/strong>.&lt;/p>
&lt;p>For a version 7.3.1, at the time of writing the following modules should be loaded:&lt;/p></description></item><item><title>Running Gaussian on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/gaussian/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="http://gaussian.com/"title="Gaussian" id="gaussian"
 class="is-pretty-link">Gaussian 16&lt;/a
>
 is a comprehensive suite for electronic structure modeling using &lt;strong>ab initio&lt;/strong>, DFT and semi-empirical methods. A list of Gaussian 16 features can be found &lt;a
 href="http://gaussian.com/g16glance/"title="Gaussian Features" id="gaussian-features"
 class="is-pretty-link">here&lt;/a
>
.&lt;/p>
&lt;h2 id='user-responsibilities-and-access'>User Responsibilities and Access&lt;a href='#user-responsibilities-and-access' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>University of Manitoba has a site license for Gaussian 16 and GaussView. However, it comes with certain license limitations, so access to the code is subject to some license conditions.&lt;/p>
&lt;p>Since, as of now, Compute Canada accounts are a superset of Grex accounts, users will want to initiate getting access by sending an email agreeing to Gaussian conditions to &lt;strong>support@tech.alliancecan.ca&lt;/strong>, confirming that you have read and agree to abide by the following conditions, and mentioning that you&amp;rsquo;d also want to access it on Grex:&lt;/p></description></item><item><title>Running GROMACS MD package on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/gromacs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/gromacs/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://www.gromacs.org"
 class="is-pretty-link">GROMACS&lt;/a
>
 (GROningen MAchine for Chemical Simulations) is a molecular dynamics package primarily designed for simulations of proteins, lipids and nucleic acids. GROMACS is one of the fastest and most popular software packages available and can run on CPUs as well as GPUs.&lt;/p>
&lt;h2 id='system-specific-notes'>System specific notes&lt;a href='#system-specific-notes' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>On the Grex&amp;rsquo;s default software stack (&lt;em>SBEnv&lt;/em>), GROMACS is built using a variety of compilers and OpenMPI 4.1&lt;/p>
&lt;p>To find out which versions are available, use &lt;strong>module spider gromacs&lt;/strong>. There could be more than one (for example, CPU and GPU) builds available for each GROMACS version as listed by &lt;em>module spider&lt;/em>.&lt;/p></description></item><item><title>Running Julia on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/julia/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/julia/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://julialang.org/"
 class="is-pretty-link">Julia&lt;/a
>
 is a programming language that was designed for performance, ease of use and portability. It is available as a module on Grex.&lt;/p>
&lt;h2 id='available-julia-versions'>Available Julia versions&lt;a href='#available-julia-versions' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>Presently, binary Julia version &lt;strong>julia/1.10.3&lt;/strong> is available. Use &lt;code>module spider julia&lt;/code> to find out other versions.&lt;/p>
&lt;h2 id='installing-packages'>Installing packages&lt;a href='#installing-packages' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>We do not maintain centralized versions of Julia packages. Users should install Julia modules in their home directory.&lt;/p>
&lt;p>The command is (in Julia REPL):&lt;/p></description></item><item><title>Running LAMMPS on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/lammps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/lammps/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://www.lammps.org/"
 class="is-pretty-link">LAMMPS&lt;/a
>
 is a classical molecular dynamics code. The name stands for Large-scale Atomic / Molecular Massively Parallel Simulator. LAMMPS is distributed by Sandia National Laboratories, a US Department of Energy laboratory.&lt;/p>
&lt;h2 id='modules'>Modules&lt;a href='#modules' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>On the Grex’s default software stack (SBEnv), LAMMPS was built using a variety of compilers and OpenMPI 4.1&lt;/p>
&lt;p>To find out which versions are available, use &lt;strong>module spider lammps&lt;/strong>&lt;/p>
&lt;p>As an example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>module load lammps/2021-09-29&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>and&lt;/p></description></item><item><title>Running MATLAB on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/matlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/matlab/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="http://www.mathworks.com/"
 class="is-pretty-link">MATLAB&lt;/a
>
 is a general-purpose high-level programming package for numerical work such as linear algebra, signal processing and other calculations involving matrices or vectors of data. We have a campus license for MATLAB which is used on Grex and other local computing resources. &lt;!-- MATLAB is available only for UManitoba users.-->&lt;/p>
&lt;p>As with most of the Grex software, MATLAB is available as a module. The following command will load the latest version available on Grex:&lt;/p></description></item><item><title>Running NWChem on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/nwchem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/nwchem/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://nwchemgit.github.io/"
 class="is-pretty-link">NWChem&lt;/a
>
 is a Scalable, massive parallel and open source solution for large scale molecular simulations. NWChem is actively developed by a consortium of developers and maintained by the EMSL located at the Pacific Northwest National Laboratory (PNNL) in Washington State. The code is distributed as open source under the terms of the Educational Community License version 2.0 (ECL 2.0).&lt;/p>
&lt;h2 id='system-specific-notes'>System specific notes&lt;a href='#system-specific-notes' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>To find out which versions of NWChem are available, use &lt;strong>module spider nwchem&lt;/strong> .&lt;/p></description></item><item><title>Running ORCA on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/orca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/orca/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="http://cec.mpg.de/forum/"
 class="is-pretty-link">ORCA&lt;/a
>
 is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semi-empirical methods to DFT to single - and multi-reference correlated ab initio methods. It can also treat environmental and relativistic effects.&lt;/p>
&lt;h2 id='user-responsibilities-and-access'>User Responsibilities and Access&lt;a href='#user-responsibilities-and-access' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>ORCA is a proprietary software, even if it is free it still requires you to agree to the ORCA license conditions. We have installed ORCA on Grex, but to access the binaries, each of the ORCA users has to confirm they have accepted the license terms.&lt;/p></description></item><item><title>Slurm partitions</title><link>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</guid><description>&lt;h2 id='partitions'>Partitions&lt;a href='#partitions' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a &amp;ldquo;partition&amp;rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who &amp;ldquo;owns&amp;rdquo; the hardware as well.&lt;/p>
&lt;p>There is no fully automatic selection of partitions, other than the default &lt;strong>skylake&lt;/strong> for most of the users for the short jobs. For the contributors&amp;rsquo; group members, the default partition will be their contributed nodes. &lt;strong>Thus, in many cases users have to specify the partition manually when submitting their jobs!&lt;/strong>&lt;/p></description></item><item><title>Running VASP on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/vasp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/vasp/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://www.vasp.at/wiki/index.php/The_VASP_Manual"
 class="is-pretty-link">VASP&lt;/a
>
 is a massively parallel plane-wave solid state DFT code. On Grex it is available only for the research groups that hold VASP licenses. To get access, PIs would need to send us a confirmation email from the VASP vendor, detailing the status of their license and a list of users allowed to use it.&lt;/p>
&lt;h2 id='system-specific-notes'>System specific notes&lt;a href='#system-specific-notes' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;!--On the Grex local software stack, we have VASP 5 and VASP 6 using Intel compiler and OpenMPI 3.1.--> 
&lt;p>To find out which versions of VASP are available, use &lt;code>module spider vasp&lt;/code> .&lt;/p></description></item><item><title>How to run interactive jobs on Grex?</title><link>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</guid><description>&lt;h2 id='interactive-work'>Interactive work&lt;a href='#interactive-work' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>edit files&lt;/li>
&lt;li>compile codes and run short interactive calculations.&lt;/li>
&lt;li>configure and build programs (limit the number of threads to 4: make -j4)&lt;/li>
&lt;li>submit and monitor jobs&lt;/li>
&lt;li>transfer and/or download data&lt;/li>
&lt;li>run short tests, &amp;hellip; etc.&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].&lt;/p></description></item><item><title>Running batch jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</guid><description>&lt;h2 id='batch-jobs'>Batch jobs&lt;a href='#batch-jobs' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>HPC systems usually are &lt;strong>clusters&lt;/strong> of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users&amp;rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called &amp;ldquo;jobs&amp;rdquo; to a &amp;ldquo;batch queue&amp;rdquo;. A subset of the available resources of the HPC machine is allocated to each of the users&amp;rsquo; batch jobs, and they run without any need for user intervention as soon as the resources become available.&lt;/p></description></item><item><title>Scheduling policies and running jobs on Contributed nodes</title><link>https://um-grex.github.io/grex-docs/running-jobs/contributed-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/contributed-systems/</guid><description>&lt;h2 id='scheduling-policies-for-contributed-systems'>Scheduling policies for contributed systems&lt;a href='#scheduling-policies-for-contributed-systems' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;!--
![](hpcc/grex-room-2020.png)
-->
&lt;p>Grex has a few user-contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the &amp;ldquo;preferred access&amp;rdquo; is preemption.&lt;/p>
&lt;h2 id='on-the-definition-of-preferential-access-to-hpc-systems'>On the definition of preferential access to HPC systems&lt;a href='#on-the-definition-of-preferential-access-to-hpc-systems' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>Preferential access is when you have non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have. HPC makes access to CPU cores / GPUs / Memory exclusive per job, for the duration of the job (as opposed to time-sharing). Priority is a factor that decides which job gets to start (and thus exclude other jobs) first if there is a competitive situation (more jobs than free cores).&lt;/p></description></item><item><title>Using local disks: $TMPDIR</title><link>https://um-grex.github.io/grex-docs/running-jobs/using-localdisks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/using-localdisks/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>High-Performance Computing (HPC) systems, such as Grex, typically provide a shared, scalable, POSIX-compliant filesystem that is accessible by all compute nodes.
For Grex and the current generation of Alliance HPC machines, this shared filesystem is powered by &lt;a
 href="https://www.lustre.org/"
 class="is-pretty-link">Lustre FS&lt;/a
>
, which enables data sharing for compute jobs running on the cluster&amp;rsquo;s nodes. Due to its scalable design, Lustre FS can offer significant bandwidth for large parallel jobs through its parallelized Object Storage Targets (OSTs).&lt;/p></description></item><item><title>Running jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/</guid><description>&lt;h2 id='why-running-jobs-in-batch-mode'>Why running jobs in batch mode?&lt;a href='#why-running-jobs-in-batch-mode' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>There are many reasons for adopting a batch mode for running jobs on a cluster. From providing user&amp;rsquo;s computations with fairness, traffic control to prevent resource congestion and wasting, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the &lt;a
 href="https://slurm.schedmd.com/documentation.html"
 class="is-pretty-link">SLURM&lt;/a
>
 batch system since &lt;strong>December 2019&lt;/strong> with the &lt;strong>Linux/SLURM update&lt;/strong> &lt;a
 href="https://um-grex.github.io/grex-docs/changes/linux-slurm-update/"
 class="is-pretty-link">project&lt;/a
>
.&lt;/p></description></item><item><title>Software Specific Notes</title><link>https://um-grex.github.io/grex-docs/specific-soft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/</guid><description>&lt;h2 id='software-specific-notes'>Software specific notes&lt;a href='#software-specific-notes' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>This page refers to the usage of some specific programs installed on Grex, like ORCA, VASP, &amp;hellip; etc.&lt;/p>
&lt;hr>
&lt;h2 id='software--applications'>Software / Applications&lt;a href='#software--applications' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;div class="sc-treeview">&lt;ul
 >&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/espresso/"
 class="is-pretty-link"
 >Espresso&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/gaussian/"
 class="is-pretty-link"
 >Gaussian&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/gromacs/"
 class="is-pretty-link"
 >GROMACS&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/julia/"
 class="is-pretty-link"
 >Julia&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/lammps/"
 class="is-pretty-link"
 >LAMMPS&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/matlab/"
 class="is-pretty-link"
 >MATLAB&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/nwchem/"
 class="is-pretty-link"
 >NWChem&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/orca/"
 class="is-pretty-link"
 >ORCA&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/vasp/"
 class="is-pretty-link"
 >VASP&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/specific-soft/python-ai/"
 class="is-pretty-link"
 >Python for ML&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;/ul>&lt;/div>

&lt;h2 id='external-links'>External links&lt;a href='#external-links' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;ul>
&lt;li>&lt;a
 href="https://docs.alliancecan.ca/wiki/Running_jobs"
 class="is-pretty-link">Running jobs&lt;/a
>
 (on the Alliance&amp;rsquo;s clusters)&lt;/li>
&lt;li>&lt;a
 href="https://slurm.schedmd.com/documentation.html"
 class="is-pretty-link">SLURM&lt;/a
>
 documentation.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;!-- Changes and update:
* Last revision: Aug 28, 2024. 
--></description></item></channel></rss>