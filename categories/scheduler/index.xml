<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scheduler on Grex</title><link>https://um-grex.github.io/grex-docs/categories/scheduler/</link><description>Recent content in Scheduler on Grex</description><generator>Hugo</generator><language>en</language><copyright>The MIT License (MIT) Copyright Â© 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-docs/categories/scheduler/index.xml" rel="self" type="application/rss+xml"/><item><title>Running Gaussian on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/gaussian/</guid><description>Introduction# Gaussian 16 is a comprehensive suite for electronic structure modeling using ab initio, DFT and semi-empirical methods. A list of Gaussian 16 features can be found here .
User Responsibilities and Access# University of Manitoba has a site license for Gaussian 16 and GaussView. However, it comes with certain license limitations, so access to the code is subject to some license conditions.
Since, as of now, Compute Canada accounts are a superset of Grex accounts, users will want to initiate getting access by sending an email agreeing to Gaussian conditions to support@tech.</description></item><item><title>Running Julia on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/julia/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/julia/</guid><description>Introduction# Julia is a programming language that was designed for performance, ease of use and portability. It is available as a module on Grex.
Available Julia versions# Presently, binary Julia versions 1.3.0, 1.5.4 and 1.6.1 are available. Use module spider julia to find out other versions.
Installing packages# We do not maintain centralized versions of Julia packages. Users should install Julia modules in their home directory.
The command is (in Julia REPL):</description></item><item><title>Running LAMMPS on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/lammps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/lammps/</guid><description>Introduction# LAMMPS is a classical molecular dynamics code. The name stands for Large-scale Atomic / Molecular Massively Parallel Simulator.
Modules# Multiple versions of LAMMPS were installed on Grex. To see all the available versions, use module spider lammps and follow the instructions.
Available CPU versions:# Version Module Name Supported Packages 29 Sep 21 lammps/29Sep21 * 05 Jun 19 lammps/5Jun19 * 11 Aug 17 lammps/11Aug17 * 05 Nov 16 lammps/5Nov16 * 30 Jul 16 lammps/30jul16 * Available GPU versions:# As for the time when writing this page, there is only one version of LAMMPS with GPU support.</description></item><item><title>Running MATLAB on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/matlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/matlab/</guid><description>Introduction# MATLAB is a general-purpose high-level programming package for numerical work such as linear algebra, signal processing and other calculations involving matrices or vectors of data. We have a campus license for MATLAB which is used on Grex and other local computing resources. MATLAB is available only for UManitoba users.
As with most of the Grex software, MATLAB is available as a module. The following command will load the latest version available on Grex:</description></item><item><title>Running NWChem on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/nwchem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/nwchem/</guid><description>Introduction# NWChem is a Scalable open-source solution for large scale molecular simulations. NWChem is actively developed by a consortium of developers and maintained by the EMSL located at the Pacific Northwest National Laboratory (PNNL) in Washington State. The code is distributed as open-source under the terms of the Educational Community License version 2.0 (ECL 2.0).
System specific notes# On the Grex software stack, NWChem is using OpenMPI 3.1 with Intel compilers toolchains.</description></item><item><title>Running ORCA on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/orca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/orca/</guid><description>Introduction# ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semi-empirical methods to DFT to single - and multi-reference correlated ab initio methods. It can also treat environmental and relativistic effects.
User Responsibilities and Access# ORCA is a proprietary software, even if it is free it still requires you to agree to the ORCA license conditions.</description></item><item><title>Slurm partitions</title><link>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</guid><description>Partitions# The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a &amp;ldquo;partition&amp;rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who &amp;ldquo;owns&amp;rdquo; the hardware as well.
There is no fully automatic selection of partitions, other than the default skylake for most of the users, and compute for the short jobs.</description></item><item><title>Running Priroda on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/priroda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/priroda/</guid><description>Introduction# Priroda is a fast parallel relativistic DFT and ab-initio code for molecular modeling, developed by Dr. Dimitri N. Laikov. The code originally implemented fast resolution-of-identity GGA DFT for coulomb and exchange integrals. Later it was extended to provide RI-DFT with hybrid functional, RI-HF and RI-MP2, and parallel high-level coupled-cluster methods. All these levels of theory can be used together with an efficient all-electron scalar-relativistic method, with small-component bases supplied for all the elements of the Periodic Table.</description></item><item><title>Running VASP on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/vasp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/vasp/</guid><description>Introduction# VASP is a massively parallel plane-wave solid state DFT code. On Grex it is available only for the research groups that hold VASP licenses. To get access, PIs would need to send us a confirmation email from the VASP vendor, detailing the status of their license and a list of users allowed to use it.
System specific notes# On the Grex local software stack, we have VASP 5 and VASP 6 using Intel compiler and OpenMPI 3.</description></item><item><title>How to run interactive jobs on Grex?</title><link>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</guid><description>Interactive work# The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:
edit files compile codes and run short interactive calculations. configure and build programs (limit the number of threads to 4: make -j4) submit and monitor jobs transfer and/or download data run short tests, &amp;hellip; etc. In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].</description></item><item><title>Running batch jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</guid><description>Batch jobs# HPC systems usually are clusters of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users&amp;rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called &amp;ldquo;jobs&amp;rdquo; to a &amp;ldquo;batch queue&amp;rdquo;.</description></item><item><title>Using local disks: $SLURM_TMPDIR</title><link>https://um-grex.github.io/grex-docs/running-jobs/using-localdisks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/using-localdisks/</guid><description>Introduction#</description></item><item><title>Scheduling policies and running jobs on Contributed nodes</title><link>https://um-grex.github.io/grex-docs/running-jobs/contributed-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/contributed-systems/</guid><description>Scheduling policies for contributed systems# Grex has a few user-contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the &amp;ldquo;preferred access&amp;rdquo; is preemption.
On the definition of preferential access to HPC systems# Preferential access is when you have non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have.</description></item><item><title>Running jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/</guid><description>Why running jobs in batch mode?# There are many reasons for adopting a batch mode for running jobs on a cluster. From providing user&amp;rsquo;s computations with fairness, traffic control to prevent resource congestion and wasting, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the SLURM batch system since December 2019 with the Linux/SLURM update project .</description></item><item><title>Software Specific Notes</title><link>https://um-grex.github.io/grex-docs/specific-soft/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/</guid><description>Software specific notes# This page refers to the usage of some specific programs installed on Grex, like ORCA, VASP, &amp;hellip; etc.
Software / Applications# Gaussian Julia LAMMPS MATLAB NWChem ORCA Priroda VASP Python for ML External links# Running jobs (on the Alliance&amp;rsquo;s clusters) SLURM documentation.</description></item></channel></rss>