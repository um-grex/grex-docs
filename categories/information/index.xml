<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Information on Grex</title><link>https://um-grex.github.io/grex-docs/categories/information/</link><description>Recent content in Information on Grex</description><generator>Hugo</generator><language>en</language><copyright>The MIT License (MIT) Copyright © 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-docs/categories/information/index.xml" rel="self" type="application/rss+xml"/><item><title>Digital Research Alliance of Canada</title><link>https://um-grex.github.io/grex-docs/friends/alliancecan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/friends/alliancecan/</guid><description>&lt;p>&lt;a
 href="https://alliancecan.ca/">&lt;img
 src="https://um-grex.github.io/grex-docs/grex-docs/alliance/Alliance_logo_English-first_slogan.jpg"
 alt="Digital Research Alliance of Canad"
 class="Digital Research Alliance of Canad"/>
&lt;/a
>
&lt;/p>
&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>Digital Research Alliance of Canada (the Alliance) is a Canadian National Digital Research Infrastructure (&lt;strong>DRI&lt;/strong>) organization (formerly known as Compute Canada). It provides the eligible researchers with the research computing resources such as several High-performance computing (HPC) systems with a large curated HPC &lt;a
 href="https://docs.alliancecan.ca/wiki/Available_software"
 class="is-pretty-link">software stack&lt;/a
>
, private OpenStack cloud, and &lt;a
 href="https://docs.alliancecan.ca/wiki/Globus"
 class="is-pretty-link">Globus&lt;/a
>
 data transfer software.&lt;/p>
&lt;p>The Alliance also maintains a user authentication service and usage database called CCDB. On Grex, we rely on CCDB for accessing our system. The first step to get started with Grex is to &lt;a
 href="https://alliancecan.ca/en/services/advanced-research-computing/account-management/apply-account"title="Apply for an account" id="apply-for-an-account"
 class="is-pretty-link">&lt;strong>register&lt;/strong>&lt;/a
>
 for an Alliance account at the CCDB website (if you do not already have one).&lt;/p></description></item><item><title>Linux/SLURM update project</title><link>https://um-grex.github.io/grex-docs/changes/linux-slurm-update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/changes/linux-slurm-update/</guid><description>&lt;h2 id='grex-changes-linuxslurm-update-project'>Grex changes: Linux/SLURM update project.&lt;a href='#grex-changes-linuxslurm-update-project' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;strong>December 10-11, 2019&lt;/strong>&lt;/p>
&lt;h2 id='introduction--motivation'>Introduction / Motivation&lt;a href='#introduction--motivation' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;strong>Grex&lt;/strong> runs an old version of CentOS 6, which gets unsupported in 2020. The 2.6.x Linux kernel that is shipped with CentOS 6 does not support containerized workloads that require recent kernel features. The Lustre parallel filesystem client had some troubles that we were unable to resolve with the CentOS 6 kernel version as well. Finally, the original Grex resource management software, Torque 2.5 and Moab7 are unable to properly schedule jobs that use newer MPI implementations (OpenMPI 2 and 3), which are increasingly common amongst HPC users. Therefore, using the power outages of October and December 2019, we have embarked on a rather ambitious project of updating the entire Grex OS and software stack and scheduling to CentOS 7 and SLURM. This document outlines the changes and how they will affect Grex users.&lt;/p></description></item><item><title>Data sizes and quotas</title><link>https://um-grex.github.io/grex-docs/storage/data-sizes-and-quota/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/storage/data-sizes-and-quota/</guid><description>&lt;h2 id='data-size-and-quotas'>Data size and quotas&lt;a href='#data-size-and-quotas' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>This section explains how to find the actual space and inode usage of &lt;strong>/home/&lt;/strong> and &lt;strong>/project&lt;/strong> allocations on Grex. We limit the size of the data and the number of files that can be stored on these filesystems. The table provides a &amp;ldquo;default&amp;rdquo; storage quota on Grex. Larger quota can be obtained on &lt;strong>/project&lt;/strong> via UM local RAC process.&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>File system&lt;/th>
 &lt;th style="text-align: center">Type&lt;/th>
 &lt;th style="text-align: center">Total space&lt;/th>
 &lt;th style="text-align: center">Bulk Quota&lt;/th>
 &lt;th style="text-align: center">Files Quota&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>&lt;strong>/home&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">NFSv4/RDMA&lt;/td>
 &lt;td style="text-align: center">&lt;strong>15 TB&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">100 GB / user&lt;/td>
 &lt;td style="text-align: center">0.5M per user&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>&lt;strong>/project&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">Lustre&lt;/td>
 &lt;td style="text-align: center">&lt;strong>2 PB&lt;/strong>&lt;/td>
 &lt;td style="text-align: center">5-20 TB / group&lt;/td>
 &lt;td style="text-align: center">1M / user, 2M / group&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>To figure out where your current usage stands with the limit, POSIX &lt;strong>quota&lt;/strong> or Lustre&amp;rsquo;s analog, &lt;strong>lfs quota&lt;/strong>, commands can be used. A convenient command, &lt;strong>diskusage_report&lt;/strong> summarizes usage and quota across all the available filesystems.&lt;/p></description></item><item><title>Grex: High Performance Computing Cluster at University of Manitoba</title><link>https://um-grex.github.io/grex-docs/grex/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/grex/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;strong>Grex&lt;/strong> is a UManitoba High Performance Computing (HPC) system, first put in production in early &lt;strong>2011&lt;/strong> as part of WestGrid consortium. &amp;ldquo;Grex&amp;rdquo; is a &lt;em>Latin&lt;/em> name for &amp;ldquo;herd&amp;rdquo; (or maybe &amp;ldquo;flock&amp;rdquo;?). The names of the Grex login nodes (&lt;a
 href="https://en.wikipedia.org/wiki/Bison"title="Bison" id="bison"
 class="is-pretty-link">bison&lt;/a
>
, tatanka, &lt;a
 href="https://en.wikipedia.org/wiki/Zebu"title="Zebu" id="zebu"
 class="is-pretty-link">zebu&lt;/a
>
, &lt;a
 href="https://en.wikipedia.org/wiki/Yak"title="Yak" id="yak"
 class="is-pretty-link">yak&lt;/a
>
) also refer to various kinds of bovine animals.&lt;/p>
&lt;div class="sc-alert sc-alert-warning">Please note that &lt;em>bison&lt;/em> and &lt;em>tatanka&lt;/em> are decommissioned during and after the outage of August - September 2024. These login nodes are no longer available.
For more information, visit the updates &lt;a
 href="https://um-grex.github.io/grex-docs/grex-docs/updates/"
 class="is-pretty-link">page&lt;/a
>&lt;/div>

&lt;p>Since being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators.&lt;/p></description></item><item><title>Access and Usage Conditions</title><link>https://um-grex.github.io/grex-docs/access/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/access/</guid><description>&lt;h1 id='access-conditions'>Access Conditions&lt;a href='#access-conditions' class='anchor'>#&lt;/a>
&lt;/h1>&lt;hr>
&lt;p>Grex is open to all researchers at University of Manitoba and their collaborators. The main purpose of the Grex system is Research; it might be used for grad studies courses with a strong research component, for their course-based research.&lt;/p>
&lt;p>Access to the system, and resource allocaions are &lt;em>by research group&lt;/em>; that is, the Principal Investigator (PI) receives resources for his group, and approves access for his group members.&lt;/p></description></item><item><title>Data Backup</title><link>https://um-grex.github.io/grex-docs/storage/data-backup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/storage/data-backup/</guid><description>&lt;h2 id='backup-policies'>Backup policies&lt;a href='#backup-policies' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>Since late 2023, there has been a tape backup for user data stored on main Grex filesystems, &lt;strong>/home&lt;/strong> and &lt;strong>/project&lt;/strong>. Our limited resources and large amounts of data do put some limitations on what and how fast can be backed up and restored. All backup is done to tape in the same HPCC data centre.&lt;/p>
&lt;p>The &lt;strong>/home&lt;/strong> filesystem is backed up daily using incremental backup. A new full backup is done quarterly.&lt;/p></description></item><item><title>Connecting to Grex and Transferring data</title><link>https://um-grex.github.io/grex-docs/connecting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/connecting/</guid><description>&lt;h2 id='connecting-to-grex'>Connecting to Grex&lt;a href='#connecting-to-grex' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>In order to use almost any HPC system, you would need to be able to somehow connect and log in to it. Also, it would be necessary to be able to transfer data to and from the system. The standard means for these tasks are provided by the &lt;a
 href="https://en.wikipedia.org/wiki/Secure_Shell"title="Secure Shell" id="secure-shell"
 class="is-pretty-link">SSH protocol&lt;/a
>
. The following hosts (login nodes) are available:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>yak.hpc.umanitoba.ca&lt;/strong>&lt;/li>
&lt;li>&lt;strong>grex.hpc.umanitoba.ca&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>To log in to Grex in the text (or bash) mode, connect to one of the above hosts using an &lt;a
 href="https://um-grex.github.io/grex-docs/grex-docs/connecting/ssh/"
 class="is-pretty-link">&lt;strong>SSH&lt;/strong>&lt;/a
>
 (Secure SHELL) client.&lt;/p></description></item><item><title>Data sharing</title><link>https://um-grex.github.io/grex-docs/storage/data-sharing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/storage/data-sharing/</guid><description>&lt;h2 id='data-sharing'>Data sharing&lt;a href='#data-sharing' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>During a research project, it is often required to share datasets or code within a research group, or between collaborating research groups. This documentation page explains how to share data residing within a given HPC system (in our case, Grex). Sharing data oitside of the HPC system is done by other means (for example Globus, MS OneDrive).&lt;/p>
&lt;blockquote>
&lt;p>How not to share data: no sharing account credentials, which is forbidden. Nor opening the data to be &amp;ldquo;World-accessible&amp;rdquo;, which is a bad practice.&lt;/p></description></item><item><title>Local IT Resources</title><link>https://um-grex.github.io/grex-docs/friends/localit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/friends/localit/</guid><description>&lt;h2 id='introduction'>Introduction&lt;a href='#introduction' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>In addition to the &lt;strong>HPC&lt;/strong> Research Computing facility, there are other &lt;strong>IT&lt;/strong> providers that might relate to research.&lt;/p>
&lt;h2 id='resources-provided-by-ist'>Resources provided by IST&lt;a href='#resources-provided-by-ist' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>&lt;a
 href="https://umanitoba.ca/computing/ist/index.html"title="Information Services and Technology" id="information-services-and-technology"
 class="is-pretty-link">IST&lt;/a
>
 provides a number of services related to Administrative IT, Teaching, and Research Computing as well.&lt;/p>
&lt;p>For more information about IST, please visit &lt;strong>UM IST&lt;/strong> &lt;a
 href="http://umanitoba.ca/ist/help/"title="UM IST Help" id="um-ist-help"
 class="is-pretty-link">Help&lt;/a
>
 website and &lt;strong>IST&lt;/strong> service &lt;a
 href="http://umanitoba.ca/ist/service_catalogue/"title="IST service catalog" id="ist-service-catalog"
 class="is-pretty-link">catalog&lt;/a
>
.&lt;/p>
&lt;h2 id='resources-provided-by-the-libraries'>Resources provided by the Libraries&lt;a href='#resources-provided-by-the-libraries' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>Libraries provide a variety of services related to Research Data Management, as well as a GIS service: UM &lt;a
 href="https://libguides.lib.umanitoba.ca/researchservices"
 class="is-pretty-link">Libraries&lt;/a
>
 Research Services.&lt;/p></description></item><item><title>Getting Help</title><link>https://um-grex.github.io/grex-docs/support/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/support/</guid><description>&lt;h2 id='the-alliance-support'>The Alliance support&lt;a href='#the-alliance-support' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>The single point support contact for the Alliance is &lt;strong>support@tech.alliancecan.ca&lt;/strong>&lt;/p>
&lt;p>Emailing to this address will create a &lt;strong>support ticket&lt;/strong> in the Alliance ticketing system (Help Desk). We support both local (Grex) and National resources through the Alliance support ticketing system. This is the main support contact for our HPC group and it is a preferred method (as compared to contacting an HPC analyst directly). If you use your UManitoba email address (email registered in CCDB) to contact the Alliance support, it will reach us faster because the system will automatically detect it and assign your user name to the generated ticket.&lt;/p></description></item><item><title>Friendly Organizations</title><link>https://um-grex.github.io/grex-docs/friends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/friends/</guid><description>&lt;div class="sc-treeview">&lt;ul
 class='sc-treeview-tree'>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;div class="sc-treeview-icon">
 &lt;i class="fa-solid fa-house-chimney">&lt;/i>
 &lt;/div>&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/grex-docs/friends/alliancecan/"
 class="is-pretty-link"
 >DRAC (Alliance)&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;li class="">
 &lt;div class="sc-treeview-label">&lt;div class="sc-treeview-icon">
 &lt;i class="fa-solid fa-house-chimney">&lt;/i>
 &lt;/div>&lt;p class="is-marginless">
 &lt;a
 href="https://um-grex.github.io/grex-docs/grex-docs/friends/localit/"
 class="is-pretty-link"
 >UManitoba IT resources&lt;/a
 >
 &lt;/p>
 &lt;/div>&lt;/li>&lt;/ul>&lt;/div>

&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></description></item><item><title>Frequently Asked Questions.</title><link>https://um-grex.github.io/grex-docs/faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/faq/</guid><description>&lt;h2 id='access'>Access&lt;a href='#access' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;h3 id='getting-an-account'>Getting an account&lt;a href='#getting-an-account' class='anchor'>#&lt;/a>
&lt;/h3>&lt;hr>
&lt;h3 id='forgot-andor-reset-password'>Forgot and/or reset password&lt;a href='#forgot-andor-reset-password' class='anchor'>#&lt;/a>
&lt;/h3>&lt;hr>
&lt;p>If you forgot and/or would like to reset your password for Grex and/or any Alliance (former Compute Canada) national cluster, visit &lt;a
 href="https://ccdb.computecanada.ca/security/forgot"
 class="is-pretty-link">CCDB&lt;/a
>
. Please note that you will not be able to reset your password before your first role gets approved by an Alliance (Compute Canada) administrator. Once updated, please wait for at least an hour before connecting to any cluster.&lt;/p></description></item><item><title>Linux/SLURM update project</title><link>https://um-grex.github.io/grex-docs/changes/changes-before-2020/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/changes/changes-before-2020/</guid><description>&lt;h2 id='grex-defunded-since-april-2-2018'>Grex defunded since April 2, 2018&lt;a href='#grex-defunded-since-april-2-2018' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>Since being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators. The old WestGrid documentation, hosted on the WestGrid website became irrelevant after the Grex upgrade, so please visit Grex’s New &lt;a
 href="https://um-grex.github.io/grex-docs/grex-docs/"
 class="is-pretty-link">Documentation&lt;/a
>
. Thus, if you are an experienced user in the previous “version” of Grex, you might benefit from reading this document: Description of Grex &lt;a
 href="https://um-grex.github.io/grex-docs/grex-docs/changes/linux-slurm-update/"
 class="is-pretty-link">changes&lt;/a
>
.&lt;/p></description></item><item><title>Disclaimer</title><link>https://um-grex.github.io/grex-docs/disclaimer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/disclaimer/</guid><description>&lt;hr>
&lt;blockquote>
&lt;p>This website is a place for &lt;strong>technical information&lt;/strong> related to certain Research Computing resources, maintained for the benefit of the researchers at the University of Manitoba, &lt;a
 href="https://umanitoba.ca/"title="University of Manitoba" id="university-of-manitoba"
 class="is-pretty-link">UofM&lt;/a
>
 and their external collaborators. The information, which is technical in nature, represents advice on best practices for using the Research Computing resources (HPC). Parts of the website are preliminary/draft texts released provisionally, in order to speed up the documentation process, and may contain inaccuracies and errors.&lt;/p></description></item><item><title>Glossary</title><link>https://um-grex.github.io/grex-docs/glossary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/glossary/</guid><description>&lt;h2 id='a'>A&lt;a href='#a' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>AI&lt;/strong> :: Artificial Intelligence&lt;/li>
&lt;li>&lt;strong>ARC&lt;/strong> :: Advanced Research Computing&lt;/li>
&lt;li>&lt;strong>AVX&lt;/strong> :: Advanced Vector eXtensions&lt;/li>
&lt;/ul>
&lt;h2 id='b'>B&lt;a href='#b' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>BLAS&lt;/strong> :: Basic Linear Algebra Subprograms&lt;/li>
&lt;/ul>
&lt;h2 id='c'>C&lt;a href='#c' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>CPU&lt;/strong> :: Central Processing Unit&lt;/li>
&lt;li>&lt;strong>CC&lt;/strong> :: Compute Canada, a former organization replaced by DRAC&lt;/li>
&lt;li>&lt;strong>CCDB&lt;/strong> :: CC User Database, a user accounts portal maintained by DRAC&lt;/li>
&lt;li>&lt;strong>CVMFS&lt;/strong> :: Cern Virtual Machines File System&lt;/li>
&lt;li>&lt;strong>CY&lt;/strong>:: Core Year, a unit of CPU and GPU resources used by CCDB and RAC&lt;/li>
&lt;/ul>
&lt;h2 id='d'>D&lt;a href='#d' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>DFT&lt;/strong> :: 1. Discrete Fourier Transform, 2. Density Functional Theory&lt;/li>
&lt;li>&lt;strong>DRAC&lt;/strong> :: Digital Research Alliance of Canada, same as The Alliance&lt;/li>
&lt;/ul>
&lt;h2 id='e'>E&lt;a href='#e' class='anchor'>#&lt;/a>
&lt;/h2>&lt;h2 id='f'>F&lt;a href='#f' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>FFT&lt;/strong> :: Fast Fourier Transform&lt;/li>
&lt;/ul>
&lt;h2 id='g'>G&lt;a href='#g' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>GCC&lt;/strong> :: GNU Compiler Collection&lt;/li>
&lt;li>&lt;strong>GPU&lt;/strong> :: Graphics Processing Unit&lt;/li>
&lt;li>&lt;strong>GUI&lt;/strong> :: Graphical User Interface&lt;/li>
&lt;/ul>
&lt;h2 id='h'>H&lt;a href='#h' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>HPC&lt;/strong> :: High-Performance Computing&lt;/li>
&lt;li>&lt;strong>HTTP&lt;/strong> :: Hypertext Transfer Protocol&lt;/li>
&lt;li>&lt;strong>HTTPS&lt;/strong> :: Hypertext Transfer Protocol Secure&lt;/li>
&lt;li>&lt;strong>HDF5&lt;/strong> :: Hierarchical Data file Format version 5&lt;/li>
&lt;/ul>
&lt;h2 id='i'>I&lt;a href='#i' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>I/O&lt;/strong> :: Input/Output&lt;/li>
&lt;li>&lt;strong>IOPS&lt;/strong> :: I/O Operations Per Second&lt;/li>
&lt;li>&lt;strong>IST&lt;/strong> :: Information Services and Technologies at UM&lt;/li>
&lt;/ul>
&lt;h2 id='j'>J&lt;a href='#j' class='anchor'>#&lt;/a>
&lt;/h2>&lt;h2 id='k'>K&lt;a href='#k' class='anchor'>#&lt;/a>
&lt;/h2>&lt;h2 id='l'>L&lt;a href='#l' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>LAPACK&lt;/strong> :: Linear Algebra PACKage, used together with BLAS&lt;/li>
&lt;/ul>
&lt;h2 id='m'>M&lt;a href='#m' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>MDS&lt;/strong> :: Metadata Server&lt;/li>
&lt;li>&lt;strong>MDT&lt;/strong> :: Metadata Target&lt;/li>
&lt;li>&lt;strong>ML&lt;/strong> :: Machine Learning&lt;/li>
&lt;li>&lt;strong>MPI&lt;/strong> :: Message-Passing Interface&lt;/li>
&lt;/ul>
&lt;h2 id='n'>N&lt;a href='#n' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>NVMe&lt;/strong> :: Non-Volatile Memory express&lt;/li>
&lt;/ul>
&lt;h2 id='o'>O&lt;a href='#o' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>OMP&lt;/strong> :: Open Multi-Processing&lt;/li>
&lt;li>&lt;strong>OOD&lt;/strong> :: Open OnDemand&lt;/li>
&lt;li>&lt;strong>OSS&lt;/strong> :: Object Storage Server&lt;/li>
&lt;li>&lt;strong>OST&lt;/strong> :: Object Storage Target&lt;/li>
&lt;/ul>
&lt;h2 id='p'>P&lt;a href='#p' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>PI&lt;/strong> :: Principal Investigator&lt;/li>
&lt;/ul>
&lt;h2 id='q'>Q&lt;a href='#q' class='anchor'>#&lt;/a>
&lt;/h2>&lt;h2 id='r'>R&lt;a href='#r' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>RAC&lt;/strong> :: Resource Allocation Competition&lt;/li>
&lt;li>&lt;strong>RDM&lt;/strong> :: Research Data Management&lt;/li>
&lt;/ul>
&lt;h2 id='s'>S&lt;a href='#s' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>SLURM&lt;/strong> :: Simple Linux Utility for Resource Management&lt;/li>
&lt;li>&lt;strong>SSH&lt;/strong> :: Secure SHell&lt;/li>
&lt;li>&lt;strong>SSL&lt;/strong> :: Secure Socket Layers, same as TLS&lt;/li>
&lt;/ul>
&lt;h2 id='t'>T&lt;a href='#t' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>TLS&lt;/strong> :: Transport Layer Security&lt;/li>
&lt;/ul>
&lt;h2 id='u'>U&lt;a href='#u' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>UM&lt;/strong> :: University of Manitoba&lt;/li>
&lt;/ul>
&lt;h2 id='v'>V&lt;a href='#v' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>VM&lt;/strong> :: Virtual Machine&lt;/li>
&lt;/ul>
&lt;h2 id='w'>W&lt;a href='#w' class='anchor'>#&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>WAN&lt;/strong> :: A Wide Area Network (WAN) is a network that spans several geographically distributed locations.&lt;/li>
&lt;/ul>
&lt;h2 id='x'>X&lt;a href='#x' class='anchor'>#&lt;/a>
&lt;/h2>&lt;h2 id='y'>Y&lt;a href='#y' class='anchor'>#&lt;/a>
&lt;/h2>&lt;h2 id='z'>Z&lt;a href='#z' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></description></item></channel></rss>