'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/grex-docs/docs/longread/',title:"Notes of Grex Changes",content:"Grex changes: Linux/SLURM update project. December 10-11, 2019\nIntroduction / Motivation Grex runs an old version of CentOS 6, which gets unsupported in 2020. The 2.6.x Linux kernel that is shipped with CentOS 6 does not support containerized workloads that require recent kernel features. The Lustre parallel filesystem client had some troubles that we were unable to resolve with CentOS 6 kernel version as well. Finally, the original Grex resource management software, Torque 2.5 and Moab7 are unable to properly schedule jobs that use newer MPI implementations (OpenMPI 2 and 3), which are increasingly common amongst HPC users. Therefore, using the power outages of October and December 2019, we have embarked on a rather ambitious project of updating entire Grex OS and software stack and scheduling to CentOS 7 and SLURM. This document outlines the changes and how they will affect Grex users.\nConnecting to Grex Grex is still using Westgrid accounting system (Portal/LDAP). To connect to Grex, one needs an active Compute Canada account and a Westgrid consortium account linked to it. You are likely to have one.\nHosts to connect to   During the outage, most of the Grex compute nodes, and all the contributed systems have been reprovisioned to CentOS 7. Public access login nodes, bison.westgrid.ca and tatanka.westgrid.ca (and their DNS alias, grex.westgrid.ca ) give the new CentOS 7.6 / SLURM / LMOD environment.\n  A test node, aurochs.westgrid.ca was preserved in the original CentOS 6 state, as well as about 600 cores of the Grex compute nodes. Logging in to aurochs for now allows users access to the original Grex environment (Torque, Moab, Tcl-Modules). We plan to eventually decommission the CentOS 6 partition when the CentOS 7 is debugged and fully in production.\n   Command line access via SSH clients Because Grex login nodes were reinstalled, SSH clients might give either a warning or an error for not recognizing the Grex host keys. Remove the offending keys from the file ~/.ssh/known_hosts mentioned in the error message.\nGraphical access with X2go The new CentOS 7 supports X2Go connections to use GUI applications. However, the GNOME Desktop environment that was default in CentOS 6 is no longer available! Please use either ICEWM or OPENBOX Desktop environment in the X2Go client to connect.\nBecause X2Go is using SSH under the hood to establish the connection, the advice above about ~/.ssh/known_hosts holds: delete the old SSH keys from it if you have connection problems. On Windows, it is often located under C:\\Users\\username\\.ssh\\known_hosts.\nStorage Lustre storage (/global/scratch) was updated to Lustre 2.10.8 on both server and client sides. We have ran our extensive tests and observed an increase of the write throughputs for large parallel I/O up to 3x. The change should be transparent to Grex users.\nSoftware, Interconnect, LMOD, CC/local stacks Interconnect and communications libraries Grex\u0026rsquo;s HPC interconnect hardware is no longer officially supported by commercial MLNX IB Verbs drivers. At the same time, open source projects like RDMA-Core and the new universal interconnect, UCX almost reached maturity and superior performance. Therefore, for the Grex software update, we have opted for vanilla Kernel drivers for our Infiniband, RDMA-Core for verbs userland libraries and UCX for the communication layer for the new OpenMPI versions, of which we support OpeMPI 3.1.4 (the new default) and 4.0.2 (An experimental, bleeding edge MPI v3 standard implementation that obsoletes many old MPI features). The previous default version OpenMPI 1.6.5 is still supported, and still uses IB Verbs from RDMA-core.\nUsers that have codes directly linked against any IBVerbs or other low level MLNX libraries, or having fixed RPATH to the old OpenMPI binaries will have to recompile their codes!\nSoftware Modules: LMOD This is a most major change! We have made obsolete the tried and tested flat, TCL-based software Modules system in favour of Lmod. Lmod is a new software Module system developed by Robert McLay at TACC. The main difference between Lmod and TCL-mod is that Lmod is built to have a hierarchical module structure: it ensures that no modules of the same “kind” can be loaded simultaneously; that there be no deep module paths like “intel/ompi/1.6.5” or “netcdf/pnetcdf-ompi165-nc433” . Rather, users will load “root” modules first and dependent modules second. That is, instead of TCL-mode’s way on the old system (loading OpenMPI for Intel-14 compilers:\nmodule load intel/14.0.2.144 module load intel/ompi/1.6.5  The new Lmod way would be:\nmodule load intel/14.0.2.144 module load ompi/1.6.5  The hierarchy ensures that only a good ompi/1.6.5 module that corresponds to the previously loaded Intel-14 compilers gets loaded. Note that swapping the compiler modules (Intel to GCC or Intel 14 to Intel 15) results in automatic reload of the dependent modules, if possible. Loading two versions of the same modules simultaneously is no longer possible! This largely removes the need for “module purge” command.\nIn the hierarchical module system, dependent modules are not visible for “module avail” unless their dependencies are loaded. To figure what modules are available use now “module spider” instead.\nFor more information and features, visit the official documentation of Lmod and/or Compute Canada documentation for modules .\nWe have tried to preserve the module names and paths closest to the original TCL-modules on Grex, whenever that was possible with the new hierarchy format. Note that the hierarchy is not “complete”: not every combination of software, compilers, and MPI exists on Grex, for practical reasons. Use \u0026ldquo;module spider /\u0026rdquo; to check what built variants are there. Send us a request to support@computecanada.ca if there is any missing software/toolchain combination you may want to use.\nSoftware Modules: Stacks , CVMFS, Defaults One of the nice features of Lmod is its ability to maintain several software stacks at once. We have used it and now provide the following software stacks modules. The modules are “sticky” which means, one of them is always loaded. (Use module avail to see them). GrexEnv (default), OldEnv and CCEnv .\n  GrexEnv is the current Grex default software environment, (mostly) recompiled to CentOS-7. Note that we do not load Intel compilers and OpenMPI modules by default anymore, in contrast to the old environment of Grex. The recompilation is mostly done, but not completely: should you miss a software item, please contact us!\n  OldEnv is an Lmod-ized version of the old CentOS-6 modules, should you need it for compatibility reasons. The software (except OpenMPI) is as it was before (not recompiled). It may run on CentOS-7.\n  CCEnv is the full Compute Canada software stack, brought to Grex via Cern Virtual Filesystem (CVMFS). We use the “sse3” software stack of Compute Canada because this is the one that suits our older hardware. Note that the default CC environment (StdEnv, nixpkgs) are NOT loaded by default on Grex! In order to access the CC software, they have to be loaded after the CCEnv as follows. (Note that first load of the CC modules and software items might take a few seconds! It is probably a good practice to first access a CC software binary in a small interactive job to warm the local cache).\n  module load CCEnv module load StdEnv/2016.4 nixpkgs/16.09 arch/sse3 module avail  The CC software stack is documented at Compute Canada wiki: Available_software page. A caveat: it is in general impossible to isolate and containerize high-performance software completely, so not all CC CVMFS software might work on Grex: the most troublesome parts are MPI and CUDA software that rely on low level hardware drivers and direct memory access. Threaded SMP software and serial software we expect to run without issues.\nNote that for Contributed systems it might be beneficial to use different architecture than the default SSE3, which is available at loading corresponding arch/ module.\nRunning jobs, migration to SLURM CentOS-7 provides a different method of process isolation (cgroups). It also allows for better support of containers such as Singularity which (hopefully) can now be run in user namespaces. Incidentally, the cgroups are not supported well by any Torque/Moab scheduler version that was compatible with our current Moab software license. Torque is not known to support the new process management interface (PMIX) that increasingly becoming a standard for MPI libraries either. Therefore, we had little choice but to migrate our batch workload management from Torque to SLURM. It wil also make Grex more similar to Compute Canada machines (which are documented here: Running_jobs ).\nUnlike Torque which is a monolithic, well engineered piece of software that we knew well, SLURM is a very modular, plugin-based ecosystem which is new to us. Therefore, initially we will enable only short walltimes to test our SLURM configuration (48h) before going to full production.\nTorque wrappers To make life easier for PBS/Torque users, SLURM developers provided most of the Torque commands as wrappers over SLURM commands. So with some luck, you can continue using qsub, qstat, pbsnodes etc. keeping in mind correspondence between Torque’s queues and SLURM partitions. For example:\nsbatch \u0026ndash;allocation=abc-668-aa \u0026ndash;partition=compute my.job  Can be called using Torque syntax as:\nqsub -A abc-668-aa -q compute my.job  Presently, and unlike old Grex, there is no automatic queue/partition/QOS assignment based on jobs resource request. The option --partition must be selected explicitly for using High Memory and contributed nodes, if you have access to the later. By default, jobs go into “compute” partition comprised of the original 48 GB, 12 Nehalem CPUs nodes. The command qstat -q now actually lists partitions (which it thinks being queues).\nInteractive Jobs As before, and as usual for HPC systems, we ask users to limit their work on login nodes to code development and small, infrequent test runs and to use interactive jobs for longer and/or heavier interactive workloads.\nInteractive jobs can be done using SLURM salloc command as well as using qsub -I . The one limitation for the later is there for graphical jobs: the latest and greatest SLURM 19.05 supports --x11 flags natively for salloc, but does not support yet corresponding qsub -I -X flags for the Torque wrapper. So graphical interactive jobs are only possible with salloc.\nAnother limitation of qsub is the syntax: SLURM does distinguish between --ntasks= for MPI parallel jobs and --cpus-per-task= for SMP threaded jobs; while for qsub it is all the same for its -l nodes=1:ppn= syntax. Therefore, the SMP threaded applications might not be placed correctly with the jobs submitted with qsub.\nBatch Jobs. There are two changes: need to specify partitions explicitly and short maximal walltimes during initial Grex testing. Resource allocations for Grex RAC 2019-2020 will be implemented in the SLURM scheduler. Two useful commands: sshare (shows your groups fairshare parameters) and seff (shows efficiency of your jobs: CPU and memory usage) might help with effective usage of your allocation.\nIn general, Compute Canada documentation on running SLURM jobs can be followed, obviously with the exception of different core count on Grex nodes. Presently, we schedule on Grex by CPU core (as opposed to by-node) so whole-node jobs do not get any particular prioritization.\nFor the local scratch directories, $TMPDIR should be used in batch scripts on Grex (as opposed to Compute Canada where $SLURM_TMPDIR is defined). Thus, for using software from Compute Canada stack that has references to SLURM_TMPDIR hardcoded in some scripts it relies on (an example being GAMESS-US) the following line should be added on Grex to your job scripts:\nexport SLURM_TMPDIR=$TMPDIR  "}),a.add({id:1,href:'/grex-docs/docs/computecanada/',title:"Accessing Compute Canada resources",content:"Introduction Compute Canada is a Canadian National digital research infrastructore (DRI) organization. It provides the eligible researchers with the research computing resources such as several High-performance computing (HPC) systems with a large curated HPC software stack, private OpenStack cloud, and Globus data transfer software.\nCompute Canada also maintains a user authentication service and usage database called CCDB. On Grex, we rely on CCDB for accessing our system. The first step to get started with Grex is to register for a Compute Canada account at the CCDB website.\nThe Alliance (Compute Canada) on the Web  Digital Research Alliance of Canada official website  The Alliance database CCDB  The Alliance user documentation Wiki  Status page for the Alliance clusters   Getting a Compute Canada Account University of Manitoba Faculty memebers are eligible for a getting Compute Canada account. Once it is obtained, they can manage accounts of their group members in CCDB.\nIf you are a postdoc, a student or an external collaborator, you can apply for an account as part of the Faculty\u0026rsquo;s group. You will need your PI\u0026rsquo;s CCDB role identificator code (CCRI). For more information, please follow this guide .\nThe account will allow you to manage your roles and groups in CCDB, to access National HPC and Cloud machines, as well as few other services such as Globus and NextCloud .\nSince we have switched Grex to CCDB credentials in late 2019, the CCDB account is both necessary and sufficient to access Grex.\nThe old WestGrid accounts are no longer needed, nor working on Grex.  Regional Partners Compute Canada is also a Federation of four Regions: Westgrid, which consists of institutional members belonging to Western Canada, Compute Ontario, Calcul Quebec and AceNET which is a consortium of universities of Maritime provinces.\n WestGrid \u0026ldquo;Training material; the official website is down\u0026rdquo;) Compute Ontario  Calcul Quebec  AceNET   University of Manitoba is a Westgrid member insitution, and provides a site for Compute Canada support staff that support usage of the National DRI by Manitoba researchers and their collaborators.\nThe single point of contact for Compute Canada support is: support@tech.alliancecan.ca  "}),a.add({id:2,href:'/grex-docs/docs/grex/running/batch/',title:"Batch jobs",content:"Batch jobs HPC systems usually are \u0026ldquo;clusters\u0026rdquo; of many compute nodes, which are joined by an interconnect, and under control of a resource management software. From the users' point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so called \u0026ldquo;jobs\u0026rdquo; to a \u0026ldquo;batch queue\u0026rdquo;. A subset of the available resources of the HPC machine is allocated to each of the users batch jobs, and they run without any need for user intervention as soon as the resources become available.\nThe job placement, usage monitoring and job accounting are done via a special software, the HPC scheduler. This is an often under-appreciated automation that makes usage efficient and saves a lot of work on part of the user. However, using HPC is hard in a sense that users have to make effort in order to figure out what are the available resources on an HPC cluster, and what is the efficient way of requesting the resoures for their jobs. Asking for too many resources might be wasteful both in preventing others of using them and in making for a longer queuing time.\nThe resources (\u0026ldquo;tractable resources\u0026rdquo; in SLURM speak) are CPU time, memory, and GPU time. Generic resources can be software licenses, \u0026hellip; etc. Requesting resources is done via command line options to job submission commands sbatch and salloc, or via special comment lines (starting with #SBATCH) in job scripts. There are also options to control job placement such as partitions.\nThere are default values for the resources which are taken when you do not specify the resource limit. Note that the default values are, as a rule, quite small. On Grex, the default values are set as follow: 3 hours of walltime, 256mb of memory per CPU. In most of the cases, it is better to have an explicit request of an appropriate resource limit rather than using the default.\nWe ask our users to be fair and considerate and do not allow for deliberate waste of resources (such as running serial jobs on more than one CPU core, or running CPU-only calculations on GPU nodes).\nThere are certain scheduling policies in place to prevent the cluster from being swamped by a single user. In particular, the MAXPS / GrpRunMins limit disfavours asking for many CPU cores for long walltimes, a MaxCPU limits restricts number of CPU cores used, and there are limits on number of user\u0026rsquo;s jobs in the system and number of array job elements, as described below.\nBatch job policies The following policies are implemented on Grex:\n The default walltime is 3 hours (equivalent to: --time=3:00:00). The default amount of memory per processor (--mem-per-cpu=) is 256 mb. Memory limits are enforced, so an accurate estimate of memory resource (either in form of --mem= or --mem-per-cpu=) should be provided. The maximum walltime is 21 days on compute and skylake partitions, 14 days on largemem partition. The maximum number of processor-minutes for all currently running jobs of a group without a RAC is 4 M. The maximum number of jobs that a user may have queued to run is 4000. The maximum size of an array job is 2000. Users without a RAC award are allowed to simultaneously use up to 400 CPU cores per accounting group. There are limits on number of GPUs that can be used on contributed hardware (1 GPU per job).   Note that you can see some information about the partitions by running the custom script partition-list from your terminal:\npartition-list  Typical Batch job cases Any batch job is submitted with sbatch command. Batch jobs are usually shell (BASH, etc.) scripts wrapping around the invocation of a code. The comments on top of the script that start with #SBATCH are interpreted by the SLURM scheduler as options for resource requests:\n --ntasks= : specifies number of tasks (MPI processes) per job. --nodes= : specifies number of nodes (servers) per job. --ntasks-per-node= : specifies number of tasks (MPI processes) per node. --cpus-per-task= : specifies number of threads per task. This parameter should not exceed the number of physical cores per node. --mem-per-cpu= : specifies memory per task (or thread?). --mem= : specifies memory per node. --gpus= : specifies number of GPUs per job. There are also --gpus-per-XXX and --XXX-per-gpu resource specs. --time- : specifies walltime in format DD-HH:MM:SS --qos= : specifies a QOS by its name (Not to be used on Grex!). --partition= : specifies a partiton by its name (very much used on Grex!).   Assuming the name of myfile.slurm (the name or the extension does not matter, it can can be called afile.job, otherjob.sh, \u0026hellip; etc.), a job is submitted with the command:\nsbatch myfile.slurm  or\nsbatch [+ some options] myfile.slurm  Some options like --partition=compute could be invoked at sumbmission time.\nRefer to the official SLURM documentation and/or man sbatch for the available options. Below we provide examples for typical cases of SLURM jobs.\nSerial Jobs The simplest kind of job is a serial job when one compute process runs in a sequential fashion. Naturally, such job can utilize only a single CPU core: even large parallel supercomputers as a rule do not parallelize binary codes automatically. So the CPU request for a serial job is always 1, which is the default; the other resources can be wall time and memory. SLURM has two ways of specifying the later: memory per cpu core (--mem-per-cpu=) and total memory per node (--mem=). It is more logical to use per-core memory always; except in case of the whole-node jobs when special value --mem=0 gives all the available memory for the allocated node. An example script (for 1 CPU, wall time of 30 minutes and a memory of 2500M) is provided below.\n#!/bin/bash #SBATCH --time=0-0:30:00 #SBATCH --mem=2500M #SBATCH --job-name=\u0026#34;Serial-Job-Test\u0026#34; # Script for running serial program: your_program echo \u0026#34;Current working directory is `pwd`\u0026#34; # Load modules if needed: echo \u0026#34;Starting run at: `date`\u0026#34; ./your_program \u0026lt;+options or arguments if any\u0026gt; echo \u0026#34;Job finished with exit code $?at: `date`\u0026#34;  An important special case of serial jobs is high-throughput computing: jobs are serial because they are too short to parallelize them, however there are very many such jobs per research project. The case of embarassingly parallel computations like some of the Monte Carlo simulations are often High Throughput Computing (HTC).\n Serial jobs that have regularely named inputs and run more than a few minutes each best be specified as Array Jobs (see below). Serial jobs that are great in numbers, and run less than a few minutes each, better be joined into a task farm running within a single larger job using tools like GLOST, GNU Parallel or a workflow engine like QDO.   An example of GLOST job is under MPI jobs section (see below).\nSMP / threaded / single node jobs A next kind of job is multi-threaded, shared memory or or single-node parallel jobs. Often these jobs are for Symmetric Multiprocessing (SMP) codes that can use more than one CPU on a given node to speed up the calculations. However, SMP/multithreaded jobs rely on some form of inter-process communication (shared memory, \u0026hellip; etc.) that limits them to the CPU cores within just a single server. They cannot scale across multiple compute nodes. Examples are OpenMP, pthreads, Java codes, etc. Gaussian and PSI4 are SMP codes; threaded BLAS/LAPACK routines from MKL (inside NumPY) can utilize multiple threads, \u0026hellip; etc. Note that this kind of programs do not scle very well when increasing the number of threads. We recommend to our users to run a benchmark to see how your programs scale with number of threads to define the combination or a set of threads for better performance.\nThus, from the point of view of the SMP/threaded jobs resources request, the following considerations are important:\n asking always only a single compute node (--nodes=1 --ntasks=1) node asking for several CPU cores on it per job, up to the maximum number of CPU cores per node (--cpus-per-task=N) where N should not exceed the total physical cores available on the node. Depending on the partition, you may choose N up to 12 compute, up to 52 on skylake, up to 40 on largemem, \u0026hellip; etc. making sure that the total memory asked for do not exceed the memory available on the node. making sure that the code would use exactly the number of CPU cores allocated to the job, to prevent waste or congestion of the resources.   In SLURM, it makes a difference whether you ask for \u0026lsquo;parallel tasks (--ntasks)\u0026rsquo; or \u0026lsquo;threads (--cpus-per-task)\u0026rsquo; ; the threads should not be isolated from each other (because they might need to use shared memory!) but the tasks are isolated to each own \u0026ldquo;cgroup\u0026rdquo;.\nAn enviromental variable ${SLURM_CPUS_PER_TASK} is set in the job, so you can set an appropriate parameter of your code to the same value.\nFor OpenMP, it would be done like:\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}  For MKL it is MKL_NUM_THREADS, for Julia -- JULIA_NUM_THREADS, for Java -Xfixme parameter.\n#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --nodes=1 #SBATCH --ntask-per-node=1 #SBATCH --cpus-per-task=12 #SBATCH --mem=0 #SBATCH --partition=compute #SBATCH --job-name=\u0026#34;OMP-Job-Test\u0026#34; # An example of an OpenMP threaded job that takes a whole \u0026#34;old\u0026#34; Grex node for 8 hours.  export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; ./your-openmp.x input.dat \u0026gt; output.log # OpenMP codes are usually homegrown. echo \u0026#34;Job finished with exit code $?at: `date`\u0026#34;  Note that the above example request whole node\u0026rsquo;s memory with --mem=0 because the node is allocated to the job fully due to all the CPUs anyways. It is easier to use the --mem syntax for SMP jobs because typically the memory is shared between threads (i.e., the amount of memory used does not change with the number of SMP threads). Note, however, that the memory request should be reasonably \u0026ldquo;efficient\u0026rdquo; if possible.\nGPU jobs The GPU jobs would usually be similar to SMP/threaded jobs, with the following differences:\n  The GPU jobs should run on the nodes that have GPU hardware, which means you\u0026rsquo;d want always to specify --partition=gpu or --partition=stamps-b or livi-b.\n  SLURM on Grex uses so called \u0026ldquo;GTRES\u0026rdquo; plugin for scheduling GPU jobs, which means that the request syntax in the form --gpus=N or --gpus-per-node=N or --gpus-per-task=N is used.\n   How many GPUs to ask for?\nGrex, at the moment, does not have GPU-direct MPI enabled, which means that most of the jobs would be single-node. The GPU nodes in either gpu (two nodes there, 32GB V100s) or stamps-b (three nodes, 16GB V100s) partition have 4 V100 GPUs, 32 Intel 52xx CPUs and 192GB of CPU memory. (There also is the livi-b partition with a large single 16x v100 GPU server, but it seldom has all 16GPUs idle). So, asking 1 to 4 GPUs, one node, and 6-8 CPUs per GPU with an appropriate amount of RAM (4-8GB) per job would be a good starting point.\nNote that V100 is a fairly large GPU for most of the jobs, and for good utilization of the GPU resources available on Grex, it is a good idea to start with single GPU, and then try if the code actually is able to saturate it with load. Many codes cannot scale to utilize more than one GPU, and few codes can utilize more than two of them.\n#!/bin/bash #SBATCH --ntasks=1 --cpus-per-task=6 #SBATCH --time=0-12:00:00 --mem-per-cpu=6000M #SBATCH --job-name=genomics-test #SBATCH --gpus=1 --partition=stamps-b # adjust the resource requests above to your needs. # Example of loading modules, CUDA: module load gcc/4.8 cuda/10.2 export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; nvidia-smi guppy_basecaller -x auto --gpu_runners_per_device 6 -i Fast5 -s GuppyFast5 -c dna_r9.4.1_450bps_hac.cfg echo \u0026#34;Job finished with exit code $?at: `date`\u0026#34;  The above script (if called say, gpu.job) can be submitted with the usual command:\nsbatch gpu.job  Distributed, massively parallel jobs Parallel jobs that can spawn multiple servers are the most scalable ones, because they are not limited by the number of CPUs or memory per node. Running many parallel tasks across more than one node requires some inter-node communication (which as a rule is slower than shared memory within one server). In HPC, high speed interconnect and specialized RDMA-aware communication libraries make distributed parallel computational very scalable. Grex uses Infiniband interconnect.\nMost often (but not always), parallel programs are built upon a low-level message passing library called MPI. Refer to the Software section for the informations of the parallel libraries on Grex. Examples of distributed parallel codes are GAMESS-US, ORCA , LAMMPS , VASP , \u0026hellip; etc.\nThe distributed parallel jobs can be placed across their compute nodes in several ways (i.e., how many parallel tasks per compute node?). Thus SLURM resource request syntax alows to specify the required layout of nodes/tasks (or nodes/tasks/threads, or even nodes/tasks/GPUs since hybrid MPI+OpenMP and MPI+GPU programs exist). A consideration about the layout is a tradeoff between making program to work faster (and sometimes to work correctly at all) and making the scheduler\u0026rsquo;s work easier.\nA well written MPI software theoretically should not care how the tasks are distributed across how many physical compute nodes. Thus, SLURM\u0026rsquo;s --ntasks= request (similar to the old Torque procs=) specified without --nodes would work and make the scheduling easier.\nA note on process starting:\nSince MPI jobs are distributed, there should be a mechanism to start the compute processes across all of the nodes (or CPUs) allocated for it. The mechanism should know which nodes to use, and how many. Most modern MPI implementations \u0026ldquo;tightly integrate\u0026rdquo; with SLURM, so they will get this informaton automatically via a Process Management Interface (PMI). SLURM provides its own job starting command called srun . Most MPI implementations also provide their own job spawner commands, usually called mpiexec or mpirun . These are specific to each MPI vendor/kind and not well standartised, and differ in the support of SLURM.\nFor example, OpenMPI (the default, supported MPI implementation) on Grex is compiled against PMIx (3.x, 4.x) or PMI1 (1.6.5). So it is preferrable to use srun instead of mpiexec to kickstart the MPI proecesses, because srun would use PMI.\nFor Intel MPI (another MPI, also available on Grex and required by some of the binary codes, like ANSYS or ADF), srun sometimes may not work, but PMI1 can be used with mpiexec.hydra by setting the following environmemt variable:\nexport I_PMI_LIBRARY=/opt/slurm/lib/libpmi.so  #!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --mem-per-cpu=2500M #SBATCH --ntasks=32 #SBATCH --job-name=\u0026#34;Espresso-Job\u0026#34; # A example of an MPI parallel that takes 32 cores on Grex for 8 hours.  # Load the modules: module load intel/15.0.5.223 ompi/3.1.4 espresso/6.3.1 echo \u0026#34;Starting run at: `date`\u0026#34; srun pw.x -in MyFile.scf.in \u0026gt; Myfile.scf.log echo \u0026#34;Job finished with exit code $?at: `date`\u0026#34;  However, in practice there are cases when layout should be more restrictive. If the software code assumes equal distribution of processes per node, the request should be --nodes=N --ntasks-per-node=M. A similar case is MPMD codes (Like NWCHem or GAMESS or OpenMolcas) that have some of the processes doing computation and some communication functions, and therefore requiring at least two tasks running per each node.\nFor some codes, especially for large parallel jobs with intensive communication between tasks there can be performance differences due to memory and interconnect bandwidths, depending on whether the same number of parallel tasks is compacted on few nodes or spread across many of them. Find an example of the job below.\n#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --mem-per-cpu=4000M #SBATCH --ntasks-per-node=8 --nodes=4 #SBATCH --job-name=\u0026#34;NWchem-Job\u0026#34; # An example of an MPI parallel that takes 32 cores  # across 4 grex Grex nodes for 8 hours.  module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1 # Uncomment/Change these in case you want to use custom basis sets NWCHEMROOT=/global/software/cent7/nwchem/6.8.1-intel15-ompi314 export NWCHEM_NWPW_LIBRARY=${NWCHEMROOT}/data/libraryps export NWCHEM_BASIS_LIBRARY={NWCHEMROOT}/data/libraries # In most cases SCRATCH_DIR would be on local nodes scratch # While results are in the same directory export NWCHEM_SCRATCH_DIR=$TMPDIR export NWCHEM_PERMANENT_DIR=`pwd` # Optional memory setting; note that this one or the one in your code # must match the #SBATCH -l mem= value ! export NWCHEM_MEMORY_TOTAL=1000000000 # 12000 MB, double precision words only export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; srun nwchem dft_feco5.nw \u0026gt; dft_feco5.$SLURM_JOBID.log echo \u0026#34;Job finished with exit code $?at: `date`\u0026#34;  OpenMPI OpenMPI is the default MPI implementation for Grex (and ComputeCanada). The modules for it on Grex are called ompi . The MPI example scripts above are all OpenMPI based. The old version 1.6.5 is there for compatibility reasons with older software; most users should use 3.1.x or 4.x.x versions. Using srun is recommended in all cases.\nIntel MPI For applications using IntelMPI (impi modules on Grex, or Intel-MPI based software from Compute Canada CVMFS software stack), a few environment variables have to be set. The following link explains it: Using SLURM with PMI .\nThe JLab documentation example shows an example of SLURM script with IntelMPI .\nOther MPIs Finally, some canned codes like ANSYS or StatCCM+ would use a vendor-specific MPI implementation that would not tightly integrate with our scheduler\u0026rsquo;s process to CPU core placement. In that case, several whole nodes (that is, with number of tasks equal to the node\u0026rsquo;s number of CPU cores) should be requested to prevent the impact on other jobs with resource congestion.\nSuch codes will also require a nodelist (machinefile) file obtained from SLURM and provided to them in their own format.\nCompute Canada\u0026rsquo;s slurm_hl2hl.py script makes this easier (see CC StarCCM+ or CC ANSYS documentation ). The script slurm_hl2hl.py is already available on Grex.\nslurm_hl2hl.py --format STAR-CCM+ \u0026gt; machinefile  Array jobs Array jobs allow for submitting many similar jobs \u0026ldquo;in one blow\u0026rdquo;. It saves users work on job sumbission, and also makes SLURM scheduler more efficient in scheduling the array jobs because it would know they are the same with respect to size, expected walltime etc.\nArray jobs work most naturally when a single code has to be applied for paramter sweep and/or to a large number of input files that are reguraly named, for example as: test1.in, test2.in, \u0026hellip; test99.in\nThen, a single job script with #SBATCH --array=1,99 can be used to submit the 99 jobs.\nIn order to distinguish between the input files, within each of the jobs at run time, you would have to obtain a value for the array index. Which is set by SLURM as ${SLURM_ARRAY_TASK_ID} environment variable. The call to the code on a particular input will then be like:\n./my_code test${SLURM_ARRAY_TASK_ID}.in   This way each of the array element jobs can distinguish their own portion of the work to do. A real life example is below; it attempts to run all of the Gaussian standard tests which have names of the format test0001.com, test0002.com, .. test1204.com, etc. Note the printf trick to deal with trailing zeroes in the input names.\n#!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1000MB #SBATCH --job-name=\u0026#34;G16-tests\u0026#34; #SBATCH --array=1-1204 echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Running on `hostname`\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; # Set up the Gaussian environment using the module command: module load gaussian/g16.c01 # Run g16 on an array job element id=`printf \u0026#34;%04d\u0026#34; $SLURM_ARRAY_TASK_ID` v=test${id}.com w=`basename $v .com` g16 \u0026lt; $v \u0026gt; ${w}.${SLURM_JOBID}.out echo \u0026#34;Job finished with exit code $?at: `date`\u0026#34;  There are limits on how large array jobs can be (see our scheduling policies): the maximal number of elements in job array, as well as the maximal number of jobs that can be submitted by a user.\nUsing CC CVMFS software As explained in more detail in the software/Modules documentation, we provide Compute Canada\u0026rsquo;s software environment. Most of it can run out of the box by just specifying corresponding module.\nThere are some caveats:\n  Some of the Compute Canada software might have hardcoded environmental variables that exists only on these systems. An example being SLURM_TMPDIR. On Grex, add export SLURM_TMPDIR=$TMPDIR to your job scripts.\n  In general, it is hard to containerize HPC. So the software that requires low-lewel hardware/device drivers access (OpenMPI, CUDA) may have problems when running on non-CC systems. Newer version of OpenMPI (3.1.x) seems to be more portable for using the PMIx job starting mechanism.\n  \u0026ldquo;Restricted\u0026rdquo; (commercial) software\u0026rsquo;s binaries are not distributed by Compute Canada CVMFS due to the obvious licensing issues. It has to be installed locally on Grex.\n   Having said that, module load CCEnv gives the right software environment to be run on Grex for a vast majority of threaded and serial software items from CC softvare stack. See discussion about MPI-parallel jobs below.\nBecause of the distributed nature of CVMFS, it might take time to download a program or library or data file. It would probably make sense to first access it interactively or from an interactive job to warm the CVMFS local cache, to avoid job failures due to the delay.\nBelow is an example of an R serial job that uses quite a few packages from Compute Canada software stack.\n#!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-72:00:00 #SBATCH --job-name=\u0026#34;R-gdal-jags-bench\u0026#34; cd ${SLURM_SUBMIT_DIR} # Load the modules:  module load CCEnv module load nixpkgs/16.09 gcc/5.4.0 module load r/3.5.2 jags/4.3.0 geos/3.6.1 gdal/2.2.1 export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; R --vanilla \u0026lt; Benchmark.R \u0026amp;\u0026gt; benchmark.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Users of contributed systems which are newer than the original Grex nodes might want to switch to arch/avx2 or arch/avx512 from the default arch/sse3.\nUsing CC CVMFS software that is MPI-based. We have found that the recent Compute Canada toolchains that use OpenMPI 3.1.x work on Grex without any changes (that is, with srun). Therefore for OpenMPI based applications, we recommend to load Compute Canada\u0026rsquo;s software that depends on the recent toolchains, 2018.3 or later (Intel 2018 compilers, GCC 7.3 compilers and openmpi/3.1.2).\nFor example, the module commands below would load the Intel/OpenMPI 3.1.2 toochain-based environment:\nmodule load CCEnv\nmodule load StdEnv/2018.3\n Below is an arbitrarily chosen IMB benchmark result for MPI1 on Grex, the sendrecv tests using two processes on two nodes with several MPI implementations (CC means MPI coming from the ComputeCanada stack, Grex means compiled locally on Grex).\nYou can see that differences in performance between OpenMPI 3.1.x from CC stack and Grex are minor for this benchmark, even vithout attemting at any local tuning for the CC OpenMPI.\nUsing New 2020 CC CVMFS Stack Since Spring 2021, Compute Canada had updated the default software stack on ther CVMFS distribution to StdEnv/2020 and gentoo. This version will not run on legacy Grex partitions (compute) at all, because it requires AVX2 CPU architecture. It will work as expected on all new GPU and CPU nodes (skylake, largemem, gpu and contributed systems).\n"}),a.add({id:3,href:'/grex-docs/docs/grex/connecting/ssh/',title:"Connecting with SSH",content:"SSH Most of the work on shared HPC computing systems is done via Linux command line / shell. To connect, in a secure manner, to a remote Linux system, you would like to use SSH protocol. You will need to have:\n access to Internet that lets SSH ports open a user account on Grex (presently, it is an Alliance (formerly known as Compute Canada) Account!) and an SSH client for your operating system   If you are not sure what your account on Grex is, check Getting Access . You will also need the DNS name of Grex Which is grex.hpc.umanitoba.ca.\nSSH clients MacOS  SSH clients for MacOS MacOS X has a built-in OpenSSH command line client. It also has a full-fledged UNIX shell. Therefore, using SSH under MacOS is not different from Linux. In any terminal, ssh (as well as scp , sftp ) just works with one caveat: for the support of X11 tunneling, some of the MacOS X versions would require the XQuartz package installed.\nssh -Y username@grex.hpc.umanitoba.ca  You can manage your keys (adding key pairs, ediding known_hosts etc.) in the $HOME/.ssh directory. Compute Canada has several documentation pages on managing SSH keys and creating SSH tunnels\n Linux  SSH clients for Linux Linux provides the command line SSH package, OpenSSH, which is installed by default in most of the Linux distributions. If not, or you are using a very minimal Linux installation, use your package manager to install OpenSSH package. In any terminal window ssh (as well as scp , sftp ) commands should work. To connect to Grex, use:\nssh -Y username@grex.hpc.umanitoba.ca  You can manage your keys (adding key pairs, ediding known_hosts etc.) in the $HOME/.ssh directory. Compute Canada has several documentation pages on managing SSH keys and [creating SSH tunnels](https://docs.computecanada.ca/wiki/SSH_tunnelling\n Windows  SSH clients for Windows Windows has a very diverse infrastructure for SSH (and Linux support in general). You would like to pick one of the options below and connect to grex.hpc.umanitoba.ca with your Westgrid username ans password.\nPutty, WinSCP and VCXsrv The (probably the most popular) free software combination to work under Windows are:\n Putty SSH client : download PuTTY WinSCP graphical SFTP client: download WinSCP A free X11 server for Windows: download VCXSrv   WinSCP interacts with PuTTY, so you can configure it to open SSH terminal windows from WinSCP client. For X11 forwarding, make sure the \u0026ldquo;X11 tunneling\u0026rdquo; is enabled in PuTTY\u0026rsquo;s session settings, and VCXSrv is running (it sits in the system tray and does nothing unless you start a graphical X11 appication).\nCompute Canada has a PuTTY documentation page which has some useful screenshots.\nMobaXterm There is a quite popular package: MobaXterm. It is not open source, but has a limited free version MobaXterm.\nPlease check out Compute Canada\u0026rsquo;s documentation on MobaXterm here\nAll Windows versions, CygWin shell There is a way to use Linux command shell tools under Windows. Cygwin. When openssh package is installed, you can use OpenSSH\u0026rsquo;s command line tools like ssh, scp and sftp as if you were under Linux:\nssh -Y username@grex.hpc.umanitoba.ca  Windows 10, WSL subsystem There is a Linux Subsystem for Windows which allows you running a containerized instance of Lunux (Ubuntu, for example) from under Windows 10. Refer to MS documentation on enabling WSL. Then, you will have same OpenSSH under Linux.\nIt is actually possible to run X11 applications from WSL as well; you would need to get VCXSrv running, on the Windows side, and DISPLAY variable set on the WSL SLinux side.\nWindows 10, native OpenSSH package Actually, some of the Windows 10 version have OpenSSH as a standalone package. Refer to corresponsing MS documentation on enabling OpenSSH. If it works with your version of Windows 10, you should have the OpenSSH command line tools like ssh, scp and sftp in Windows command line, as if you were under Linux.\nthe original SSHSecureShell client The original SSHSecureShell and SecureFTP client from www.ssh.fi is now obsolete. It is unmaintained since 2001 and may not work with newest SSH keys/encryption mechanisms and does not have any security updates for the last 20 years. We do not support it and advise users to switch to one of the others more modern clients listed above.\n  Using command line What to do after you connect? You will be facing a Linux shell, most likely BASH. There is a plenty of online documentation on how to use it, HPC Carpentries , Compute Canada\u0026rsquo;s SSH documentation page , Bash Guide for Beginners and simple googling for the commands.\nYou would probably like to explore software via Modules , and learning how to submit jobs .\n"}),a.add({id:4,href:'/grex-docs/docs/grex/',title:"Grex HPC QuickStart",content:"Grex Grex is an UManitoba High Performance Computing (HPC) system, first put in production in early 2011 as part of WestGrid consortium. Now it is owned and operated by University of Manitoba. Grex is accessible only for UManitoba users and their collaborators.\nA Very Quick Start guide   Create an account on CCDB . You will need an institutional Email address. If you are a sponsored user, you\u0026rsquo;d want to ask your PI for his/her CCRI code {Compute Canada Role Identifier}. For a detailed procedure, visit the page Apply for an account .\n  Wait for half a day. While waiting, install an SSH client, and SFTP client for your operating system.\n  Connect to grex.hpc.umanitoba.ca with SSH, using your username/password from step 1.\n  Make a sample job script, call it, for example, sleep.job . The job script is a text file that has a special syntax to be recognized by SLURM. You can use the editor nano , or any other right on Grex SSH prompt (vim, emacs, pico, \u0026hellip; etc); you can also create the script file on your machine and upload to Grex using your SFTP client or scp.\n  #!/bin/bash #SBATCH --ntasks=1 --cpus-per-task=1 #SBATCH --time=00:01 --mem-per-cpu=500M echo \u0026#34;Hello world! will sleep for 10 seconds\u0026#34; time sleep 10 echo \u0026#34;all done\u0026#34;   Submit the script using sbatch command, to the compute partition using:  sbatch \u0026ndash;partition=compute sleep.job   Wait until the job finishes; you can monitor queue\u0026rsquo;s state with the \u0026lsquo;sq\u0026rsquo; command. When the job finishes, a file slurm-NNNN.out should be created in the same directory.\n  Download the output slurm-NNNN.out from grex.westgrid.ca to your local machine using your SFTP client.\n  Congratulations, you have just ran your first HPC-style batch job. This is the general workflow, more or less; you\u0026rsquo;d just want to substitute the sleep command to something useful, like ./your-code.x your-input.dat .\n  More information on this website Check out Getting an ccount , Moving Data and Running jobs for general information. Software pages might have information specific to running particular software items . OpenOndemand pages explain how to use the new Grex\u0026rsquo;s Web portal.\n"}),a.add({id:5,href:'/grex-docs/docs/grex/connecting/x2go/',title:"Connecting with X2go",content:"Graphical user interface access with X2Go Linux (and UNIX) have a graphical user interface framework, which is called X11, X-Window system . It is possible to use remote X11 applications with the combination of SSH client with X11-tunnelling enabled and a local X11-server running. However, this way is often quite slow and painful, especially over WAN networks, where latencies of the network really impair user experience.\nLuckily, there is a solution for this, which is called NX. NX software on client and server sides caches and compresses X11 traffic, leading to a great improvement of the performance of X11 applications. A free NX-based remote desktop solution exists, and is called X2Go .\nOn Grex, we support X2Go since 2015; that is, we run an X2Go server on Grex login nodes. So if you have a valid Grex account, and an X2Go client installed on your local machine, you can connect to Grex and use a remote Linux desktop to run your favourite GUI applications.\nSince X2Go runs over an encrypted SSH connection, it does not require anything else to access Grex. If you have SSH command line access working, and have X2Go client working, it should be enough to get you started.\nX2go clients and sesions The X2Go authors provide clients for MacOS X, Linux and Windows operating systems: download X2go .\nThere are also alternative X2Go clients (PyHoca CLI and GUI, etc.) that you could try, but we will not cover them here.\nAfter installing the X2Go client, you\u0026rsquo;d need to start it and create a \u0026ldquo;New Session\u0026rdquo; by clicking the corresponding icon.\nFor now, there is no load balancing support for connections: while connecting to the host address grex.hpc.umanitoba.ca will work, session suspend/resume functionality might require specifying connection to a physical Grex login node explicitly, using either of tatanka.hpc.umanitoba.ca or bison.hpc.umanitoba.ca correspondingly in the Host field. (You can also create two sessions, one for tatanka and another for bison.)\nThe same username should be used as for SSH text based connections in the Login field. It is also possible to provide an SSH key instead of the password.\nWhen creating a new Session, a \u0026ldquo;Desktop Environment\u0026rdquo; needs to be selected in the \u0026ldquo;Session Type\u0026rdquo; menu Not all DE\u0026rsquo;s that are listed in this X2go menu are available on Grex. We support the following Linux Desktop environments :\n OPENBOX : a lightweight DE (Desktop Environment) IceWM : a lightweight DE that looks like Windows95 XFCE4 : a full fledged Linux DE   It is also possible to avoid using te desktops altogether and select \u0026ldquo;Published Applications\u0026rdquo; instead following the documentation here ; however, most of the Grex applications are only accessible as modules and therefore not present in this menu.\nIn the Media tab, you might want to disable printing and sound support to suppress the corresponding warnings.\nAfter saving the new session, you should be able to connect to Grex with X2go!\nProblems and Limitations of X2go X2go relies on older version of NX library, that might fail to support newer versions of OpenGL based software.\nLinks Compute Canada has an X2Go documentation page here , with useful screenshots. X2go installation on the X2Go Wiki and X2go FAQ "}),a.add({id:6,href:'/grex-docs/docs/grex/running/interactive/',title:"Interactive work",content:"Interactive work The login nodes of Grex should be used to compile codes and to run short interactive calculations, compilation of code, and/or test runs. It is very easy to cause resource congestion on a shared Linux server. Therefore, all production calculations should be sumbitted in the batch mode, using our resource management system, SLURM. It is possible to submit so called interactive jobs: a job that creates an interactive session but actually runs on dedicated CPUs (and GPUs if needed) on the compute nodes rather than on the login servers (or head nodes).\nSuch mode of running interactive computations ensures that login nodes are not congested. A drawback is that when a cluster is busy, getting an interactive job to start will take some queuing time, just like any other job. So in practice interactive jobs are to be short and small to be able to utilize backfill-able nodes. This section covers how to run such jobs with SLURM.\nNote that manual SSH connections to the compute nodes without having active running jobs is forbidden on Grex.\nInteractive batch jobs To request an interactive jobs, the salloc command should be used. These jobs are not limited to single node jobs; any nodes/tasks/gpus layout can be requested by salloc in the same way as for sbatch. However, to minimize queuing time, usually a minimal set of required resources should be used when submitting interactive jobs (less than 3 H of walltime, less than 4 GB memory per core, etc.). Because there is no batch file for interactive jobs, all the resource requests should be added as command line options of the salloc command. The same logic of --nodes=, --ntasks-per-node= , --mem= and --cpus-per-task= resources as per batch jobs applies here as well.\nFor a threaded SMP code asking for half a node for two hours:\nsalloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=6 --mem=12000M --time=0-2:00:00  For an MPI jobs asking for 48 tasks, irrespectively of the nodes layout:\nsalloc --ntasks=48 --mem-per-task=2000M --time=0-2:00:00  Similar to batch jobs , specifying a partition with --partition= is required. Otherwise, the default partition will be used (as for now, skylake is set as default partition for CPU jobs).\nInteractive GPU jobs The difference for GPU jobs is that they would have to be directed to a node with GPU hardware:\n  The GPU jobs should run on the nodes that have GPU hardware, which means you\u0026rsquo;d want always to specify --partition=gpu or --partition=stamps-b.\n  SLURM on Grex uses the so called \u0026ldquo;GTRES\u0026rdquo; plugin for scheduling GPU jobs, which means that a request in the form of --gpus=N or --\\gpus-per-node=N or --gpus-per-task=N is required. Note that both partitions have up to four GPU per node, so asking more than 4 GPUs per node, or per task, is nonsensical. For interactive jobs, it makes more sense to use single GPU in most of the cases.\n   For an interactive session using two hours of one 16 GB V100 GPU, 4 CPUs and 4000MB per cpu:\nsalloc --gpus=1 --cpus-per-task=4 --mem-per-cpu=4000M --time=0-2:00:00 --partition=stamps-b  Similarly, for a 32 GB memory V100 GPU:\nsalloc --gpus=1 --cpus-per-task=4 --mem-per-cpu=4000M --time=0-2:00:00 --partition=gpu  Graphical jobs What to do if your interactive job involves a GUI based program? You can SSH to a login node with X11 forwarding enabled, or using X2Go remote desktop, and run it there. It is also possible to forward the X11 connection to compute nodes where your interactive jobs run with --x11 flag to salloc:\nsalloc --ntasks=1 --x11 --mem=4000M  To make it work you\u0026rsquo;d want the SSH session login node is also supporting graphics: either through -Y flag of ssh (or X11 enabled in PuTTY) or by using X2Go.\nYou may also try to use OpenOnDemand portal on Grex.\n"}),a.add({id:7,href:'/grex-docs/docs/longread/training/',title:"Training Materials and Presentations",content:"Autumn workshop, Oct 2022  Program and updates  High Performance Computing: Start Guide  High Performance Computing and software environments  OSC OpenOnDemand portal on Grex   Spring workshop, May 2022  Introduction to local and National HPC at UManitoba  Introduction to High Performance Computing step by step  Using GP GPU compute on Grex  High Performance Computing and software environments  OSC OpenOnDemand portal on Grex   Autumn workshop, November 2021 Slides from Day 1 (Nov 1, 2021):\n Introduction To HPC  Basics of Linux Shell   Slides from Day 2 (Nov 2, 2021):\n HPC software environments   Spring workshop , April 2021 Below are the slides from the Grex workshop that was held on April 21,22 2021\n Updates: ComputeCanada and Grex status, New Hardware, local RAC 2021  Beginners Introduction to HPC on Grex and ComputeCanada systems  Introduction to SLURM resources and partitions  Introduction to HPC software   "}),a.add({id:8,href:'/grex-docs/docs/grex/access/',title:"Access and Usage conditions",content:"Access Conditions Grex is open to all researchers at University of Manitoba and their collaborators. The main purpose of the Grex system is Research; it might be used for grad studies courses with a strong research component, for their course-based research.\nThe access and job accounting is by research group; that is, the Principal Investigator (PI)\u0026rsquo;s \u0026ldquo;accounting group\u0026rdquo; gets resource usage of their group members accounted to it. Grex\u0026rsquo;s resources (CPU and GPU time, disk space, software licenses) are automatically managed by a batch scheduler, according to the University\u0026rsquo;s priorities. There is a process of resource allocation competition (RAC) to get an increased share of Grex resources; however, a \u0026ldquo;default\u0026rdquo; share of the resources is available immediately and free of charge by getting an account.\nIt is expected that Grex accounts and resource allocations are used for the research projects they are requested for.\nOwners of the user-contributed hardware on Grex have preferential access to their hardware, which can only be used by the general community of UM researchers when idle and not reserved.\nGetting an account on Grex As of the moment, Grex is using the Compute Canada account management database (CCDB ). Any eligible Canadian Faculty member can get a Compute Canada account in the CCDB system. If you are a graduate student, postdoctoral fellow, research assistant, undergraduate student, or a non-research staff member, visiting faculty or external collaborator, you will need to be sponsored in CCDB by a Principal Investigator (PI), i.e. a Faculty member. The PI must register in the CCDB first, and then he/she can sponsor you as a \u0026lsquo;Group Member\u0026rsquo; under his/her account. Once your application for a Compute Canada account has been approved, you will receive a confirmation email and you can start using the computing and data storage facilities.\nThere are two technical conditions for getting access:\n An active CCDB account. An active CCDB \u0026ldquo;role\u0026rdquo; affiliated with UManitoba.   Guidelines of the Acceptable Use Grex adheres to Compute Canada\u0026rsquo;s Privacy and Security policy, and to University of Manitoba IT Security and Privacy policies. Users that have Grex account have accepted both.  In particular, user accounts can not be shared with anyone for whatever reason. Sharing any account information (login/password or SSH private keys) leads to immediate blocking of the account. UNIX groups and shared project spaces can be used for sharing data.  Usage is monitored and statistics are collected automatically, to asses the researcher\u0026rsquo;s needs and future planning, and to troubleshoot day to day operational issues.  Users of Grex should be \u0026ldquo;fair and considerate\u0026rdquo; in their usage of the systems, trying not to allow for unnecessary and/or ineffecient use of the resources or interfering with other users\u0026rsquo;s work. We reserve to ourself the right to monitor for inefficient use and ask users to stop their activities if they threaten the general stability of the Grex system.  Getting a Resource Allocation on Grex  Similarly to Compute Canada, there is a two-tier system for resource allocation on Grex. Every group gets a \u0026ldquo;Default\u0026rdquo; allocation of Grex resources (computing time and storage space). Groups that need a larger fraction of resources and use Grex intensively, might want to apply for a local Resource Allocation Competition (RAC). Grex\u0026rsquo;s local RAC calls are issued once a year. They are reviewed by a local Advanced Research Computing Committee and may be scaled according to the availability of resources. The instructions and conditions of this year\u0026rsquo;s RAC are provided on the RAC template document. This year\u0026rsquo;s local RAC application call was closed in May 1, 2021.   "}),a.add({id:9,href:'/grex-docs/docs/grex/software/cern-vmfs/',title:"CVMFS and ComputeCanada",content:"Cern VMFS on Grex CVMFS or CernVM stands for CernVM File System. It provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications.\nPresently, we use CernVMFS (CVMFS) to provide Compute Canada\u0026rsquo;s software stack. We plan to add more publically available CVMFS software repositories such as the one from OpenScienceGrid , in a near future. Note that we can only \u0026ldquo;pull\u0026rdquo; software from these repositories. To actually add or change software, datasets etc., the respective orgaizations controlling CVMFS repositories should be contacted directly.\nAccess to the CVMFS should be transparent to the Grex users: no action is needed other than loading a software module or setting a path.\nGrex does not have a local CVMFS stratum server. All we do is to cache the software items as they get requested. Thus there can be a delay associated with pulling a software item from Stratum 1 (Replica Server) for the first time. It usually does not matter for serial progams but parallel codes, that rely on simultaneus process spawning across many nodes, it might cause timeout errors. Thus, it is probably a good idea to first access the codes in a small interactive job to warm up the Grex\u0026rsquo;s local CVMFS cache.\nCompute Canada software stack The main reason for having CVMFS supported on Grex is to provide Grex users with the software environment as similar as possible with the environment existig on National Compute Canada HPC machines. On Grex, the module tree from Compute Canada software stack is not set as default, but has to be loaded with the following commands:\nmodule purge\nmodule load CCEnv\n After the above command, use module spider to search for any software that might be available in the CC software stack. Note that \u0026ldquo;default\u0026rdquo; environment (the StdEnv and nixpkgs modules of the CC stack) are not loaded automatically, unlike on Compute Canada general purpose (GP) clusters. Therefore, it is a good practice to load these modules right away after the CCEnv. The example below loads the Nix package layer that forms the base layer of CC software stack, and then one of the \u0026ldquo;standard environments\u0026rdquo;, in this case based on Intel 2018 and GCC 7.3 compilers, MKL and OpenMPI.\nThere is more than one StdEnv version to chose from.\nmodule load nixpkgs/16.09\nmodule load StdEnv/2018.3\n Note that there are several CPU architectures in the CC software stack. They differ in the CPU instruction set used by the compilers, to generate the binary code. The default for legacy systems like Grex is the lowest SSE3 architecture arch/sse3. It ensures that there is no failure on the legacy Grex nodes (which are of NEHALEM, SSE4.2 architecture) due to more recent instructions like AVX, AVX2 and AVX512 that were added by Intel afterwards.\nFor running on Contributed Nodes, that may be of much newer CPU generation, it is better to use the arch/avx512 module and setting RSNT_ARCH=avx512 environment variable in the job scripts.\nSome of the software items on CC software stack might assume certain environment variables set that are not present on Grex; one example is SLURM_TMPDIR. In case your script fails for this reason, the following line could be added to the job script:\nexport SLURM_TMPDIR=$TMPDIR  While a majority of CC software stack is built using OpenMPI, some items might be based on IntelMPI. These will require following additional environment variables to be able to integrate with SLURM on Grex:\nexport I_MPI_PMI_LIBRARY=/opt/slurm/lib/libpmi.so\nexport I_MPI_FABRICS_LIST=shm:dapl\n If a script assumes, or relies on using the mpiexec.hydra launcher, the later might have to be provided with -bootstrap slurm option.\nHow to find software on CC CVMFS Compute Canada\u0026rsquo;s software building system automatically generates documentation for each item, which is available at the Available Software page. So the first destination to look for a software item is probably to browse this page. Note that this page covers the default CPU arhitectures (AVX2, AVX512) of the National systems, and legacy architecturs (SSE3, AVX) might not necessary have each of the software versions and items compiled for them. It is possible to request such versions to be added.\nThe Lmod, module spider command can be used on Grex to search for modules that are actually available. Note that the CCEnv software stack is not loaded by default; you would have to load it first to enable the spider command to search through the CC software stack:\nmodule purge; module load CCEnv\nmodule spider mysoftware\n Then, when finding available software versions and their dependencies, module load command can be used, as descibed here How to request software added to CC CVMFS Compute Canada maintains and distributes the software stack as part of its mandate to maintain the National HPC systems. To request a software item installed, the requestor should be part of Compute Canada system (that is, have an account in CCDB , which is also a prerequisite to have access to Grex. Any CC user can submit such request to support@tech.alliancecan.ca and notify if a version for non-default CPU architecture such as SSE3 is also necessary to build.\nAn example, R code with dependencies from CC CVMFS stack A real world example of using R on Grex, with several dependencies required for the R packages.\nFor the dynamic languages like R and Python, Compute Canada does not, in general, provide or manage pre-installed packages. Rather, users are expected to load the base R (Python, Perl, Julia) module and then proceed for the loacl installation of the required R (or Python, Perl, Julia etc.) packages in their home directories. Check the CC R documentation and CC Python documentation .\n#!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-72:00:00 #SBATCH --job-name=\u0026#34;R-gdal-jags-bench\u0026#34; # Load the modules: module load CCEnv module load nixpkgs/16.09 gcc/5.4.0 module load r/3.5.2 jags/4.3.0 geos/3.6.1 gdal/2.2.1 export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; R --vanilla \u0026lt; Benchmark.R \u0026amp;\u0026gt; benchmark.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Notes on MPI based software from CC Stack We recommend to use a recent ComputeCanada environment/toolchain that provides OpenMPI 3.1.x or later, which has a recent PMIx process management interface and supports UCX interconnect libraries that are used on Grex. Earlier versions of OpenMPI might or might not work. With OpenMPI 3.1.x or 4.0.x, srun command should be used in SLURM job scripts on Grex. Below is an example of MPI job (Intel benchmark) using the StdEnv/2018.3 toolchain (Intel 2018 / GCC 7.3.0 and OpenMPI 3.1.2).\n#!/bin/bash #SBATCH --ntasks-per-node=2 --nodes=2 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-1:00:00 #SBATCH --job-name=\u0026#34;IMB-MPI1-4\u0026#34; # Load the modules: module load CCEnv module load StdEnv/2018.3 module load imb/2019.3 module list echo \u0026#34;Starting run at: `date`\u0026#34; srun IMB-MPI1 \u0026gt; imb-ompi312-2x2.txt echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  If the script above is saved into imb.slurm, it can be submitted as follows:\nsbatch imb.slurm  Notes on Code development with CC Stack Because Compute Canada software stack can only distribute open source software to non-CC systems like Grex, proprietary/restricted software items are omitted. This means that Intel compiler modules, while providing their redistributable parts necessary to run the code compiled with them, will not work to compile new code on Grex. Thus, only GCC compilers and GCC-based toolchains from CC Stack are useful for the local code development on Grex.\nOpenScienceGrid On Grex, we mount OSG reporsitories, mainly for Singularity containers provided through OSG. Pointing the singularuty to the desired path under /cvmfs/singularity.opensciencegrid.org/ will automatically mount and fetch the required software items. Discovering them is up the the users. See more in our Containers documentation page.\n"}),a.add({id:10,href:'/grex-docs/docs/faq/',title:"Frequently Asked Questions",content:"Access Forgot and/or reset password If you forgot and/or would like to reset your password for Grex and/or any Compute Canada national cluster, visit CCDB . Please note that you will not be able to reset your password before your first role gets approved by a Compute Canada administrator.\n Frequently Asked Questions Access How to connect to Grex SSH gives key errors SSH Software Running jobs How to get support "}),a.add({id:11,href:'/grex-docs/docs/grex/connecting/',title:"Connecting / Transferring data",content:"Connecting to Grex In order to use almost any HPC system, you would need to be able to somehow connect and log in to it. ALso, it would be necessary to be able to transfer data to and from the system. The standard means for these tasks are provided by the SSH protocol .\nTo log in to Grex in the text mode, connect to grex.hpc.umanitoba.ca using an SSH (secure shell) client. The DNS name grex.hpc.umanitoba.ca serves as an alias for two login nodes: bison.hpc.umanitoba.ca and tatanka.hpc.umanitoba.ca .\nUploading and downloading your data can be done using an SCP/SFTP capable file transfer client. The recommended clients are OpenSSH (providing ssh and scp, sftp command line tools on Linux and MacOS X) and PuTTY/WinSCP/X-Ming or MobaXterm under Windows. Note that since Jun 1, 2014, the original \u0026ldquo;SSH Secure Shell\u0026rdquo; Windows SSH/SFTP client is not supported anymore.\nSince Dec. 2015, support is provided for the graphical mode connection to Grex using X2go .\nX2go remote desktop clients are available for Windows, MacOS X and Windows. When creting a new session, please chose either of the supported desktop environments: \u0026ldquo;OPENBOX\u0026rdquo; or \u0026ldquo;ICEWM\u0026rdquo; in the \u0026ldquo;Session type\u0026rdquo; menu. The same login/password should be used as for SSH text based connections.\nNew login node:\nSince early 2021, a new login node, yak.hpc.umanitoba.ca is available to access and build software that uses new Intel AVX2, AVX512 CPU instructions. Yak is not part of the grex.hpc.umanitoba.ca alias.  OpenOnDemand (OOD) Web interface: Since October 2021, there is an OpenOnDemand (OOD) Web interface to Grex, available at https://aurochs.hpc.umanitoba.ca from UManitoba IP addresses. OOD provides a way to connect both in text and graphical mode right in the browser, to transfer data between Grex local machines, and to run jobs.  See the documentation for more details on how to connect from various client operaton systems:\n Connecting to Grex with SSH  Connecting to Grex with X2Go  Connecting to Grex with OOD   "}),a.add({id:12,href:'/grex-docs/docs/grex/connecting/ood/',title:"Connecting and Transferring data with OOD",content:"OSC OpenOnDemand on Grex There is a Web-portal software that can replace both SSH and SCP clients via a convenient Web access.\nFor more information, please visit the page: Connecting with OOD "}),a.add({id:13,href:'/grex-docs/docs/localit/',title:"Local IT Resources",content:"Introduction In addition to the HPC Research Computing facility, there are other IT providers that might relate to research.\nResources provided by IST IST provides a number of services related to Administrative IT, Teaching, and Research Computing as well.\nFor more information about IST, please visit UM IST Help website and IST service catalog .\nResources provided by the Libraries Libraries provide a variety of services relaed to Research Data Management, as well as a GIS service: UM Libraries Research Services .\nFaculties IT representatives Several Faculties/Department have local IT representatives that maintain servers, workstation and computing labs.\n"}),a.add({id:14,href:'/grex-docs/docs/grex/software/general-linux/',title:"General Linux tools",content:"Linux tools on Grex There is a number of general and distro-specific tools on Grex that are worth mentioning here. Such tools are: text editors, image viewers, file managers, \u0026hellip; etc.\nCommand line Text editors Command line text editors allow you to edit files right on Grex in any terminal session (such as SSH session or an X terminal under X2Go):\n  The (arguably) most popular editor is vi, or vim. It is very powerful, but requires some experience to use. To exit a vim session, you can use ZZ key combination (hold shift key + zz), or ESC, :x!. There are many vi tutorials around, for example this one.\n  Another lightweight text-mode editor is nano. It provides self-explanatory key-combination menu at the bottom of the screen.\n  The MidnightCommander file manager provides a text-mode editor that can be invoked stand-alone as mc -e filename.\n   GUI Text editors Sometimes it is useful (for example, for copy/paste operations with mouse, between client computer and a remote session) or convenient to have a text editor with a graphical user interface. Note that a most practical way to use this is from X2Go sessions that provides tolerable interaction speeds.\nVi has a GUI counterpart which is accesible as evim command. There are also the following GUI editors: nedit and xfe-xfw.\nImage viewers There are the following commands that can be used for viewing images: xfe-xfi and nemacs. A simple PDF viewer for X11, xpdf and ghostscript are also available.\n"}),a.add({id:15,href:'/grex-docs/docs/grex/data/',title:"Storage and Data",content:"General Storage Information As of now, the storage system of Grex consists of the following:\n  The /home NFSv4 filesystem is served by a very fast NVME disk server. The total size of the filesystem is 15 TB. The quota per-user is 100 GB of space and 500K of files.\n  The /global/scratch Lustre filesystem, Seagate SBB, total usable size of 418 TB. It is intended to be used as the high-performance, scalable workspace for active projects. It is not backed up and is not intended for long-time storage of users data that is not actively used. The default quota is 2 TB of space and 1M files per user and can be increased on request to 10 TB per research group. Larger disk space requires a local RAC application.\n  The local node storage as defined by the environment variable $TMPDIR is recommended for temporary job data that is not needed after job completes. Grex nodes have SATA local disks of various capacities, leaving 150 Gb, 400 Gb, 800 Gb and 1700 Gb usable space per node, depending of the kind of local disk it has.\n   Most user would want to use /home for code development, source code, scripts, visualization, processed data, \u0026hellip; etc., that do not take much space and benefits for small files I/O. For production data processing, that is, massive I/O tasks from many compute or interactive jobs, /global/scratch should be used. It is often beneficial to place temporary files on the local disk space of the compute nodes, if space permits, so that the jobs do not load Lustre or NFS extensively.\nData retention and Backup Data retention policy as of now conforms to the corresponding Compute Canada policy. Namely, the data will not be kept on Grex indefinitely after user\u0026rsquo;s account is expired. The data retention period for expired accounts is 1 year. Note that we do not do regular, short term data purges for /global/scratch filesystem. However, data on login and compute nodes local scratch gets purged regularly and automatically.\nBackup: after migration to new /home, the backup temporarily lapsed. There is no backup on any of Grex filsystems now. We are working on resuming the backup.  Data sharing Sharing of accounts login information (like passwords or SSH keys) is stricty forbidden on Grex, as well as on most of the HPC systems. There is a mechanism of data/file sharing that does not require sharing of the accounts. To access each others' data on Grex, the UNIX groups and permissions mechanism can be used.\nDisclaimer of any responsibility for Data loss Every effort is made to design and maintain Grex storage systems in a way that they are a reliable storage for researcher\u0026rsquo;s data. However, we (Grex Team, or the University) make no guarantees that any data can be recovered, regardless of where they are stored, even in backed up volumes. Accidents happen, whether caused by human error, hardware or software errors, natural disasters, or any other reason. It is the user\u0026rsquo;s responsibility that the data is protected against possible risks, as appropriate to the value of the data.  "}),a.add({id:16,href:'/grex-docs/docs/grex/software/containers/',title:"Containers for Software",content:"Introduction Linux Containers are means to isolate software dependencies from the base Linux operation system. On Grex, we support the Singularity container system, now developed by a company called SyLabs. Several other Linux containers engines exist, most notably Docker which is a very popular tool in DevOps community. Presently Docker containers cannot be directly supported on shared HPC systems like Grex. However, with help of Singularity, it is possible to run Docker images from the DockerHub , as well as native Singularity images from other repositories, such as SingularityHub and SyLabsCloud .\nUsing Singularity on Grex Start with module spider singularity; it will list the current version. Due to the nature of container runtime environments, we update Singularity regularly, so the installed version is usually the latest one. Load the module (in the default Grex environment) by the following command:\nmodule load singularity  With singularuty command, one can list singularity commands and their options:\nmodule load singularity\nsingularity help\n A brief introduction on getting started with Singularity can be useful to get started. You will not need to install Singularity on Grex since it is already provided as module.\nTo execute an application within the container, do it in the usual way for that application, but prefix the command with \u0026lsquo;\u0026lsquo;singularity exec image_name.sif\u0026quot;. For example, to run R on an R script, using a container named R-INLA.sif:\nsingularity exec ./R-INLA.sif R --vanilla \u0026lt; myscript.R  Quite often, it is useful to provide the containerized application with data residing on a shared HPC filesystem such as /home or /global/scratch. This is done via bind mounts . Normally, the container bind-mounts $HOME, /tmp and the current working directory. On Grex to bind the global Lustre filesystem the following -B option should be used:\nsingularity exec -B /sbb/scratch:/global/scratch ./R-INLA.sif R --vanilla \u0026lt; myscript.R   In case you do not want to mount anything to preserve the containers\u0026rsquo; environment from any overalapping data/code from say $HOME, use the --containall flag.\nSome attention has to be paid to Singularity\u0026rsquo;s local cache and temporary directories. Singularity caches the container images it pulls and Docker layers under $HOME/.singularity. Containers can be large, in tens of gigabytes, and thus they can easily accumulate and exhaust the users storage space quota on $HOME. Thus users might want to set the SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR variables to some place under their /global/scratch space.\nFor example, to change the location of SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR, ome might run:\nmkdir -p /global/scratch/$USER/singularity/{cache,tmp}\nexport SINGULARITY_CACHEDIR=\u0026quot;/global/scratch/$USER/singularity/cache\u0026quot;\nexport SINGULARITY_TMPDIR=\u0026quot;/global/scratch/$USER/singularity/tmp\u0026quot;\n before building the singularity image.\nGetting and building Singularity images The commands singularty build and singularity pull would get Singularity images from DockerHub, SingularityHub or SyLabsCloud. Images can also be built from other images, and from recipes. A recipe is a text file that specifies the base image and post-install commands to be performed on it.\nSingularity with GPUs Use the --nv flag to singularity run/exec/shell commands. Naturally, you should be on a node that has a GPU, in an interactive job. NVIDIA provides many pre-built Docker and Singularity container images on their \u0026ldquo;GPU cloud\u0026rdquo; , together with instructions on how to pull them and to run them. These should work on Grex without much changes.\nOpenScienceGrid CVMFS We can run Singularity containers distributed with OSG CVMFS which is currenlly mounted on Grex\u0026rsquo;s CVMFS. The containers are distributed via CVMFS as unpacked directory images. So the way to access them is to find a directory of interest and point singularity runtime to it. The directories will then be mounted and fetched automatically. The repository starts with /cvmfs/singularity.opensciencegrid.org/. Then you\u0026rsquo;d need an idea from somewhere what you are looking for in the subdirectories of the above mentioned path. An example (accessing, that is, exploring via singularity shell command, Remoll software distributed trough OSG CVMFS by jeffersonlab):\nmodule load singularity\nsingularity shell /cvmfs/singularity.opensciencegrid.org/jeffersonlab/remoll\\:develop \n A partial description of what is present on OSG CVMFS is available here .\nMore information  Singularity/Sylabs homepage  Compute Canada Singularity documentation  Westgrid Singularity tutorial , a recording can be found here  Docker Hub  Singularity Hub  Sylabs Cloud  NVIDIA NGC cloud  OSG Helpdesk for Singularity   "}),a.add({id:17,href:'/grex-docs/docs/grex/running/',title:"Running Jobs",content:"Running Jobs Why running jobs in batch mode? There is a number of reasons for adopting a batch mode for running jobs on a cluster. From providing user\u0026rsquo;s computations with fairness, traffic control to prevent resource congestion and resource trashing, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the SLURM batch system since December 2019.\nPartitions The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM as a scheduler, this requires partitioning: a \u0026ldquo;partition\u0026rdquo; is a set of compute nodes, groupped by a characteristic, usually by kind of hardware the nodes have, and sometimes by who \u0026ldquo;owns\u0026rdquo; the hardware as well.\nThere is no fully automatic selection of partitions, other than the default skylake for most of the users, and compute for the short jobs. For the contributors' group members, the default partition will be their contributed nodes. Thus in many cases users have to specify the partition manually when submitting their jobs!\nCurrently, the following partitions are available on Grex:\n* General purpose partitions:  skylake : the new 52-core, CascadeLakeRefresh compute nodes, 96 Gb/node (set as the default partition). NEW largemem : the new 40-core, CascadeLake compute nodes, 384 Gb/node. NEW compute : the original SSE4.2 12-core Grex nodes, RAM 48 Gb/node (no longer set as the default partition for jobs over 30 minutes). gpu : two GPU V100/32 GB AVX512 nodes, RAM 192 GB/node. NEW test : a 24-core Skylake CPU Dell large memory (512 GB), NVMe workstation for interactive work and visualizations. NEW   * Contributed partitions:  stamps : three 4 x GPU v100/16GB AVX512 nodes contributed by Prof. R. Stamps (Department of Physics and Astronomy). livi : a HGX-2 16xGPU V100/32GB, NVSwitch server contributed by Prof. L. Livi (Department of Computer Science). agro : two 24-core AMD Zen, RAM 256 GB/node, two NVIDIA A30 GPUs per node, contributed by Faculty of Agriculture.   * Preemptible partitions:  stamps-b : Preemptible partition for general use of the above nodes contributed by Prof. R. Stamps. livi-b : Preemptible partition for general use of the above nodes contributed by Prof. L. Livi. agro-b : Preemptible partition for general use of the above nodes contributed by Faculty of Agriculture.   The former five partitions (skyake, compute, largemem, test and gpu) are generally accessible. The next three are open only to the contributor\u0026rsquo;s groups.\nOn the contributed partitions, the owner\u0026rsquo;s group has preferencial access. However, users belonging to other groups can submit jobs to one of the preemptible partitions (ending with -b) to run on the contributed hardware as long as it is unused, on the condition that their jobs can be preempted (that is, killed) should owners jobs need the hardware. There is a minimum runtime guaranteed to preemptible jobs, which is as of now 1 hour. The maximum walltime for the preemptible partition is set per partition (and can be seen in the output of the sinfo command). To have a global overview of all partitions on Grex, run the custom script partition-list from your terminal.\nOn the special partition test, oversubcsription is enabled in SLURM, to facilitate better turnaround of interactive jobs.\nJobs cannot run on several partitions at the same time; but it is possible to specify more than one partiton, like in --partition=compute,skylake, so that the job will be directed by the scheduler to the first partiton available.\nJobs will be rejected by the SLURM scheduler if partition\u0026rsquo;s hardware and requested resources do not match (that is, asking for GPUs on compute, largeme or skylake partitions is not possible). So in some cases, explicitly adding --partition= flag to SLURM job submission is needed.\nJobs that require stamps-b or gpu partitons have to use GPUs, otherwise they will be rejected; this is to prevent of bogging up the precious GPU nodes with CPU-only jobs!\nAccounts Users belong to \u0026ldquo;accounting groups\u0026rdquo;, led by their principal investigators (PIs). The accounting grops on Grex match CCDB roles. By default, a user\u0026rsquo;s jobs are assigned to his primary/default accounting group. But it is possible for a user to belong to more than one group; then the --account= parameter can be used for sbatch or salloc to select non-default account to run jobs under. For example, a use who belongs to two accounting groups: def-sponsor1 and def-sponsor2, can specify which one to use:\n#SBATCH \u0026ndash;account=def-sponsor1  or #SBATCH \u0026ndash;account=def-sponsor2  QOSs QOS stands for Quality of Service. It is a mechanism to modify scheduler\u0026rsquo;s limits, hardware access policies and modify job priorities and job accounting/billing. Presently, QOS might be used by our Scheduler machinery internally, but not specified by the users. Jobs that specify explicit --qos= will be rejected by the SLURM job submission wrapper.\nLimits and policies In order to prevent monopolization of the entire cluster by a single user or single accountig group, we enforce a MAXPS like limit; for non-RAC accounts it is set to 4 M CPU-minutes and 400 CPU cores. Accounts that have allocation (RAC) on Grex get higher MAXPS and max CPU cores limits in order to let them to utilize their usage targets.\nPartitions for preemptible jobs running on contributed hardware might be further limited, so that they can not occupy the whole contributed hardware.\nIn cases when Grex is underutilized, but some jobs exist in the queue that can be run if not for the above mentioned limits, we might relax the limits as a temporary \u0026ldquo;bonus\u0026rdquo;.\nSLURM commands Naturally, SLURM provides a command line user interface. Some of the most useful commands are listed below.\nExploring the system The SLURM command that shows state of nodes and partitions is sinfo:\nExamples:\n sinfo: to list all the nodes (idle, down, allocated, mixed) and partitions. sinfo \u0026ndash;state=idle: to lisl all idle nodes. sinfo -p compute: to list information about partition (compute in this case). sinfo -p skylake \u0026ndash;state=idle: to list idle nodes on a given partition (skylake in this case).   Submitting jobs Batch jobs are submitted as follow:\nsbatch [options] myfile.job  Interactive jobs are submitted in exactly same way, but they do not need the job script because they will give you an interactive session:\nsalloc [options]\nThe command sbatch returns a number called JobID and used by SLURM to identify the job in the queuing system.  The options are used to specify resources (wall time, tractable resources such as cores and nodes and GPUs) and accounts and QOS and partitions under which the jobs should run, and various other options like specifying whether jobs need X11 GUI (--x11), where its output should go, whether email should be sent when job changes its states and so on. Here is a list of the most frequent options to sbatch command:\nResources (tractable resources in SLURMspeak) are CPU time, memory, and GPU time.\nGeneric resources can be software licenses, etc. There are also options to control job placement such as partitions and QOSs.\n --ntasks=: specifies number of tasks (MPI processes) per job. --nodes=: specifies number of nodes (servers) per job. --ntasks-per-node=: specifies number of tasks (MPI processes) per node. --cpus-per-task=: specifies number of threads per task. --mem-per-cpu=: specifies memory per task (or thread?) --mem=: specifies the memory per node. --gpus=: specifies number of GPUs per job. There are also --gpus-per-XXX and --XXX-per-gpu --time-: specifies walltime in format DD-HH:MM --qos=: specifies a QOS by its name (Should not be used on Grex!) --partition=: specifies a partiton by its name (Can be very useful on Grex!)   An example of using some of these options with sbatch and salloc are listed below:\n  sbatch --nodes=1 --ntasks-per-node=1 --cpus-per-task=12 --mem=40gb --time=0-48:00 gaussian.job\n  salloc --nodes=1 --ntasks-per-node=4 --mem-per-cpu=4000mb --x11 --partition=compute\n   And so on. The options for batch jobs can be either in command line, or (perhaps better) in the special comments in the job file, like:\n#SBATCH --mem=40gb Refer to the subsection for Batch jobs and Interacive jobs for more information, examples of job scripts and how to actually sumbit jobs.\nMonitoring jobs Checking on the queued jobs:\n squeue -u someuser (to see all queued jobs of the user someuser) squeue -u someuser -t R (to see all queued and running jobs of the user someuser) squeue -u someuser -t PD (to see all queued and pending jobs of the user someuser) squeue -A def-sponsor1 (to see all queued jobs for the accounting group def-sponsor1)   Without the above parameters, squeue would return all the jobs in the system. There is a shortcut \u0026lsquo;\u0026lsquo;sq\u0026rsquo;\u0026rsquo; for \u0026lsquo;\u0026lsquo;squeue -u $USER\u0026rsquo;\u0026rsquo;\nCancelling jobs:\n scancel JobID (to cancel a job JobID) echo \u0026quot;Deleting all the jobs by $USER\u0026quot; \u0026amp;\u0026amp; scancel -u $USER (to cancel all your queued jobs at once). echo \u0026quot;Deleting all the pending jobs by $USER\u0026quot; \u0026amp;\u0026amp; scancel -u $USER --state=pending (to cancel all your pending jobs at once).   Hold and release queued jobs:\nTo put hold some one or more jobs, use:\n scontrol hold JobID (to put on hold the job JobID). scontrol hold JobID01,JobID02,JobID03 (to put on hold the jobs JobID01,JobID02,JobID03).   To release them, use:\n scontrol release JobID (to release the job JobID). scontrol release JobID01,JobID02,JobID03 (to release the jobs JobID01,JobID02,JobID03).   Checking job efficiency:\nThe command seff is a wrapper around the command sacct ang gives a friendly output, like the actual utilization of walltime and memory:\n seff JobID seff -d JobID   Note that the output from seff command is not accurate if the job was not successful.\nChecking resource limits and usage for past and current jobs:\n sacct -j {JobID} -l sacct -u $USER -s {STARTDATE} -e {ENDDATE} -l --parsable2   Getting info on accounts and priorities Fairshare and accounting information for the accounting group abc-12-aa:\n sshare -l -A abc-12-aa sshare -l -A abc-12-aa --format=\u0026quot;Account,EffectvUsage,LevelFS\u0026quot; sshare -a -l -A abc-12-aa sshare -a -l -A abc-12-aa --format=\u0026quot;Account,User,EffectvUsage,LevelFS\u0026quot;   Fairshare and accounting information for a user:\nsshare -l -U -u $USER  Limits and settings for an account:\nsacctmgr list assoc account=abc-12-aa format=account,user,qos  Useful links  SLURM documentation  Running jobs on Compute Canada clusters. References for migrating from PBS to SLURM: ICHEC , HPC-USC  Westgrid training materials on SLURM: Scheduling   Since the HPC technology is widely used by most of universities and National labs, simple googling your SLURM question will likely return a few useful links to their HPC/ARC documentation.\n"}),a.add({id:18,href:'/grex-docs/docs/grex/ood/',title:"Grex's OpenOnDemand Web Portal",content:"Introduction OpenOnDemand or OOD for short, is an open source Web portal for High-Performance computing, developed at Ohio Supercomputing Center. OOD makes it easier for beginner HPC user to access the resources via a Web interface. OOD also allows for interactive, visualization and other linux Desktop applications to be accessed on HPC systems via a convenient Web user interface.\nSince end of October 2021, OnDemand version 2 is officially in production on Grex.  For more general OOD information, see the OpenOnDemand Paper OpenOndemand on Grex Grex\u0026rsquo;s OOD instance runs on aurochs.westgrid.ca . It is available only from UManitoba IP addresses \u0026ndash; that is, your computer should be on UM Campus network to connect.\nTo connect from outside UM network, please install and start UManitoba Virtual Private Network . OOD relies on in-browser VNC sessions; so a modern browser with HTML5 support is required; we recommend Google Chrome or Firefox and its derivatives (Waterfox, for example).\nConnect to OOD using UManitoba VPN :  Make sure Pulse Secure VPN is connected Point your Web browser to https://aurochs.hpc.umanitoba.ca Use your Compute Canada username and password to log in to Grex OOD.   OOD expects user accounts and directories on Grex to be already created. Thus, new users who want to work with OOD should first connect to Grex normally, via SSH shell at least once, to make the creation of account, directories, and quota complete. Also, OOD creates a state directory under users' /home (/home/$USER/ondemand) where it keeps information about running and completed OOD jobs, shells, desktop sessions and such. Deleting the ondemand directory while a job or session is running would likely cause the job or session to fail.\n It is better to leave the /home/$USER/ondemand directory alone!   Working with files and directories One of the convenient and useful features of OOD is its Files app that allows you to browse the files and directories across all Grex filesystems: /home and /global/scratch.\nYou can also upload your data to Grex using this Web interface. Note that there are limits on uploads on the Web server (a few GBs) and there can be practical limits on download sizes as well due to internet connection speed and stability.\nCustomized OOD apps on Grex The OOD Dashboard menues Interactive Apps shows interactive applications. This is the main feature of OOD, it allows interactive work and visualizations, all in browser. These application will run on as SLURM Jobs on Grex compute nodes. Users can specify required SLURM resources such as time, number of cores and partitions.\nAs for now, the following applications are supported:\n Linux Desktops in VNC Matlab GUI in VNC GaussView GUI in VNC Jupyter Noteboooks servers   As with regular SLURM jobs, it is important to specify SLURM partitions for them to start faster. Perhaps the test partition for Desktop is the best place to start interactive Desktop jobs, so it is hardcoded in the Simplified Desktop item.\n"}),a.add({id:19,href:'/grex-docs/docs/grex/software/',title:"Software",content:"Software In HPC world, software is more often meant as codes that do some scientific or engineering computation, data processing and visualiation (as opposed to web services, relational databases, client-server businness systems, email and office, \u0026hellip; etc).\nTools and libraries used to develop HPC software are also software, and have several best practices associated with them. Some of that will be covered below. Without means to provide software to do computations, HPC systems would be rather useless.\nFortunately most HPC systems, and Grex is no exception, come with a pre-installed, curated software stack. This section covers how to find the installed software, how to access it, and what options you have if some of the software you need is missing.\nHow software is installed and distributed There are several mechanisms of software installation under Linux. One of them is using Linux software package manager (apt on Ubuntu, yum on Centos, etc) and a binary package repository provided by some third party. These package managers would install a version of the code system wide, into standard OS directories like /usr/bin where it would be immediately available in the systems PATH (PATH is a variable that specifies where the operating systems would look for executable code).\nThis method of installation is often practiced on person\u0026rsquo;s own workstations, because it requires no knowledge other than syntax of the OS package manager. There are however significant drawbacks to it for using on HPC clusters that consists of many compute nodes and are shared by many users:\n  Need of root access to install in the base OS is a systems stability and security threat, and has a potential of users interfering with each other.\n  Package managers as a rule do not keep several versions of the same package; they are geared towards having only the newest one (as in \u0026ldquo;software update\u0026rdquo;), which poses a problem for reproducible research.\n  A package should be installed across all the nodes of the cluster; thus the installations should be somehow managed.\n  Binary packages in public repos tend to be compiled for generic CPU architectures rather than optimized for a particular system.\n   Thus in HPC world, as a rule, only a minimal set of core Linux OS packages is installed by system administrators, and no access to package managers is given to the end users. There are ways to let users have their own Linux OS images through virtualization and containerization technolgies (see the Containers section) when it is really necessary.\nOn most of the HPC machines, the application software is recompiled from sources and installed into a shared filesystem so that each compute node has access to the same code. Multiple versions of a software package can be installed into each own PATH; dependencies between software (such as libraries from one package needed to be accessed by another package) are tracked via a special software called Environmental Modules.\nThe Modules package would manipulate the PATH (and other systems environment variables like LD_LIBRARY_PATH, CPATH and application-specific ones like MKLROOT, HDF5_HOME) on user request, \u0026ldquo;loading\u0026rdquo; and \u0026ldquo;unloading\u0026rdquo; specified software items.\nThe two most popular Modules are the original Tcl Modules and its Lmod rewrite in Lua at TACC . On the new Grex, Lmod is used.\nLmod The main feature of Lmod is hierarchical module system to provide a better control of software dependencies. Modules for software items that depend on a particular core modules (toolchains: a compiler suite, a MPI library) are only accessible after the core modules are loaded. This prevents situations where conflicts appear when software items built with different toolchains are loaded simultaneously. Lmod will also automatically unload conflicting modules and reload their dependencies should toolchain change. Finally, by manipulating module root paths, it is possible to provide more than one software stack per HPC system. For more information, please refer the software stacks available on Grex.\nHow to find the software with Lmod Modules A \u0026ldquo;software stack\u0026rdquo; module should be loaded first. On Grex, there are two software stack, called GrexEnv and CCEnv, and standing for the sofware built on Grex locally and the software environment from Compute Canada, correspondingly. GrexEnv is the only module loaded by default.\nWhen a software stack module is loaded, the module spider command will find a specific software item (for example, GAMESS; note that all the module names are lower-case on Grex and on Compute Canada software stacks) if it exist under that stack:\nmodule spider gamess  It might return several versions; then usually a subsequent command with the version is used to determine dependencies required for the software. In case of GAMESS on Grex:\nmodule spider gamess/Sept2019  It will advise to load the following modules: \u0026ldquo;intel/15.0.5.223 impi/5.1.1.109\u0026rdquo;. Then, module load command can be used actually to load the GAMESS environment (note that the dependencies must be loaded first:\nmodule load intel/15.0.5.223 impi/5.1.1.109\nmodule load gamess/Sept2019\n For more information about using Lmod modules, please refer to Compute Canada documentation about Using Modules and Lmod User Guide .\nHow and when to install software in your HOME directory Linux (Unlike some Desktop operatinhg systems) has a concept of user permissions separation. Regular users cannot, unless explicitly permitted, access systems files and files of other users.\nYou can almost always install software without super-user access into your /home/$USER directory. Moreover, you can manage the software with Lmod: Lmod automatically searches for module files under $HOME/modulefiles and adds the modules it discovers there into the modules tree so they can be found by module spider, loaded by module load, etc.\nMost Linux software can be installed from sources using either Autoconf or CMake configuration tools. These will accept --prefix=/home/$USER/my-software/version or -DCMAKE_INSTALL_PREFIX=/home/$USER/my-software/version.\nSoftware that come as binary archive to be unpacked can be simply unpacked into your home directory location. Then, the paths should be set for the sofware to be found: either by including the environment variable in $HOME/.bashrc or by creating a specific module in $HOME/modulefiles/my-software/version following Lmod instructions for writing Modules .\nThere exist binary software environments like Conda that manage their own tree of binary-everything. These can be used as well, with some caution, because automatically pulling everything might conflict with the same software existing in the HPC environment (Python package paths, MPI libraries, etc.).\nHowever, if a software is really a part of the base OS (something like a graphics Desktop software, etc.), it can be hard to rebuild from sources due to many dependencies. If needed, it may be better if installed centrally or used in a container (see Containers documentation).\nRelated links  Lmod  Tcl Modules  CMake  Autoconf   "}),a.add({id:20,href:'/grex-docs/docs/grex/connecting/data-transfer/',title:"Transferring data",content:"Transferring Data to and from Grex GlobusOnline file transfer Unfortunately, the WestGrid Globus endpoint on Grex had expired. It is not possible to use Globus on Grex as of the time of writing this documentation. However, You can still use Globus to transfer data between Compute Canada systems as described here .\nCheck the ESNet website if you are curious about Globus, and why large data transfers over WAN might need specialized networks and software setups.\nOpenSSH tools: scp, sftp On MacOS and Linux, where OpenSSH client packages are always available, the following command line tools are present: scp, sftp. They work similar to UNIX cp and ftp commands, except that there is a remote target or source.\nSFTP opens a session and then drops the user to a command line, which provides commands like ls, lls, get, put, cd, lcd to navigate the local and remote directories, upload and download files etc.\nsftp someuser@grex.hpc.umanitoba.ca sftp\u0026gt; lls sftp\u0026gt; put myfile.fchk  Please replace someuser with your user name.  SCP behaves like cp. To copy a file myfile.fchk to Grex, from the current directory, into his /global/scratch/, a user would run the following command:\nscp ./myfile.fchk someuser@grex.hpc.umanitoba.ca:/global/scratch/someuser  Note that the destination is remote (for it has the form of user@host:/path). More information about file transfer tools exist on Compute Canada documentation LFTP tool LFTP is a multi-protocol file tansfer code for Linux, that supports some of the advanced features of Globus, enabling better bandwidth utilization through socket tuning and using multiple streams. On Grex (and between Grex and Compute Canada systems) only SFTP (that is, sftp:// URIs) is supported! So the minimal syntax for operning a transfer session from Grex to Cedar would be (on Grex):\nlftp sftp://someuser@cedar.computecanada.ca  It has a command line interface not unlike the ftp or sftp command line tools, with ls, get, and put commands.\nFile transfer clients with GUI There are many file transfer clients that provide convenient graphical user interface.\nSome examples of the popular file transfer clients are\n WinSCP for Windows. CyberDuck for MacOS X crossplatform FileZilla Client   Other GUI clients will work with Grex too, as long as they provide SFTP protocol support.\nTo use such clients, one would need to tell them that SFTP is needed, and to provide the address, which is grex.hpc.umanitoba.ca and your Grex/Alliance (Compute Canada) username.\nNote that we advise against saving your password in the clients: first, it is less secure, and second, it is easy to store a wrong password. FIle transfer clients would try to autoconnect automatically, and having a wrong password stored with them will create many failed connection attempts from your client machine, which in turn might temporarily block your IP address from accessing Grex.\nFile transfers with OOD browser GUI NEW: It is now possible to use OpenOnDemand on aurochs Web interface to download and upload data to and from Grex. Use Files dashboard menu to select a filesystem (currently /home/$USER and /global/scratch/$USER are available), and then Upload and Download buttons.\nThere is a limit of about 10GB to the file transfer sizes with OOD. The OOD interface is, as of now, open for UManitoba IP addresses only (i.e., machines from campus and on UM PulseVPN will work).\nMore information is available on our OOD pages "}),a.add({id:21,href:'/grex-docs/docs/grex/software/code-development/',title:"Code Development on Grex",content:"Code Developing on Grex Grex comes with a sizeable software stack that contains most of the software development environment for typical HPC applications. This section of the documentation covers the best practices for compiling and building your own software on Grex.\nThe login nodes of Grex can be used to compile codes and to run short interactive and/or test runs. All other jobs must be submitted to the batch system. We do not do as heavy resource limiting on Grex login nodes as, for example, Compute Canada does; so code development on login nodes is entirely possible. However, it might still make sense to perform some of the code development in interactive jobs, in cases of a) the build process and/or tests requires heavy, many-core computations and/or b) you need access to specific hardware not present on the login nodes, such as GPUs and AVX512 CPUs.\nMost of the software on Grex is available through environmental modules. To find a software development tool or a library to build your code against, the module spider command is a good start. The applications software is usually installed by us from sources, into sub-directories under /global/software/cent7\nIt is almost always better to use communication libraries (MPI) provided on Grex rather than building your own, because ensuring tight integration of these libraries with our SLURM scheduler and low-level, interconnect-specific libraries might be tricky.\nGeneral CentOS-7 notes The base operating system on Grex is CentOS 7.x that comes with its set of development tools. However, due to the philosophy of CentOS, the tools are usually rather old. For example, cmake and git are of ancient versions of 2.8, 1.7 correspondingly. Therefore, even for these basic tools you more likely want to load a module with newest versions of these tools:\nmodule load git\nmodule load cmake\n CentOS also has its system versions of Python, Perl, and GCC compilers. When no modules loaded, the binaries of these will be available in the PATH. The purpose of these is to make some systems scripts possible, to compile OS packages, drivers and so on.\nWe do not install many packages for the dynamic languages system-wide, because it makes maintaining different versions of them complicated. The same adivce applies: use the module spider command to find a version of Perl, Python, R, etc. to suit your needs. The same applies to compiler suites like GCC and Intel.\nWe do install CentOS packages with OS that are:\n base OS things necessary for functioning graphical libraries that have many dependencies never change versions that are not critical for performance and/or security.    Here are some examples: FLTK, libjpeg, PCRE, Qt4 and Gtk. Login nodes of Grex have many \u0026lsquo;'-devel\u0026rsquo;' packages installed, while compute nodes do not because we want them lean and quickly reinstallable. Therefore, compiling codes that requires \u0026lsquo;'-devel\u0026rsquo;' base OS packages might fail on compute nodes. Contact us us if something like taht happens when compiling or running your applications.   Finally, because HPC machines are shared systems and users do not have \u0026lsquo;\u0026lsquo;sudo\u0026rsquo;\u0026rsquo; access, following some instructions from a Web page that asks for \u0026lsquo;\u0026lsquo;apt-get install this '\u0026rsquo; or \u0026lsquo;\u0026lsquo;yum install that\u0026rsquo;\u0026rsquo; will fail. Rather, module spider should be used to see if the package you want is already installed and available as a module.\nCompilers and Toolchains Due to the hierarchical nature of our Lmod modules system, compilers and certain core libraries (MPI and CUDA) form toolchains. Normally, you would need to chose a compiler suite (Intel or GCC) and, in case of parallel applications, a MPI library (OpenMPI or IntelMPI). These come in different versions. Also, you\u0026rsquo;d want to know if you want CUDA should your applications be able to utilize GPUs. A combination of compiler/version, MPI/version and possibly CUDA makes a toolchain. Toolchains are mutually exclusive; you cannot mix software items compiled with different toolchains!\nSee Modules for more information.\nThere is no module loaded by defaulti! There will be only system\u0026rsquo;s GCC-4.8 and no MPI whatsoever. To get started, load a compiler/version. Then, if necessary, an MPI (ompi or impi) and if necessary, CUDA (for which 10.2 is the current version, there is no known reason to use another).\nmodule load intel/2019.5\nmodule load ompi/3.1.4\n The above loads Intel compilers and OpenMPI 3.1.4. The example below is for GCC 7 and openmpi 4.1.2.\nmodule load gcc/7.4\nmodule load ompi/4.1.2\n The MPI wrappers (mpicc, mpicxx, mpif90, \u0026hellip; etc.) will be set correctly by ompi modules to point to the right compiler.\nIntel compilers suite At the moment of writing this documentation, the following Intel Compilers Suites are available on Grex:\n Intel 2020.4 : a recent Intel Parallel Studio suite. Intel 2019.5 : a recent Intel Parallel Studio suite. Most software is compiled with it, so use this one if unsure. Intel 2017.8 : a somewhat less recent Intel Parallel studio suite. Intel 15.0 : Legacy, for maintenance of older Grex software. Do not use for anything new, unless absolutely must. Intel 14.1 : a very old one, for maintenance of older Grex software, and broken. Do not use. It will be removed soon. Intel 12.1 : a very old one, for maintenance of a very old PETSc version. Do not use.  The name for the Intel suite modules is intel; module spider intel is the command to find available Intel versions. The later is left for compatibility with legacy codes. It does not work with systems C++ standard libraries well, so icpc for Intel 12.1 might be dysfunctional. So the intel/12.1 toolchain is actually using GCC 4.8\u0026rsquo;s C++ and C compilers.\nIf unsure, or do not have a special reason otherwise, use Intel 15.0 compilers (icc, icpc, ifort). Intel 15.0 is probably the first Intel compiler to support AVX512 if you are going to use the contributed nodes that have AVX512 architecture.\nThe Intel compilers suite also provides tools and libraries such as MKL (Linear Algebra, FFT, etc.), Intel Performance Primitives (IPP), Intel Threads Building Blocks (TBB), and VTune . Intel MPI as well as MKL for GCC compilers are available as separate modules, should they be needed for use separately.\nGCC compilers suite At the moment of writing this page, the following GCC compilers are available:\n GCC 11.2 GCC 9.2 GCC 7.4 GCC 5.2 GCC 4.8  The name for GCC is gcc, as in module spider gcc. The GCC 4.8 is a place holder module; its use is to unload any other modules of the compiler family, to avoid toolchains conflicts. Also, GCC 4.8 is the only multilib GCC compiler around; all the others are strictly 64-bit, and thus unable to compile legacy 32-bit programs.\nFor utilizing of the AVX512 instructions, probably the best way is to go with the latest GCC compilers (9.2 and 7.4) and latest MKL. GCC 4.8 does not handle AVX512. Generally Intel compilers outperform GCC, but GCC might have better support for the recent C++11,14,17 standards.\nMPI and Interconnect libraries The standard distribution of MPI on Grex is OpenMPI . We build most of the software with it. To keep compatibility with the old Grex software stack, we name the modules ompi. MPI modules depend on the compiler they were built with, which means, that a compiler module should be loaded first; then the dependent MPI modules will become available as well. Changing the compiler module will trigger automatic MPI module reload. This is how Lmod hierarchy works now.\nFor a long time Grex was using the interconnect drivers with ibverbs packages from the IB hardware vendor, Mellanox. It is no longer the case: for CentOS-7, we have switched to the vanilla Linux infiniband drivers, the open source RDMA-core package, and OpenUCX libraries. The current version of UCX on Grex is 1.6.1. Recent versions of OpenMPI (3.1.x and 4.0.x) do support UCX. Also, our OpenMPI is built with process management interface versions PMI1, PMIx2 and 3, for tight integration with the SLURM scheduler.\nThe current default and recommended version of MPI is OpenMPI 4.1.1. OpenMPI 4.1 works well for new codes but could break old ones There is an older version, OpenMPI 3.1.4 or 3.1.6 that is more compatible. A very old OpenMPI 1.6.5 exists for compatibily with older software.\nmodule load ompi/3.1.4  There is also IntelMPI, for which the modules are named impi. See the notes on running MPI applications under SLURM here .\nAll MPI modules, be that OpenMPI or Intel, will set MPI compiler wrappers such as mpicc, mpicxx, mpif90 to the compiler suite they were built with. The typical workflow for building parallel programs with MPI would be to first load a compiler module, then an MPI module, and then use the wrapper of C, C++ or Fortran in your makefile or build script.\nIn case a build or configure script does not want to use the wrapper and needs explicit compiler and link options for MPI, OpenMPI wrappers provide the --show option that list the required command line options. Try for example:\nmpicc \u0026ndash;show  to print include and library flags to the C compiler to be linked against currently loaded OpenMPI version.\nLinear Algebra BLAS/LAPACK It is always a bad idea to use the reference BLAS/LAPACK/CLAPACK from Netlib (or the generic -lblas, -llapack from CentOS or EPEL which also likely is the reference BLAS/LAPACK from Netlib). The physical Computer architecture has much evolved, and is now way different from the logical Computer the human programmer is presented with. Todays, it takes careful, manual assembly coding optimization to implement BLAS/LAPACK that performs fast on modern CPUs with their memory hierarchies, instruction prefetching and speculative execution. A vendor-optimized BLAS/LAPACK implementation should always be used. For the Intel/AMD architectures, it is Intel MKL, OpenBLAS, and BLIS.\nALso, it is worth noting that the linear algebra libraries might come with two versions: one 32 bit array indexes, another of full 64 bit. Users must pay attention and link against the proper version for their software (that is, a Fortran codes with -i8 or -fdefault-integer-8 would link against 64 bit pointers BLAS).\nMKL The fastest BLAS/LAPACK implementation from Intel. With Intel compilers, it can be used as a convenient compiler flag, -mkl or if threaded version is not needed, -mkl=sequential.\nWith both Intel and GCC compilers, the MKL libraries can be linked explicitly with compiler/linker options. The base path for MKL includes and libraries is defined as the MKLROOT environment variable. For GCC compilers, module load mkl is needed to add MKLROOT to the environment. There is a command line advisor Website to pick the correct order and libraries. Libraries with the _ilp64 suffix are for 64 bit indexes while _lp64 are for the default, 32 bit indexes.\nNote that when Threaded MKL is used, the number of threads is controlled with MKL_NUM_THREADS environment variable. On Grex software stack, it is set by the MKL module to 1 to prevent accidental CPU over-subscription. Redefine it in your SLURM job scripts if you really need threaded MKL execution as follows:\nexport MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK  We use the MKL\u0026rsquo;s BLAS and LAPACK for compiling R and Python\u0026rsquo;s NumPY package on Grex, and thats one example when threaded MKL can speed up computations if the code spends significant time in linear algebra routines by using SMP.\nNote that MKL also provides ScaLAPACK and FFTW libraries.\nOpenBLAS The successor and continuation of the famous GotoBLAS2 library. It contains both BLAS and LAPACK in a sigle library, libopenblas.a . Use \u0026lsquo;\u0026lsquo;module spider openblas\u0026rsquo;\u0026rsquo; to find available versions for a given compiler suite. We provide both 32 bit and 64 bit indexes versions (and reflect ot in the version names, like openblas/0.3.7-i32). The performance of OpenBLAS is close to that of MKL.\nBLIS Blis is a recent, C++ template based implementation of linear algebra that contains a BLAS interface. On Grex, only 32 bit indexes BLIS is available at the moment. Use \u0026lsquo;\u0026lsquo;module spider blis\u0026rsquo;\u0026rsquo; to see how to load it.\nScaLAPACK MKL has ScaLAPACK included. Note that it depends on BLACS which in turn depends on an MPI version. MKL comes with support of OpenMPI and IntelMPI for the BLACS layer; it is necessary to pick the right library of them to link against.\nThe command line advisor is helpful for that.\nFast Fourier Transoform (FFTW) FFTW3 is the standard and well performing implementation of FFT. module spider fftw should find it. There is parallel version of the FFTW3 that depends on MPI it uses, thus to load the fftw module, compiler and MPI modules would have to be loaded first. MKL also provides FFTW bindings, which can be used as follows:\nEither of Intel or GCC MKL modules would set the MKLROOT environment variable, and add necessary directories to LD_LIBRARY_PATH. The MKLROOT is handy when using explicit linking against libraries. It can be useful if you want to select a particular compiler (Intel or GCC), pointer width (the corresponding libraries have suffix _lp64 for 32 bit pointers and_ilp64 for 64 bit ones; the later is needed for, for example, Fortran codes with INTEGER*8 array indexes, explicit or set by -i8 compiler option) and kind of MPI library to be used in BLACS (OpenMPI or IntelMPI which both are available on Grex). An example of the linker options to link against sequential, 64 bit pointed vesion of BLAS, LAPACK for an Intel Fortran code is:\nifort -O2 -i8 main.f -L$MKLROOT/lib/intel64 -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core -lpthread -lm  MKL also has FFTW bindings. They have to be enabled separately from the general Intel compilers installation; and therefore details of the usage might be different between different clusters. On Grex, these libraries are pesent in two versions: 32 bit pointers (libfftw3xf_intel_lp64) and 64 bit pointers (fftw3xf_intel_ilp64). To link against these FFT libraries, the following include and library options to the compilers can be used (for the _lp64 case):\n-I$MKLROOT/include/fftw -I$MKLROOT/interfaces/fftw3xf -L$MKLROOT/interfaces/fftw3xf -lfftw3xf_intel_lp64  The above line is, admittedly, rather elaborate but give the benefit of compiling and building all of the code with MKL, without the need for maintaining a separtate library such as FFTW3.\nHDF5 and NetCDF Popular hierarchical data formats. Two versions exist on the Grex software stack, one serial and another MPI-dependent version. Which one you load depends whether MPI is loaded.\nTo see the available versions, use:\nmodule spider hdf5  and/or:\nmodule spider netcdf  Python There are Conda python modules and the Python built from sources with variety of the compilers. The conda based modules can be distinguished by the module name. Note that the base OS python should in most cases not be used; rather use a module:\nmodule spider python  We do install certain most popular python modules (such as Numpy, Scipy, matplotlib) centrally. pip list and conda list would show the instaled modules\nR We build R from sources and link against MKL. There are Intel 15 versions of R; however we find that some packages would only work with GCC-compiled versions of R because they assume GCC or rely on some C++11 features that the older Intel C++ might be lacking. To find available modules for R, use:\nmodule spider \u0026quot;r\u0026quot;  Several R packages are installed with the R modules on Grex. Note that it is often the case that R packages ar bindings for some other software (JAGS, GEOS, GSL, PROJ, etc.) and require the software or its dynamic libraries to be available at runtime. This means, the modules for the dependencies (JAGS, GEOS, GSL, PROJ) are also to be loaded when R is loaded.\n"}),a.add({id:22,href:'/grex-docs/docs/grex/software/jupyter-notebook/',title:"Using JuPyTer Notebooks",content:"Jupyter on Grex Jupyter is a Web-interface aimed to support interactive data science and scientific computing. Jupyter supports several dynamic languages, most notably Python, R and Julia. Jupyter offers a metaphor of \u0026ldquo;computational document\u0026rdquo; that combines code, data and visualizations, and can be published or shared with collaborators.\nJupyter can be used either as a simple, individual notebook or as a multi-user Webserver/Interactive Development Environment (IDE), such as JupyterHub/JupyterLab. The JupyterHub servers can use a variety of computational back-end configurations: from free-for-all shared workstation to job spawning interfaces to HPC schedulers like SLURM or container workflow systems like Kubernetes.\nThis page lists examples of several ways of accessing Jupyter.\nUsing Notebooks via SSH tunnel and interactive jobs Any Python installation that has Jupyter notebooks installed can be used for the simple notebook interface. Most often, activity on login nodes of HPC systems is limited, so first an interactive batch job should be started. Then, in the interactive job, users would start jupyter notebook server and use SSH tunnel to connect to it from a local Web browser.\n Details and Example for Grex ↕  After loggin on to Grex as usual, issue the following salloc command to start an interactive job:\nsalloc --partition=compute --nodes=1 --ntasks-per-node=2 --time=0-3:00:00  It should give you a command prompt on a compute node. (change parameters like partition, time, \u0026hellip; etc. to your needs). Then, make sure that a Python module is loaded and Jupyter is installed, either in the Python or in a virtualenv, lets start a Notebook server, using an arbitrary port 8765. If the port is already in use, pick another number.\njupyter-notebook --ip 0.0.0.0 --no-browser --port 8765  If succesfull, there should be http://g333:8675/?token=ae348acfa68edec1001bcef58c9abb402e5c7dd2d8c0a0c9 or similar, where g333 is a compute node it runs, 8675 is a local TCP port and token is an access token. Now we have Jupyter notebook server running on the node, but how do we access it from our own browser? To that end, we will need an SSH tunnel.\nAssuming a command line SSH client (OpenSSH or MobaXterm command line window), in a new tab or terminal issue the following:\nssh -fNL 8765:g333:8765 youruser@bison.westgrid.ca  Agan, g333, port 8765 and your user name in the example above should be changed to reflect the actual node and user! When succesfull, the SSH command above returns nothing. Keep the terminal window open for as long as you need the tunnel. Now, the final step is to point your browser (Firefox is the best as Chrome might refuse to do plain http://) to the specified port on localhost or 127.0.0.1, as in http://localhost:8765 or http://127.0.0.1:8765 . Use the token as per above to authenticate into the Jupyter notebook session, either copying it into the prompt or providing in the browser address line.\n   The notebook session will be usable for as long as the interactive (salloc) job is valid and both salloc session and the SSH tunnel connections stay alive. This usually is a limitation on for how long Jupyter notebook calculations can be, in practice.\nThe above mentioned method will work not only on Grex, but on Compute Canada systems as well.\nUsing Notebooks via Grex OOD Web Portal Grex provides Jupyter server as OnDemand Dashboard application. This is much more convenient than handling SSH tunnels manually. The servers will run as a batch job on Grex compute nodes, so as usual, a choice of SLURM partition will be needed.\nPresently, Jupyter for these apps uses an installation of Grex\u0026rsquo;s Python 3.7 module. There are two versions of the app, one for the GCC 7.4 toolchain , another for Intel the 2019.5 toolchain.\nFind out more on how to use OOD on the Grex OOD pages To use R, Julia, and different instances or versions of Pyton, a Jupyter Notebook kernel needs to be installed by each user in their home directories. Check out the corresponding ComputeCanada documentation here .\nOther Jupyter instances around There is SyZyGy instance umanitoba.syzygy.ca that gives a free-for-all shared JupyterHub for UManitoba users.\nMost of the Compute Canada HPC machines deployed JupyterHub as a job (Cedar, Beluga, Narval) or as a free-for-all shared server (Niagara).\n"}),a.add({id:23,href:'/grex-docs/docs/support-contacts/',title:"Support and Training",content:"Compute Canada support The single point support contact for Compute Canada is support@tech.alliancecan.ca .\nEmailing to this address will create a support ticket in the Compute Canada ticketing system. We support both local (Grex) and National resources through Compute Canada support system. This is the main support contact for our HPC group and it is a preferred method (as compared to contacting an HPC analyst directly). If you use your UManitoba email address (email registered in CCDB) to contact Compute Canada support, it will reach us faster because the system will automatically detect it.\nYou can also open a web interface to the ticketing system Compute Canada OTRS . This requires actually having a Compute Canada account to access, so it can be less useful for inquiries on how to get the account .\nTo sum it all up: the main way to contact us is: mailto:support@tech.alliancecan.ca \nSecondary support contacts in the Compute Canada that are useful for particular services:\n cloud@tech.alliancecan.ca for Cloud support. frdr@computecanada.ca for Federated Data Repository FRDR. globus@tech.alliancecan.ca for issues related to Compute Canada\u0026rsquo;s Globus file transfer.   More information on Compute Canada Support: here .\nWe support both local (Grex) and National resources through Compute Canada support ticketing system.\nIST Helpdesk We have an HPC group in IST\u0026rsquo;s HelpDesk system support@umanitoba.ca called Cherwell, so the tickets opened in this system will reach us, especially if you clearly state that they are HPC or Westgrid or Compute Canada related. More info on the IST Helpdesk website.\nIn person training sessions and workshops You can book an in-person session (face to face meeting). We help new users for getting started on the systems we support, or for solving a specific problem that is harder to do over email, or for a consultation about Grex or Compute Canada resources.\nHow to find us on campus:\nE2-588 EITC, FortGarry Campus University of Manitoba Winnipeg, MB R3T 2N2 Telephone: 204-474-9625 Hours: Monday - Friday 8:30 am - 4:30 pm   We also provide small-group workshops on Research Computing topics when there is enough demand. Contact us if you are intrested in having a workshop on a particular topic.\n"}),a.add({id:24,href:'/grex-docs/docs/disclaimer/',title:"Disclaimer",content:"Disclaimer This website is a place for technical information related to certain Research Computing resources, maintained for the benefit of the researchers at the University of Manitoba and their external collaborators. The information which is technical in nature, represents advice on best practices of using the Research Computing resources. Parts of the website are preliminary/draft texts released provisionally, in order to speed up the documentation process, and may contain inaccuracies and errors.  We advise users to exercise reason when following the advice on these pages. We disclaim responsibility for any harm, loss of data, loss of good will or whatever negative effects might happen due to reading this documentation.  The website does not represents official views, policies, or standards of University of Manitoba, WestGrid, or Compute Canada. Please refer to the official sites of the above mentioned institutions.  The website does not represents official views of any hardware, software or services vendors that might be mentioned on the website\u0026rsquo;s pages.  "}),a.add({id:25,href:'/grex-docs/docs/grex/software/specific/',title:"Software-specific notes",content:"Software specific notes This page refers to usage of some programs installed on Grex, like ORCA, VASP, \u0026hellip; etc.\nGaussian   LAMMPS   MATLAB   NWCHEM   ORCA   Priroda   VASP     Work in progress.  "}),a.add({id:26,href:'/grex-docs/posts/atestofpost/',title:"A New Blog post",content:"Introduction I do not know if adding a text file with date would just place it right\nSome Definitions There are a few concepts that you need to understand before creating a theme.\nDon\u0026rsquo;t Repeat Yourself DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"}),a.add({id:27,href:'/grex-docs/posts/creating-a-new-theme/',title:"Creating a New Theme",content:"Introduction This tutorial will show you how to create a simple theme in Hugo. I assume that you are familiar with HTML, the bash command line, and that you are comfortable using Markdown to format content. I\u0026rsquo;ll explain how Hugo uses templates and how you can organize your templates to create a theme. I won\u0026rsquo;t cover using CSS to style your theme.\nWe\u0026rsquo;ll start with creating a new site with a very basic template. Then we\u0026rsquo;ll add in a few pages and posts. With small variations on that, you will be able to create many different types of web sites.\nIn this tutorial, commands that you enter will start with the \u0026ldquo;$\u0026rdquo; prompt. The output will follow. Lines that start with \u0026ldquo;#\u0026rdquo; are comments that I\u0026rsquo;ve added to explain a point. When I show updates to a file, the \u0026ldquo;:wq\u0026rdquo; on the last line means to save the file.\nHere\u0026rsquo;s an example:\n## this is a comment $ echo this is a command this is a command ## edit the file $ vi foo.md +++ date = \u0026quot;2014-09-28\u0026quot; title = \u0026quot;creating a new theme\u0026quot; +++ bah and humbug :wq ## show it $ cat foo.md +++ date = \u0026quot;2014-09-28\u0026quot; title = \u0026quot;creating a new theme\u0026quot; +++ bah and humbug $ Some Definitions There are a few concepts that you need to understand before creating a theme.\nSkins Skins are the files responsible for the look and feel of your site. It’s the CSS that controls colors and fonts, it’s the Javascript that determines actions and reactions. It’s also the rules that Hugo uses to transform your content into the HTML that the site will serve to visitors.\nYou have two ways to create a skin. The simplest way is to create it in the layouts/ directory. If you do, then you don’t have to worry about configuring Hugo to recognize it. The first place that Hugo will look for rules and files is in the layouts/ directory so it will always find the skin.\nYour second choice is to create it in a sub-directory of the themes/ directory. If you do, then you must always tell Hugo where to search for the skin. It’s extra work, though, so why bother with it?\nThe difference between creating a skin in layouts/ and creating it in themes/ is very subtle. A skin in layouts/ can’t be customized without updating the templates and static files that it is built from. A skin created in themes/, on the other hand, can be and that makes it easier for other people to use it.\nThe rest of this tutorial will call a skin created in the themes/ directory a theme.\nNote that you can use this tutorial to create a skin in the layouts/ directory if you wish to. The main difference will be that you won’t need to update the site’s configuration file to use a theme.\nThe Home Page The home page, or landing page, is the first page that many visitors to a site see. It is the index.html file in the root directory of the web site. Since Hugo writes files to the public/ directory, our home page is public/index.html.\nSite Configuration File When Hugo runs, it looks for a configuration file that contains settings that override default values for the entire site. The file can use TOML, YAML, or JSON. I prefer to use TOML for my configuration files. If you prefer to use JSON or YAML, you’ll need to translate my examples. You’ll also need to change the name of the file since Hugo uses the extension to determine how to process it.\nHugo translates Markdown files into HTML. By default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent Content is stored in text files that contain two sections. The first section is the “front matter,” which is the meta-information on the content. The second section contains Markdown that will be converted to HTML.\nFront Matter The front matter is information about the content. Like the configuration file, it can be written in TOML, YAML, or JSON. Unlike the configuration file, Hugo doesn’t use the file’s extension to know the format. It looks for markers to signal the type. TOML is surrounded by “+++”, YAML by “---”, and JSON is enclosed in curly braces. I prefer to use TOML, so you’ll need to translate my examples if you prefer YAML or JSON.\nThe information in the front matter is passed into the template before the content is rendered into HTML.\nMarkdown Content is written in Markdown which makes it easier to create the content. Hugo runs the content through a Markdown engine to create the HTML which will be written to the output file.\nTemplate Files Hugo uses template files to render content into HTML. Template files are a bridge between the content and presentation. Rules in the template define what content is published, where it\u0026rsquo;s published to, and how it will rendered to the HTML file. The template guides the presentation by specifying the style to use.\nThere are three types of templates: single, list, and partial. Each type takes a bit of content as input and transforms it based on the commands in the template.\nHugo uses its knowledge of the content to find the template file used to render the content. If it can’t find a template that is an exact match for the content, it will shift up a level and search from there. It will continue to do so until it finds a matching template or runs out of templates to try. If it can’t find a template, it will use the default template for the site.\nPlease note that you can use the front matter to influence Hugo’s choice of templates.\nSingle Template A single template is used to render a single piece of content. For example, an article or post would be a single piece of content and use a single template.\nList Template A list template renders a group of related content. That could be a summary of recent postings or all articles in a category. List templates can contain multiple groups.\nThe homepage template is a special type of list template. Hugo assumes that the home page of your site will act as the portal for the rest of the content in the site.\nPartial Template A partial template is a template that can be included in other templates. Partial templates must be called using the “partial” template command. They are very handy for rolling up common behavior. For example, your site may have a banner that all pages use. Instead of copying the text of the banner into every single and list template, you could create a partial with the banner in it. That way if you decide to change the banner, you only have to change the partial template.\nCreate a New Site Let\u0026rsquo;s use Hugo to create a new web site. I\u0026rsquo;m a Mac user, so I\u0026rsquo;ll create mine in my home directory, in the Sites folder. If you\u0026rsquo;re using Linux, you might have to create the folder first.\nThe \u0026ldquo;new site\u0026rdquo; command will create a skeleton of a site. It will give you the basic directory structure and a useable configuration file.\n$ hugo new site ~/Sites/zafta $ cd ~/Sites/zafta $ ls -l total 8 drwxr-xr-x 7 quoha staff 238 Sep 29 16:49 . drwxr-xr-x 3 quoha staff 102 Sep 29 16:49 .. drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ Take a look in the content/ directory to confirm that it is empty.\nThe other directories (archetypes/, layouts/, and static/) are used when customizing a theme. That\u0026rsquo;s a topic for a different tutorial, so please ignore them for now.\nGenerate the HTML For the New Site Running the hugo command with no options will read all the available content and generate the HTML files. It will also copy all static files (that\u0026rsquo;s everything that\u0026rsquo;s not content). Since we have an empty site, it won\u0026rsquo;t do much, but it will do it very quickly.\n$ hugo --verbose INFO: 2014/09/29 Using config file: config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ The \u0026ldquo;--verbose\u0026rdquo; flag gives extra information that will be helpful when we build the template. Every line of the output that starts with \u0026ldquo;INFO:\u0026rdquo; or \u0026ldquo;WARN:\u0026rdquo; is present because we used that flag. The lines that start with \u0026ldquo;WARN:\u0026rdquo; are warning messages. We\u0026rsquo;ll go over them later.\nWe can verify that the command worked by looking at the directory again.\n$ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static $ See that new public/ directory? Hugo placed all generated content there. When you\u0026rsquo;re ready to publish your web site, that\u0026rsquo;s the place to start. For now, though, let\u0026rsquo;s just confirm that we have what we\u0026rsquo;d expect from a site with no content.\n$ ls -l public total 16 -rw-r--r-- 1 quoha staff 416 Sep 29 17:02 index.xml -rw-r--r-- 1 quoha staff 262 Sep 29 17:02 sitemap.xml $ Hugo created two XML files, which is standard, but there are no HTML files.\nTest the New Site Verify that you can run the built-in web server. It will dramatically shorten your development cycle if you do. Start it by running the \u0026ldquo;server\u0026rdquo; command. If it is successful, you will see output similar to the following:\n$ hugo server --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop Connect to the listed URL (it\u0026rsquo;s on the line that starts with \u0026ldquo;Web Server\u0026rdquo;). If everything is working correctly, you should get a page that shows the following:\nindex.xml sitemap.xml That\u0026rsquo;s a listing of your public/ directory. Hugo didn\u0026rsquo;t create a home page because our site has no content. When there\u0026rsquo;s no index.html file in a directory, the server lists the files in the directory, which is what you should see in your browser.\nLet’s go back and look at those warnings again.\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] WARN: 2014/09/29 Unable to locate layout: [404.html] That second warning is easier to explain. We haven’t created a template to be used to generate “page not found errors.” The 404 message is a topic for a separate tutorial.\nNow for the first warning. It is for the home page. You can tell because the first layout that it looked for was “index.html.” That’s only used by the home page.\nI like that the verbose flag causes Hugo to list the files that it\u0026rsquo;s searching for. For the home page, they are index.html, _default/list.html, and _default/single.html. There are some rules that we\u0026rsquo;ll cover later that explain the names and paths. For now, just remember that Hugo couldn\u0026rsquo;t find a template for the home page and it told you so.\nAt this point, you\u0026rsquo;ve got a working installation and site that we can build upon. All that’s left is to add some content and a theme to display it.\nCreate a New Theme Hugo doesn\u0026rsquo;t ship with a default theme. There are a few available (I counted a dozen when I first installed Hugo) and Hugo comes with a command to create new themes.\nWe\u0026rsquo;re going to create a new theme called \u0026ldquo;zafta.\u0026rdquo; Since the goal of this tutorial is to show you how to fill out the files to pull in your content, the theme will not contain any CSS. In other words, ugly but functional.\nAll themes have opinions on content and layout. For example, Zafta uses \u0026ldquo;post\u0026rdquo; over \u0026ldquo;blog\u0026rdquo;. Strong opinions make for simpler templates but differing opinions make it tougher to use themes. When you build a theme, consider using the terms that other themes do.\nCreate a Skeleton Use the hugo \u0026ldquo;new\u0026rdquo; command to create the skeleton of a theme. This creates the directory structure and places empty files for you to fill out.\n$ hugo new theme zafta $ ls -l total 8 drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 archetypes -rw-r--r-- 1 quoha staff 82 Sep 29 16:49 config.toml drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 content drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:02 public drwxr-xr-x 2 quoha staff 68 Sep 29 16:49 static drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes $ find themes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 1081 Sep 29 17:31 themes/zafta/LICENSE.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html -rw-r--r-- 1 quoha staff 93 Sep 29 17:31 themes/zafta/theme.toml $ The skeleton includes templates (the files ending in .html), license file, a description of your theme (the theme.toml file), and an empty archetype.\nPlease take a minute to fill out the theme.toml and LICENSE.md files. They\u0026rsquo;re optional, but if you\u0026rsquo;re going to be distributing your theme, it tells the world who to praise (or blame). It\u0026rsquo;s also nice to declare the license so that people will know how they can use the theme.\n$ vi themes/zafta/theme.toml author = \u0026quot;michael d henderson\u0026quot; description = \u0026quot;a minimal working template\u0026quot; license = \u0026quot;MIT\u0026quot; name = \u0026quot;zafta\u0026quot; source_repo = \u0026quot;\u0026quot; tags = [\u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot;] :wq ## also edit themes/zafta/LICENSE.md and change ## the bit that says \u0026quot;YOUR_NAME_HERE\u0026quot; Note that the the skeleton\u0026rsquo;s template files are empty. Don\u0026rsquo;t worry, we\u0026rsquo;ll be changing that shortly.\n$ find themes/zafta -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/single.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/footer.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/partials/header.html $ Update the Configuration File to Use the Theme Now that we\u0026rsquo;ve got a theme to work with, it\u0026rsquo;s a good idea to add the theme name to the configuration file. This is optional, because you can always add \u0026ldquo;-t zafta\u0026rdquo; on all your commands. I like to put it the configuration file because I like shorter command lines. If you don\u0026rsquo;t put it in the configuration file or specify it on the command line, you won\u0026rsquo;t use the template that you\u0026rsquo;re expecting to.\nEdit the file to add the theme, add a title for the site, and specify that all of our content will use the TOML format.\n$ vi config.toml theme = \u0026quot;zafta\u0026quot; baseurl = \u0026quot;\u0026quot; languageCode = \u0026quot;en-us\u0026quot; title = \u0026quot;zafta - totally refreshing\u0026quot; MetaDataFormat = \u0026quot;toml\u0026quot; :wq $ Generate the Site Now that we have an empty theme, let\u0026rsquo;s generate the site again.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ Did you notice that the output is different? The warning message for the home page has disappeared and we have an additional information line saying that Hugo is syncing from the theme\u0026rsquo;s directory.\nLet\u0026rsquo;s check the public/ directory to see what Hugo\u0026rsquo;s created.\n$ ls -l public total 16 drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 css -rw-r--r-- 1 quoha staff 0 Sep 29 17:56 index.html -rw-r--r-- 1 quoha staff 407 Sep 29 17:56 index.xml drwxr-xr-x 2 quoha staff 68 Sep 29 17:56 js -rw-r--r-- 1 quoha staff 243 Sep 29 17:56 sitemap.xml $ Notice four things:\n Hugo created a home page. This is the file public/index.html. Hugo created a css/ directory. Hugo created a js/ directory. Hugo claimed that it created 0 pages. It created a file and copied over static files, but didn\u0026rsquo;t create any pages. That\u0026rsquo;s because it considers a \u0026ldquo;page\u0026rdquo; to be a file created directly from a content file. It doesn\u0026rsquo;t count things like the index.html files that it creates automatically.  The Home Page Hugo supports many different types of templates. The home page is special because it gets its own type of template and its own template file. The file, layouts/index.html, is used to generate the HTML for the home page. The Hugo documentation says that this is the only required template, but that depends. Hugo\u0026rsquo;s warning message shows that it looks for three different templates:\nWARN: 2014/09/29 Unable to locate layout: [index.html _default/list.html _default/single.html] If it can\u0026rsquo;t find any of these, it completely skips creating the home page. We noticed that when we built the site without having a theme installed.\nWhen Hugo created our theme, it created an empty home page template. Now, when we build the site, Hugo finds the template and uses it to generate the HTML for the home page. Since the template file is empty, the HTML file is empty, too. If the template had any rules in it, then Hugo would have used them to generate the home page.\n$ find . -name index.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 20:21 ./public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 ./themes/zafta/layouts/index.html $ The Magic of Static Hugo does two things when generating the site. It uses templates to transform content into HTML and it copies static files into the site. Unlike content, static files are not transformed. They are copied exactly as they are.\nHugo assumes that your site will use both CSS and JavaScript, so it creates directories in your theme to hold them. Remember opinions? Well, Hugo\u0026rsquo;s opinion is that you\u0026rsquo;ll store your CSS in a directory named css/ and your JavaScript in a directory named js/. If you don\u0026rsquo;t like that, you can change the directory names in your theme directory or even delete them completely. Hugo\u0026rsquo;s nice enough to offer its opinion, then behave nicely if you disagree.\n$ find themes/zafta -type d | xargs ls -ld drwxr-xr-x 7 quoha staff 238 Sep 29 17:38 themes/zafta drwxr-xr-x 3 quoha staff 102 Sep 29 17:31 themes/zafta/archetypes drwxr-xr-x 5 quoha staff 170 Sep 29 17:31 themes/zafta/layouts drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/_default drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/layouts/partials drwxr-xr-x 4 quoha staff 136 Sep 29 17:31 themes/zafta/static drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/css drwxr-xr-x 2 quoha staff 68 Sep 29 17:31 themes/zafta/static/js $ The Theme Development Cycle When you\u0026rsquo;re working on a theme, you will make changes in the theme\u0026rsquo;s directory, rebuild the site, and check your changes in the browser. Hugo makes this very easy:\n Purge the public/ directory. Run the built in web server in watch mode. Open your site in a browser. Update the theme. Glance at your browser window to see changes. Return to step 4.  I’ll throw in one more opinion: never work on a theme on a live site. Always work on a copy of your site. Make changes to your theme, test them, then copy them up to your site. For added safety, use a tool like Git to keep a revision history of your content and your theme. Believe me when I say that it is too easy to lose both your mind and your changes.\nCheck the main Hugo site for information on using Git with Hugo.\nPurge the public/ Directory When generating the site, Hugo will create new files and update existing ones in the public/ directory. It will not delete files that are no longer used. For example, files that were created in the wrong directory or with the wrong title will remain. If you leave them, you might get confused by them later. I recommend cleaning out your site prior to generating it.\nNote: If you\u0026rsquo;re building on an SSD, you should ignore this. Churning on a SSD can be costly.\nHugo\u0026rsquo;s Watch Option Hugo\u0026rsquo;s \u0026ldquo;--watch\u0026rdquo; option will monitor the content/ and your theme directories for changes and rebuild the site automatically.\nLive Reload Hugo\u0026rsquo;s built in web server supports live reload. As pages are saved on the server, the browser is told to refresh the page. Usually, this happens faster than you can say, \u0026ldquo;Wow, that\u0026rsquo;s totally amazing.\u0026rdquo;\nDevelopment Commands Use the following commands as the basis for your workflow.\n## purge old files. hugo will recreate the public directory. ## $ rm -rf public ## ## run hugo in watch mode ## $ hugo server --watch --verbose Here\u0026rsquo;s sample output showing Hugo detecting a change to the template for the home page. Once generated, the web browser automatically reloaded the page. I\u0026rsquo;ve said this before, it\u0026rsquo;s amazing.\n$ rm -rf public $ hugo server --watch --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms Watching for changes in /Users/quoha/Sites/zafta/content Serving pages from /Users/quoha/Sites/zafta/public Web Server is available at http://localhost:1313 Press Ctrl+C to stop INFO: 2014/09/29 File System Event: [\u0026quot;/Users/quoha/Sites/zafta/themes/zafta/layouts/index.html\u0026quot;: MODIFY|ATTRIB] Change detected, rebuilding site WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 1 ms Update the Home Page Template The home page is one of a few special pages that Hugo creates automatically. As mentioned earlier, it looks for one of three files in the theme\u0026rsquo;s layout/ directory:\n index.html _default/list.html _default/single.html  We could update one of the default templates, but a good design decision is to update the most specific template available. That\u0026rsquo;s not a hard and fast rule (in fact, we\u0026rsquo;ll break it a few times in this tutorial), but it is a good generalization.\nMake a Static Home Page Right now, that page is empty because we don\u0026rsquo;t have any content and we don\u0026rsquo;t have any logic in the template. Let\u0026rsquo;s change that by adding some text to the template.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and then verify the results.\n$ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 0 pages created 0 tags created 0 categories created in 2 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 21:26 public/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;/html\u0026gt; Live Reload Note: If you\u0026rsquo;re running the server with the --watch option, you\u0026rsquo;ll see different content in the file:\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hugo says hello!\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt;document.write('\u0026lt;script src=\u0026quot;http://' + (location.host || 'localhost').split(':')[0] + ':1313/livereload.js?mindelay=10\u0026quot;\u0026gt;\u0026lt;/' + 'script\u0026gt;')\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When you use --watch, the Live Reload script is added by Hugo. Look for live reload in the documentation to see what it does and how to disable it.\nBuild a \u0026ldquo;Dynamic\u0026rdquo; Home Page \u0026ldquo;Dynamic home page?\u0026rdquo; Hugo\u0026rsquo;s a static web site generator, so this seems an odd thing to say. I mean let\u0026rsquo;s have the home page automatically reflect the content in the site every time Hugo builds it. We\u0026rsquo;ll use iteration in the template to do that.\nCreate New Posts Now that we have the home page generating static content, let\u0026rsquo;s add some content to the site. We\u0026rsquo;ll display these posts as a list on the home page and on their own page, too.\nHugo has a command to generate a skeleton post, just like it does for sites and themes.\n$ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/default.md ERROR: 2014/09/29 Unable to Cast \u0026lt;nil\u0026gt; to map[string]interface{} $ That wasn\u0026rsquo;t very nice, was it?\nThe \u0026ldquo;new\u0026rdquo; command uses an archetype to create the post file. Hugo created an empty default archetype file, but that causes an error when there\u0026rsquo;s a theme. For me, the workaround was to create an archetypes file specifically for the post type.\n$ vi themes/zafta/archetypes/post.md +++ Description = \u0026quot;\u0026quot; Tags = [] Categories = [] +++ :wq $ find themes/zafta/archetypes -type f | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 21:53 themes/zafta/archetypes/default.md -rw-r--r-- 1 quoha staff 51 Sep 29 21:54 themes/zafta/archetypes/post.md $ hugo --verbose new post/first.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/first.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/first.md /Users/quoha/Sites/zafta/content/post/first.md created $ hugo --verbose new post/second.md INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 attempting to create post/second.md of post INFO: 2014/09/29 curpath: /Users/quoha/Sites/zafta/themes/zafta/archetypes/post.md INFO: 2014/09/29 creating /Users/quoha/Sites/zafta/content/post/second.md /Users/quoha/Sites/zafta/content/post/second.md created $ ls -l content/post total 16 -rw-r--r-- 1 quoha staff 104 Sep 29 21:54 first.md -rw-r--r-- 1 quoha staff 105 Sep 29 21:57 second.md $ cat content/post/first.md +++ Categories = [] Description = \u0026quot;\u0026quot; Tags = [] date = \u0026quot;2014-09-29T21:54:53-05:00\u0026quot; title = \u0026quot;first\u0026quot; +++ my first post $ cat content/post/second.md +++ Categories = [] Description = \u0026quot;\u0026quot; Tags = [] date = \u0026quot;2014-09-29T21:57:09-05:00\u0026quot; title = \u0026quot;second\u0026quot; +++ my second post $ Build the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026quot;category\u0026quot;:\u0026quot;categories\u0026quot;, \u0026quot;tag\u0026quot;:\u0026quot;tags\u0026quot;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ The output says that it created 2 pages. Those are our new posts:\n$ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 78 Sep 29 22:13 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:13 public/post/second/index.html $ The new files are empty because because the templates used to generate the content are empty. The homepage doesn\u0026rsquo;t show the new content, either. We have to update the templates to add the posts.\nList and Single Templates In Hugo, we have three major kinds of templates. There\u0026rsquo;s the home page template that we updated previously. It is used only by the home page. We also have \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates that are used to group multiple pieces of content before generating output.\nGenerally speaking, list templates are named \u0026ldquo;list.html\u0026rdquo; and single templates are named \u0026ldquo;single.html.\u0026rdquo;\nThere are three other types of templates: partials, content views, and terms. We will not go into much detail on these.\nAdd Content to the Homepage The home page will contain a list of posts. Let\u0026rsquo;s update its template to add the posts that we just created. The logic in the template will run every time we build the site.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Hugo uses the Go template engine. That engine scans the template files for commands which are enclosed between \u0026ldquo;{{\u0026rdquo; and \u0026ldquo;}}\u0026rdquo;. In our template, the commands are:\n range .Title end  The \u0026ldquo;range\u0026rdquo; command is an iterator. We\u0026rsquo;re going to use it to go through the first ten pages. Every HTML file that Hugo creates is treated as a page, so looping through the list of pages will look at every file that will be created.\nThe \u0026ldquo;.Title\u0026rdquo; command prints the value of the \u0026ldquo;title\u0026rdquo; variable. Hugo pulls it from the front matter in the Markdown file.\nThe \u0026ldquo;end\u0026rdquo; command signals the end of the range iterator. The engine loops back to the top of the iteration when it finds \u0026ldquo;end.\u0026rdquo; Everything between the \u0026ldquo;range\u0026rdquo; and \u0026ldquo;end\u0026rdquo; is evaluated every time the engine goes through the iteration. In this file, that would cause the title from the first ten pages to be output as heading level one.\nIt\u0026rsquo;s helpful to remember that some variables, like .Data, are created before any output files. Hugo loads every content file into the variable and then gives the template a chance to process before creating the HTML files.\nBuild the web site and then verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026quot;tag\u0026quot;:\u0026quot;tags\u0026quot;, \u0026quot;category\u0026quot;:\u0026quot;categories\u0026quot;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:23 public/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:23 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Congratulations, the home page shows the title of the two posts. The posts themselves are still empty, but let\u0026rsquo;s take a moment to appreciate what we\u0026rsquo;ve done. Your template now generates output dynamically. Believe it or not, by inserting the range command inside of those curly braces, you\u0026rsquo;ve learned everything you need to know to build a theme. All that\u0026rsquo;s really left is understanding which template will be used to generate each content file and becoming familiar with the commands for the template engine.\nAnd, if that were entirely true, this tutorial would be much shorter. There are a few things to know that will make creating a new template much easier. Don\u0026rsquo;t worry, though, that\u0026rsquo;s all to come.\nAdd Content to the Posts We\u0026rsquo;re working with posts, which are in the content/post/ directory. That means that their section is \u0026ldquo;post\u0026rdquo; (and if we don\u0026rsquo;t do something weird, their type is also \u0026ldquo;post\u0026rdquo;).\nHugo uses the section and type to find the template file for every piece of content. Hugo will first look for a template file that matches the section or type name. If it can\u0026rsquo;t find one, then it will look in the _default/ directory. There are some twists that we\u0026rsquo;ll cover when we get to categories and tags, but for now we can assume that Hugo will try post/single.html, then _default/single.html.\nNow that we know the search rule, let\u0026rsquo;s see what we actually have available:\n$ find themes/zafta -name single.html | xargs ls -l -rw-r--r-- 1 quoha staff 132 Sep 29 17:31 themes/zafta/layouts/_default/single.html We could create a new template, post/single.html, or change the default. Since we don\u0026rsquo;t know of any other content types, let\u0026rsquo;s start with updating the default.\nRemember, any content that we haven\u0026rsquo;t created a template for will end up using this template. That can be good or bad. Bad because I know that we\u0026rsquo;re going to be adding different types of content and we\u0026rsquo;re going to end up undoing some of the changes we\u0026rsquo;ve made. It\u0026rsquo;s good because we\u0026rsquo;ll be able to see immediate results. It\u0026rsquo;s also good to start here because we can start to build the basic layout for the site. As we add more content types, we\u0026rsquo;ll refactor this file and move logic around. Hugo makes that fairly painless, so we\u0026rsquo;ll accept the cost and proceed.\nPlease see the Hugo documentation on template rendering for all the details on determining which template to use. And, as the docs mention, if you\u0026rsquo;re building a single page application (SPA) web site, you can delete all of the other templates and work with just the default single page. That\u0026rsquo;s a refreshing amount of joy right there.\nUpdate the Template File $ vi themes/zafta/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq $ Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026quot;tag\u0026quot;:\u0026quot;tags\u0026quot;, \u0026quot;category\u0026quot;:\u0026quot;categories\u0026quot;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 94 Sep 29 22:40 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:40 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:40 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:40 public/post/second/index.html $ cat public/post/first/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;first\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;first\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my first post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ cat public/post/second/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;second\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;second\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;my second post\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Notice that the posts now have content. You can go to localhost:1313/post/first to verify.\nLinking to Content The posts are on the home page. Let\u0026rsquo;s add a link from there to the post. Since this is the home page, we\u0026rsquo;ll update its template.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;{{ .Permalink }}\u0026quot;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Build the web site and verify the results.\n$ rm -rf public $ hugo --verbose INFO: 2014/09/29 Using config file: /Users/quoha/Sites/zafta/config.toml INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/themes/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 syncing from /Users/quoha/Sites/zafta/static/ to /Users/quoha/Sites/zafta/public/ INFO: 2014/09/29 found taxonomies: map[string]string{\u0026quot;tag\u0026quot;:\u0026quot;tags\u0026quot;, \u0026quot;category\u0026quot;:\u0026quot;categories\u0026quot;} WARN: 2014/09/29 Unable to locate layout: [404.html theme/404.html] 0 draft content 0 future content 2 pages created 0 tags created 0 categories created in 4 ms $ find public -type f -name '*.html' | xargs ls -l -rw-r--r-- 1 quoha staff 149 Sep 29 22:44 public/index.html -rw-r--r-- 1 quoha staff 125 Sep 29 22:44 public/post/first/index.html -rw-r--r-- 1 quoha staff 0 Sep 29 22:44 public/post/index.html -rw-r--r-- 1 quoha staff 128 Sep 29 22:44 public/post/second/index.html $ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;/post/second/\u0026quot;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;/post/first/\u0026quot;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ Create a Post Listing We have the posts displaying on the home page and on their own page. We also have a file public/post/index.html that is empty. Let\u0026rsquo;s make it show a list of all posts (not just the first ten).\nWe need to decide which template to update. This will be a listing, so it should be a list template. Let\u0026rsquo;s take a quick look and see which list templates are available.\n$ find themes/zafta -name list.html | xargs ls -l -rw-r--r-- 1 quoha staff 0 Sep 29 17:31 themes/zafta/layouts/_default/list.html As with the single post, we have to decide to update _default/list.html or create post/list.html. We still don\u0026rsquo;t have multiple content types, so let\u0026rsquo;s stay consistent and update the default list template.\nCreating Top Level Pages Let\u0026rsquo;s add an \u0026ldquo;about\u0026rdquo; page and display it at the top level (as opposed to a sub-level like we did with posts).\nThe default in Hugo is to use the directory structure of the content/ directory to guide the location of the generated html in the public/ directory. Let\u0026rsquo;s verify that by creating an \u0026ldquo;about\u0026rdquo; page at the top level:\n$ vi content/about.md +++ title = \u0026quot;about\u0026quot; description = \u0026quot;about this site\u0026quot; date = \u0026quot;2014-09-27\u0026quot; slug = \u0026quot;about time\u0026quot; +++ ## about us i'm speechless :wq Generate the web site and verify the results.\n$ find public -name '*.html' | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:08 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 527 Sep 27 15:08 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:08 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:08 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:08 public/post/second-post/index.html Notice that the page wasn\u0026rsquo;t created at the top level. It was created in a sub-directory named \u0026lsquo;about-time/\u0026rsquo;. That name came from our slug. Hugo will use the slug to name the generated content. It\u0026rsquo;s a reasonable default, by the way, but we can learn a few things by fighting it for this file.\nOne other thing. Take a look at the home page.\n$ cat public/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;http://localhost:1313/post/theme/\u0026quot;\u0026gt;creating a new theme\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;http://localhost:1313/about-time/\u0026quot;\u0026gt;about\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;http://localhost:1313/post/second-post/\u0026quot;\u0026gt;second\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026quot;http://localhost:1313/post/first-post/\u0026quot;\u0026gt;first\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt;document.write('\u0026lt;script src=\u0026quot;http://' + (location.host || 'localhost').split(':')[0] + ':1313/livereload.js?mindelay=10\u0026quot;\u0026gt;\u0026lt;/' + 'script\u0026gt;')\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Notice that the \u0026ldquo;about\u0026rdquo; link is listed with the posts? That\u0026rsquo;s not desirable, so let\u0026rsquo;s change that first.\n$ vi themes/zafta/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026quot;post\u0026quot;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026quot;{{ .Permalink }}\u0026quot;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if eq .Type \u0026quot;page\u0026quot; }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026quot;{{ .Permalink }}\u0026quot;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Generate the web site and verify the results. The home page has two sections, posts and pages, and each section has the right set of headings and links in it.\nBut, that about page still renders to about-time/index.html.\n$ find public -name '*.html' | xargs ls -l -rw-rw-r-- 1 mdhender staff 334 Sep 27 15:33 public/about-time/index.html -rw-rw-r-- 1 mdhender staff 645 Sep 27 15:33 public/index.html -rw-rw-r-- 1 mdhender staff 358 Sep 27 15:33 public/post/first-post/index.html -rw-rw-r-- 1 mdhender staff 0 Sep 27 15:33 public/post/index.html -rw-rw-r-- 1 mdhender staff 342 Sep 27 15:33 public/post/second-post/index.html Knowing that hugo is using the slug to generate the file name, the simplest solution is to change the slug. Let\u0026rsquo;s do it the hard way and change the permalink in the configuration file.\n$ vi config.toml [permalinks] page = \u0026quot;/:title/\u0026quot; about = \u0026quot;/:filename/\u0026quot; Generate the web site and verify that this didn\u0026rsquo;t work. Hugo lets \u0026ldquo;slug\u0026rdquo; or \u0026ldquo;URL\u0026rdquo; override the permalinks setting in the configuration file. Go ahead and comment out the slug in content/about.md, then generate the web site to get it to be created in the right place.\nSharing Templates If you\u0026rsquo;ve been following along, you probably noticed that posts have titles in the browser and the home page doesn\u0026rsquo;t. That\u0026rsquo;s because we didn\u0026rsquo;t put the title in the home page\u0026rsquo;s template (layouts/index.html). That\u0026rsquo;s an easy thing to do, but let\u0026rsquo;s look at a different option.\nWe can put the common bits into a shared template that\u0026rsquo;s stored in the themes/zafta/layouts/partials/ directory.\nCreate the Header and Footer Partials In Hugo, a partial is a sugar-coated template. Normally a template reference has a path specified. Partials are different. Hugo searches for them along a TODO defined search path. This makes it easier for end-users to override the theme\u0026rsquo;s presentation.\n$ vi themes/zafta/layouts/partials/header.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; :wq $ vi themes/zafta/layouts/partials/footer.html \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; :wq Update the Home Page Template to Use the Partials The most noticeable difference between a template call and a partials call is the lack of path:\n{{ template \u0026quot;theme/partials/header.html\u0026quot; . }} versus\n{{ partial \u0026quot;header.html\u0026quot; . }} Both pass in the context.\nLet\u0026rsquo;s change the home page template to use these new partials.\n$ vi themes/zafta/layouts/index.html {{ partial \u0026quot;header.html\u0026quot; . }} \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026quot;post\u0026quot;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026quot;{{ .Permalink }}\u0026quot;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if or (eq .Type \u0026quot;page\u0026quot;) (eq .Type \u0026quot;about\u0026quot;) }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026quot;{{ .Permalink }}\u0026quot;\u0026gt;{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} {{ partial \u0026quot;footer.html\u0026quot; . }} :wq Generate the web site and verify the results. The title on the home page is now \u0026ldquo;your title here\u0026rdquo;, which comes from the \u0026ldquo;title\u0026rdquo; variable in the config.toml file.\nUpdate the Default Single Template to Use the Partials $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026quot;header.html\u0026quot; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026quot;footer.html\u0026quot; . }} :wq Generate the web site and verify the results. The title on the posts and the about page should both reflect the value in the markdown file.\nAdd “Date Published” to Posts It\u0026rsquo;s common to have posts display the date that they were written or published, so let\u0026rsquo;s add that. The front matter of our posts has a variable named \u0026ldquo;date.\u0026rdquo; It\u0026rsquo;s usually the date the content was created, but let\u0026rsquo;s pretend that\u0026rsquo;s the value we want to display.\nAdd “Date Published” to the Template We\u0026rsquo;ll start by updating the template used to render the posts. The template code will look like:\n{{ .Date.Format \u0026quot;Mon, Jan 2, 2006\u0026quot; }} Posts use the default single template, so we\u0026rsquo;ll change that file.\n$ vi themes/zafta/layouts/_default/single.html {{ partial \u0026quot;header.html\u0026quot; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026quot;Mon, Jan 2, 2006\u0026quot; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026quot;footer.html\u0026quot; . }} :wq Generate the web site and verify the results. The posts now have the date displayed in them. There\u0026rsquo;s a problem, though. The \u0026ldquo;about\u0026rdquo; page also has the date displayed.\nAs usual, there are a couple of ways to make the date display only on posts. We could do an \u0026ldquo;if\u0026rdquo; statement like we did on the home page. Another way would be to create a separate template for posts.\nThe \u0026ldquo;if\u0026rdquo; solution works for sites that have just a couple of content types. It aligns with the principle of \u0026ldquo;code for today,\u0026rdquo; too.\nLet\u0026rsquo;s assume, though, that we\u0026rsquo;ve made our site so complex that we feel we have to create a new template type. In Hugo-speak, we\u0026rsquo;re going to create a section template.\nLet\u0026rsquo;s restore the default single template before we forget.\n$ mkdir themes/zafta/layouts/post $ vi themes/zafta/layouts/_default/single.html {{ partial \u0026quot;header.html\u0026quot; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} {{ partial \u0026quot;footer.html\u0026quot; . }} :wq Now we\u0026rsquo;ll update the post\u0026rsquo;s version of the single template. If you remember Hugo\u0026rsquo;s rules, the template engine will use this version over the default.\n$ vi themes/zafta/layouts/post/single.html {{ partial \u0026quot;header.html\u0026quot; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026quot;Mon, Jan 2, 2006\u0026quot; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026quot;footer.html\u0026quot; . }} :wq Note that we removed the date logic from the default template and put it in the post template. Generate the web site and verify the results. Posts have dates and the about page doesn\u0026rsquo;t.\nDon\u0026rsquo;t Repeat Yourself DRY is a good design goal and Hugo does a great job supporting it. Part of the art of a good template is knowing when to add a new template and when to update an existing one. While you\u0026rsquo;re figuring that out, accept that you\u0026rsquo;ll be doing some refactoring. Hugo makes that easy and fast, so it\u0026rsquo;s okay to delay splitting up a template.\n"}),a.add({id:28,href:'/grex-docs/posts/goisforlovers/',title:"(Hu)go Template Primer",content:"Hugo uses the excellent Go html/template library for its template engine. It is an extremely lightweight engine that provides a very small amount of logic. In our experience that it is just the right amount of logic to be able to create a good static website. If you have used other template systems from different languages or frameworks you will find a lot of similarities in Go templates.\nThis document is a brief primer on using Go templates. The Go docs provide more details.\nIntroduction to Go Templates Go templates provide an extremely simple template language. It adheres to the belief that only the most basic of logic belongs in the template or view layer. One consequence of this simplicity is that Go templates parse very quickly.\nA unique characteristic of Go templates is they are content aware. Variables and content will be sanitized depending on the context of where they are used. More details can be found in the Go docs .\nBasic Syntax Golang templates are HTML files with the addition of variables and functions.\nGo variables and functions are accessible within {{ }}\nAccessing a predefined variable \u0026ldquo;foo\u0026rdquo;:\n{{ foo }}  Parameters are separated using spaces\nCalling the add function with input of 1, 2:\n{{ add 1 2 }}  Methods and fields are accessed via dot notation\nAccessing the Page Parameter \u0026ldquo;bar\u0026rdquo;\n{{ .Params.bar }}  Parentheses can be used to group items together\n{{ if or (isset .Params \u0026quot;alt\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;) }} Caption {{ end }}  Variables Each Go template has a struct (object) made available to it. In hugo each template is passed either a page or a node struct depending on which type of page you are rendering. More details are available on the variables page.\nA variable is accessed by referencing the variable name.\n\u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt;  Variables can also be defined and referenced.\n{{ $address := \u0026quot;123 Main St.\u0026quot;}} {{ $address }}  Functions Go template ship with a few functions which provide basic functionality. The Go template system also provides a mechanism for applications to extend the available functions with their own. Hugo template functions provide some additional functionality we believe are useful for building websites. Functions are called by using their name followed by the required parameters separated by spaces. Template functions cannot be added without recompiling hugo.\nExample:\n{{ add 1 2 }}  Includes When including another template you will pass to it the data it will be able to access. To pass along the current context please remember to include a trailing dot. The templates location will always be starting at the /layout/ directory within Hugo.\nExample:\n{{ template \u0026quot;chrome/header.html\u0026quot; . }}  Logic Go templates provide the most basic iteration and conditional logic.\nIteration Just like in Go, the Go templates make heavy use of range to iterate over a map, array or slice. The following are different examples of how to use range.\nExample 1: Using Context\n{{ range array }} {{ . }} {{ end }}  Example 2: Declaring value variable name\n{{range $element := array}} {{ $element }} {{ end }}  Example 2: Declaring key and value variable name\n{{range $index, $element := array}} {{ $index }} {{ $element }} {{ end }}  Conditionals If, else, with, or, \u0026amp; and provide the framework for handling conditional logic in Go Templates. Like range, each statement is closed with end.\nGo Templates treat the following values as false:\n false 0 any array, slice, map, or string of length zero  Example 1: If\n{{ if isset .Params \u0026quot;title\u0026quot; }}\u0026lt;h4\u0026gt;{{ index .Params \u0026quot;title\u0026quot; }}\u0026lt;/h4\u0026gt;{{ end }}  Example 2: If -\u0026gt; Else\n{{ if isset .Params \u0026quot;alt\u0026quot; }} {{ index .Params \u0026quot;alt\u0026quot; }} {{else}} {{ index .Params \u0026quot;caption\u0026quot; }} {{ end }}  Example 3: And \u0026amp; Or\n{{ if and (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}}  Example 4: With\nAn alternative way of writing \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent.\nThe first example above could be simplified as:\n{{ with .Params.title }}\u0026lt;h4\u0026gt;{{ . }}\u0026lt;/h4\u0026gt;{{ end }}  Example 5: If -\u0026gt; Else If\n{{ if isset .Params \u0026quot;alt\u0026quot; }} {{ index .Params \u0026quot;alt\u0026quot; }} {{ else if isset .Params \u0026quot;caption\u0026quot; }} {{ index .Params \u0026quot;caption\u0026quot; }} {{ end }}  Pipes One of the most powerful components of Go templates is the ability to stack actions one after another. This is done by using pipes. Borrowed from unix pipes, the concept is simple, each pipeline\u0026rsquo;s output becomes the input of the following pipe.\nBecause of the very simple syntax of Go templates, the pipe is essential to being able to chain together function calls. One limitation of the pipes is that they only can work with a single value and that value becomes the last parameter of the next pipeline.\nA few simple examples should help convey how to use the pipe.\nExample 1 :\n{{ if eq 1 1 }} Same {{ end }}  is the same as\n{{ eq 1 1 | if }} Same {{ end }}  It does look odd to place the if at the end, but it does provide a good illustration of how to use the pipes.\nExample 2 :\n{{ index .Params \u0026quot;disqus_url\u0026quot; | html }}  Access the page parameter called \u0026ldquo;disqus_url\u0026rdquo; and escape the HTML.\nExample 3 :\n{{ if or (or (isset .Params \u0026quot;title\u0026quot;) (isset .Params \u0026quot;caption\u0026quot;)) (isset .Params \u0026quot;attr\u0026quot;)}} Stuff Here {{ end }}  Could be rewritten as\n{{ isset .Params \u0026quot;caption\u0026quot; | or isset .Params \u0026quot;title\u0026quot; | or isset .Params \u0026quot;attr\u0026quot; | if }} Stuff Here {{ end }}  Context (aka. the dot) The most easily overlooked concept to understand about Go templates is that {{ . }} always refers to the current context. In the top level of your template this will be the data set made available to it. Inside of a iteration it will have the value of the current item. When inside of a loop the context has changed. . will no longer refer to the data available to the entire page. If you need to access this from within the loop you will likely want to set it to a variable instead of depending on the context.\nExample:\n {{ $title := .Site.Title }} {{ range .Params.tags }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026quot;{{ $baseurl }}/tags/{{ . | urlize }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; - {{ $title }} \u0026lt;/li\u0026gt; {{ end }}  Notice how once we have entered the loop the value of {{ . }} has changed. We have defined a variable outside of the loop so we have access to it from within the loop.\nHugo Parameters Hugo provides the option of passing values to the template language through the site configuration (for sitewide values), or through the meta data of each specific piece of content. You can define any values of any type (supported by your front matter/config format) and use them however you want to inside of your templates.\nUsing Content (page) Parameters In each piece of content you can provide variables to be used by the templates. This happens in the front matter .\nAn example of this is used in this documentation site. Most of the pages benefit from having the table of contents provided. Sometimes the TOC just doesn\u0026rsquo;t make a lot of sense. We\u0026rsquo;ve defined a variable in our front matter of some pages to turn off the TOC from being displayed.\nHere is the example front matter:\n--- title: \u0026quot;Permalinks\u0026quot; date: \u0026quot;2013-11-18\u0026quot; aliases: - \u0026quot;/doc/permalinks/\u0026quot; groups: [\u0026quot;extras\u0026quot;] groups_weight: 30 notoc: true --- Here is the corresponding code inside of the template:\n {{ if not .Params.notoc }} \u0026lt;div id=\u0026quot;toc\u0026quot; class=\u0026quot;well col-md-4 col-sm-6\u0026quot;\u0026gt; {{ .TableOfContents }} \u0026lt;/div\u0026gt; {{ end }}  Using Site (config) Parameters In your top-level configuration file (eg, config.yaml) you can define site parameters, which are values which will be available to you in chrome.\nFor instance, you might declare:\nparams:CopyrightHTML:\u0026#34;Copyright \u0026amp;#xA9; 2013 John Doe. All Rights Reserved.\u0026#34;TwitterUser:\u0026#34;spf13\u0026#34;SidebarRecentLimit:5Within a footer layout, you might then declare a \u0026lt;footer\u0026gt; which is only provided if the CopyrightHTML parameter is provided, and if it is given, you would declare it to be HTML-safe, so that the HTML entity is not escaped again. This would let you easily update just your top-level config file each January 1st, instead of hunting through your templates.\n{{if .Site.Params.CopyrightHTML}}\u0026lt;footer\u0026gt; \u0026lt;div class=\u0026quot;text-center\u0026quot;\u0026gt;{{.Site.Params.CopyrightHTML | safeHtml}}\u0026lt;/div\u0026gt; \u0026lt;/footer\u0026gt;{{end}} An alternative way of writing the \u0026ldquo;if\u0026rdquo; and then referencing the same value is to use \u0026ldquo;with\u0026rdquo; instead. With rebinds the context . within its scope, and skips the block if the variable is absent:\n{{with .Site.Params.TwitterUser}}\u0026lt;span class=\u0026quot;twitter\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;https://twitter.com/{{.}}\u0026quot; rel=\u0026quot;author\u0026quot;\u0026gt; \u0026lt;img src=\u0026quot;/images/twitter.png\u0026quot; width=\u0026quot;48\u0026quot; height=\u0026quot;48\u0026quot; title=\u0026quot;Twitter: {{.}}\u0026quot; alt=\u0026quot;Twitter\u0026quot;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt;{{end}} Finally, if you want to pull \u0026ldquo;magic constants\u0026rdquo; out of your layouts, you can do so, such as in this example:\n\u0026lt;nav class=\u0026quot;recent\u0026quot;\u0026gt; \u0026lt;h1\u0026gt;Recent Posts\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt;{{range first .Site.Params.SidebarRecentLimit .Site.Recent}} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;{{.RelPermalink}}\u0026quot;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{end}}\u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; "}),a.add({id:29,href:'/grex-docs/posts/hugoisforlovers/',title:"Getting Started with Hugo",content:"Step 1. Install Hugo Go to Hugo releases and download the appropriate version for your OS and architecture.\nSave it somewhere specific as we will be using it in the next step.\nMore complete instructions are available at Install Hugo Step 2. Build the Docs Hugo has its own example site which happens to also be the documentation site you are reading right now.\nFollow the following steps:\n Clone the Hugo repository  Go into the repo Run hugo in server mode and build the docs Open your browser to http://localhost:1313  Corresponding pseudo commands:\ngit clone https://github.com/spf13/hugo cd hugo /path/to/where/you/installed/hugo server --source=./docs \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Press ctrl+c to stop  Once you\u0026rsquo;ve gotten here, follow along the rest of this page on your local build.\nStep 3. Change the docs site Stop the Hugo process by hitting Ctrl+C.\nNow we are going to run hugo again, but this time with hugo in watch mode.\n/path/to/hugo/from/step/1/hugo server --source=./docs --watch \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 27 ms \u0026gt; Web Server is available at http://localhost:1313 \u0026gt; Watching for changes in /Users/spf13/Code/hugo/docs/content \u0026gt; Press ctrl+c to stop  Open your favorite editor and change one of the source content pages. How about changing this very file to fix the typo. How about changing this very file to fix the typo.\nContent files are found in docs/content/. Unless otherwise specified, files are located at the same relative location as the url, in our case docs/content/overview/quickstart.md.\nChange and save this file.. Notice what happened in your terminal.\n\u0026gt; Change detected, rebuilding site \u0026gt; 29 pages created \u0026gt; 0 tags index created \u0026gt; in 26 ms  Refresh the browser and observe that the typo is now fixed.\nNotice how quick that was. Try to refresh the site before it\u0026rsquo;s finished building. I double dare you. Having nearly instant feedback enables you to have your creativity flow without waiting for long builds.\nStep 4. Have fun The best way to learn something is to play with it.\n"}),a.add({id:30,href:'/grex-docs/posts/migrate-from-jekyll/',title:"Migrate to Hugo from Jekyll",content:"Move static content to static Jekyll has a rule that any directory not starting with _ will be copied as-is to the _site output. Hugo keeps all static content under static. You should therefore move it all there. With Jekyll, something that looked like\n▾ \u0026lt;root\u0026gt;/ ▾ images/ logo.png  should become\n▾ \u0026lt;root\u0026gt;/ ▾ static/ ▾ images/ logo.png  Additionally, you\u0026rsquo;ll want any files that should reside at the root (such as CNAME) to be moved to static.\nCreate your Hugo configuration file Hugo can read your configuration as JSON, YAML or TOML. Hugo supports parameters custom configuration too. Refer to the Hugo configuration documentation for details.\nSet your configuration publish folder to _site The default is for Jekyll to publish to _site and for Hugo to publish to public. If, like me, you have _site mapped to a git submodule on the gh-pages branch , you\u0026rsquo;ll want to do one of two alternatives:\n  Change your submodule to point to map gh-pages to public instead of _site (recommended).\n git submodule deinit _site git rm _site git submodule add -b gh-pages git@github.com:your-username/your-repo.git public    Or, change the Hugo configuration to use _site instead of public.\n { .. \u0026quot;publishdir\u0026quot;: \u0026quot;_site\u0026quot;, .. }    Convert Jekyll templates to Hugo templates That\u0026rsquo;s the bulk of the work right here. The documentation is your friend. You should refer to Jekyll\u0026rsquo;s template documentation if you need to refresh your memory on how you built your blog and Hugo\u0026rsquo;s template to learn Hugo\u0026rsquo;s way.\nAs a single reference data point, converting my templates for heyitsalex.net took me no more than a few hours.\nConvert Jekyll plugins to Hugo shortcodes Jekyll has plugins ; Hugo has shortcodes . It\u0026rsquo;s fairly trivial to do a port.\nImplementation As an example, I was using a custom image_tag plugin to generate figures with caption when running Jekyll. As I read about shortcodes, I found Hugo had a nice built-in shortcode that does exactly the same thing.\nJekyll\u0026rsquo;s plugin:\nmodule Jekyll class ImageTag \u0026lt; Liquid::Tag @url = nil @caption = nil @class = nil @link = nil // Patterns IMAGE_URL_WITH_CLASS_AND_CAPTION = IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;(\\s+)-\u0026gt;((https?:\\/\\/|\\/)(\\S+))(\\s*)/i IMAGE_URL_WITH_CAPTION = /((https?:\\/\\/|\\/)(\\S+))(\\s+)\u0026quot;(.*?)\u0026quot;/i IMAGE_URL_WITH_CLASS = /(\\w+)(\\s+)((https?:\\/\\/|\\/)(\\S+))/i IMAGE_URL = /((https?:\\/\\/|\\/)(\\S+))/i def initialize(tag_name, markup, tokens) super if markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION_AND_LINK @class = $1 @url = $3 @caption = $7 @link = $9 elsif markup =~ IMAGE_URL_WITH_CLASS_AND_CAPTION @class = $1 @url = $3 @caption = $7 elsif markup =~ IMAGE_URL_WITH_CAPTION @url = $1 @caption = $5 elsif markup =~ IMAGE_URL_WITH_CLASS @class = $1 @url = $3 elsif markup =~ IMAGE_URL @url = $1 end end def render(context) if @class source = \u0026quot;\u0026lt;figure class='#{@class}'\u0026gt;\u0026quot; else source = \u0026quot;\u0026lt;figure\u0026gt;\u0026quot; end if @link source += \u0026quot;\u0026lt;a href=\\\u0026quot;#{@link}\\\u0026quot;\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;img src=\\\u0026quot;#{@url}\\\u0026quot;\u0026gt;\u0026quot; if @link source += \u0026quot;\u0026lt;/a\u0026gt;\u0026quot; end source += \u0026quot;\u0026lt;figcaption\u0026gt;#{@caption}\u0026lt;/figcaption\u0026gt;\u0026quot; if @caption source += \u0026quot;\u0026lt;/figure\u0026gt;\u0026quot; source end end end Liquid::Template.register_tag('image', Jekyll::ImageTag)  is written as this Hugo shortcode:\n\u0026lt;!-- image --\u0026gt; \u0026lt;figure {{ with .Get \u0026quot;class\u0026quot; }}class=\u0026quot;{{.}}\u0026quot;{{ end }}\u0026gt; {{ with .Get \u0026quot;link\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt;{{ end }} \u0026lt;img src=\u0026quot;{{ .Get \u0026quot;src\u0026quot; }}\u0026quot; {{ if or (.Get \u0026quot;alt\u0026quot;) (.Get \u0026quot;caption\u0026quot;) }}alt=\u0026quot;{{ with .Get \u0026quot;alt\u0026quot;}}{{.}}{{else}}{{ .Get \u0026quot;caption\u0026quot; }}{{ end }}\u0026quot;{{ end }} /\u0026gt; {{ if .Get \u0026quot;link\u0026quot;}}\u0026lt;/a\u0026gt;{{ end }} {{ if or (or (.Get \u0026quot;title\u0026quot;) (.Get \u0026quot;caption\u0026quot;)) (.Get \u0026quot;attr\u0026quot;)}} \u0026lt;figcaption\u0026gt;{{ if isset .Params \u0026quot;title\u0026quot; }} {{ .Get \u0026quot;title\u0026quot; }}{{ end }} {{ if or (.Get \u0026quot;caption\u0026quot;) (.Get \u0026quot;attr\u0026quot;)}}\u0026lt;p\u0026gt; {{ .Get \u0026quot;caption\u0026quot; }} {{ with .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;a href=\u0026quot;{{.}}\u0026quot;\u0026gt; {{ end }} {{ .Get \u0026quot;attr\u0026quot; }} {{ if .Get \u0026quot;attrlink\u0026quot;}}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/p\u0026gt; {{ end }} \u0026lt;/figcaption\u0026gt; {{ end }} \u0026lt;/figure\u0026gt; \u0026lt;!-- image --\u0026gt;  Usage I simply changed:\n{% image full http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg \u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were \u0026quot;having fun\u0026quot; and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; -\u0026gt;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/ %}  to this (this example uses a slightly extended version named fig, different than the built-in figure):\n{{% fig class=\u0026quot;full\u0026quot; src=\u0026quot;http://farm5.staticflickr.com/4136/4829260124_57712e570a_o_d.jpg\u0026quot; title=\u0026quot;One of my favorite touristy-type photos. I secretly waited for the good light while we were having fun and took this. Only regret: a stupid pole in the top-left corner of the frame I had to clumsily get rid of at post-processing.\u0026quot; link=\u0026quot;http://www.flickr.com/photos/alexnormand/4829260124/in/set-72157624547713078/\u0026quot; %}}  As a bonus, the shortcode named parameters are, arguably, more readable.\nFinishing touches Fix content Depending on the amount of customization that was done with each post with Jekyll, this step will require more or less effort. There are no hard and fast rules here except that hugo server --watch is your friend. Test your changes and fix errors as needed.\nClean up You\u0026rsquo;ll want to remove the Jekyll configuration at this point. If you have anything else that isn\u0026rsquo;t used, delete it.\nA practical example in a diff Hey, it\u0026rsquo;s Alex was migrated in less than a father-with-kids day from Jekyll to Hugo. You can see all the changes (and screw-ups) by looking at this diff .\n"}),a.add({id:31,href:'/grex-docs/docs/',title:"Docs",content:""}),a.add({id:32,href:'/grex-docs/categories/',title:"Categories",content:""}),a.add({id:33,href:'/grex-docs/tags/go/',title:"go",content:""}),a.add({id:34,href:'/grex-docs/tags/golang/',title:"golang",content:""}),a.add({id:35,href:'/grex-docs/categories/golang/',title:"golang",content:""}),a.add({id:36,href:'/grex-docs/tags/hugo/',title:"hugo",content:""}),a.add({id:37,href:'/grex-docs/posts/',title:"Posts",content:""}),a.add({id:38,href:'/grex-docs/tags/',title:"Tags",content:""}),a.add({id:39,href:'/grex-docs/categories/Writing/',title:"Writing",content:""}),a.add({id:40,href:'/grex-docs/tags/writing/',title:"writing",content:""}),a.add({id:41,href:'/grex-docs/categories/Development/',title:"Development",content:""}),a.add({id:42,href:'/grex-docs/tags/development/',title:"development",content:""}),a.add({id:43,href:'/grex-docs/tags/templates/',title:"templates",content:""}),a.add({id:44,href:'/grex-docs/tags/themes/',title:"themes",content:""}),a.add({id:45,href:'/grex-docs/docs/grex/data/data-sharing/',title:"Data Sharing",content:"Data sharing Sharing of accounts login information (like passwords or SSH keys) is stricty forbidden on Grex, as well as on most of the HPC systems. There is a mechanism of data/file sharing that does not require sharing of the accounts. To access each others' data on Grex, the UNIX groups and permissions mechanism can be used as explined below.\nUNIX groups Each UNIX (or Linux) file or directory is owned by an individual user and also by a group (which may be comprised of several users). The permission to access files and directories can be restricted to just the individual owning the file, to the group that owns the file, or access can be unrestricted.\nBy default, each account (username) is set up with an associated UNIX group containing just that single username. So, even if you have set permission for your UNIX group to access files, they are still not being shared with anyone else. You can override the default by using the chmod command to set unrestricted read access to your files. However, if you need more specific control over access, you can ask us to create a special UNIX group containing the usernames of other researchers with whom you want to share data by sending an email to support to ask that a new UNIX group be created for you. Include a list of the users who should be added to that group. One user should be designated as the authority for the group. If a request to join the group is made from someone else, We will ask the designated authority for the permission to add the new researcher to the group. The group name must be of the format wg-xxxxx where xxxxx represents up to five characters. Please indicate your preference for a group name in your email.\nThe group will be set up on Grex system. This may take a day or two to set up. You will get an email whenever you are added or removed from a UNIX group.\nNow that you have a wg-xxxxx UNIX group created, you can set up the data sharing with it, by setting the permissions as described below.\nThe directory you wish to share should be owned by the group and permitted to the group. For example:\nchgrp -R wg-group dir\nchmod g+s dir\n You must ensure that there is access to parent directories as well.\nA directory and all the files in it can be permitted to the group as follows:\nchmod -R g+rX /global/scratch/dirname  Tto set access for the /global/scratch/dirname directory and all its subdirectories. Note the uppercase X in the command. This will set x permissions on the subdirectories (needed for others to list the directories) as well as regular execute permission on executable files.\nIf you want the to allow other members to not only read files in the shared directory \u0026ldquo;dir\u0026rdquo;, but also permit write access to allow them to create and change files in that directory, then all members in the group must add a line:\numask 007  to the ~/.bashrc or ~/.cshrc file in their respective home directories. Furthermore, you must add write permission to the shared directory itself:\nchmod -R g+rwX dir  which would allow read and write access to the directory dir and all its files and subdirectories.\nLinux ACLs On Lustre filesystem (/global/scratch/$USER), it is possible to use Linux access control lists (ACLs) which offer more fine-grained control over access than UNIX groups. Compute Canada\u0026rsquo;s Sharing Data documentation might be relevant, with one caution: on Grex, there is no project layout as exists on Compute Canada clusters.\nAn example setting of ACL command, to allow for \u0026ldquo;search\u0026rdquo; access of the top directory to a group wg-abcdf, presumably with some of the directories under it being shared by a UNUX group:\nsetfacl -m g:wg-abcdf:X /global/scratch/$USER  Related links   Linux permissions   ACLs   "}),a.add({id:46,href:'/grex-docs/docs/grex/data/data-sizes-and-quota/',title:"Data Sizes and Quota",content:"Data sizes and quotas This section explains how to find the actual space and inode usage of your /home/ and /global/scratch allocations on Grex. We limit the size of the data and the number of files that can be stored on these filesystems. To figure out where your current usage stands with the limit, POSIX quota or Lustres' analog, lfs quota commands can be used.\nNFS quota The /home/ filesystem is served by NFSv1 and thus supports the standard POSIX quota command. For the current user, it is just:\nquota  or\nquota -s  The command will result in something like this (note the -s flag added to make units human readable:\n[someuser@grex ~]$ quota -s Disk quotas for user auser (uid 12345): Filesystem space quota limit grace files quota limit grace 192.168.0.1:/exports/home/home 4G 30G 33G 131k 500k 550k   The output is a self explanatory table. There are two values: soft \u0026ldquo;quota\u0026rdquo; and hard \u0026ldquo;limit\u0026rdquo; per each of (space, files). If you are over soft quota, the value of used resource (space or files) will have a star * to it, and grace countdown will be shown. Getting over grace period, or over the hard limit prevents you from writing new data or creating new file on the filesystem. If you are over quota on /home, it is time to do some clean up there, or migrate the data-heavy items to /global/scratch where they belong.\nCAVEAT: Compute Canada breaks the POSIX standard by redefining the quota command in their software stack. So after loading the CCEnv module on Grex, the quota command may return garbage. For accurate output about your quota, load GrexEnv first before running the command quota or use the command diskusage_report (see below).\nLustre quota The /global/scratch/ filesystem is actually a link to a (new) Lustre filesystem called /sbb/. We have retained the old name for compatibility with the old Lustre filesystem that was used on Grex between 2011 and 2017. Lustre filesystem provides a lfs quota sub-command that requires the name of the filesystem specified. So for the current user, the command to get current usage {space and number of files}, in the human-readable units, would be as follows:\nlfs quota -h -u $USER /sbb  With the output:\n[someuser@bison ~]$ lfs quota -h -u $USER /sbb Disk quotas for usr auser (uid 12345): Filesystem used quota limit grace files quota limit grace /sbb 622G 2.644T 3.653T - 5070447 6000000 7000000 -   Presently we do not enforce group or project quota on Grex.\nIf you are over quota on Lustre /global/scratch filesystem, just like for NFS, there will be a star to the value exceeding the limit, and the grace countdown will be active.\nTo make it easier, we have set a custom script with the same name as for Compute Canada clusters, diskusage_report, that gives both /home and /global/scratch quotas (space and number of files: usage/quota or limits), as in the following example:\n[someuser@bison ~]$ diskusage_report Description (FS) Space (U/Q) # of files (U/Q) /home (someuser) 254M/104G 4953/500k /global/scratch (someuser) 131G/2147G 992k/1000k   for more details, run the command: diskusage_report -dd -vv   as in the example: [someuser@bison ~]$ diskusage_report -dd -vv + Space and Inode quotas for user: someuser + Date: Thu Feb 24, 2022 Description (FS) Space (U/Q/L) # of files(U/Q/L) /home (someuser) 254M/104G/110G 4953/500k/1000k /global/scratch (someuser) 131G/2147G/3221G 992k/1000k/1100k FS ==\u0026gt; File System (/home; /global/scratch) U ==\u0026gt; Current Usage (Space, Inode) Q ==\u0026gt; Soft Quota (Space, Inode) L ==\u0026gt; Hard Quota (Space, Inode)   "}),a.add({id:47,href:'/grex-docs/docs/grex/software/specific/gaussian/',title:"Gaussian",content:"Gaussian Introduction Gaussian 16 is a comprehensive suite for electronic structure modeling using ab initio, DFT and semi-empirical methods. A list of Gaussian 16 features can be found here .\nUser Responsibilities and Access University of Manitoba has a site license for Gaussian 16 and GaussView. However it comes with certain license limitations, so access to the code is subject to some license conditions.\nSince, as of now, Compute Canada accounts are a superset of Grex accounts, users will want to initiate getting access by sendiong email agreeing to Gaussian conditions to support@tech.alliancecan.ca, confirming that you have read and agree to abide by the following conditions, and mentioning that you\u0026rsquo;d also want to access it on Grex:\n I am not a member of a research group developing software competitive to Gaussian. I will not copy the Gaussian software, nor make it available to anyone else. I will properly acknowledge Gaussian Inc. and Compute Canada in publications. I will notify Compute Canada of any change in the above acknowledgement.   If you are a sponsored user, your sponsor (PI) must also have such a statement on file with us.\nMoreover, the terms of the UManitoba license are actually stricter than for the Compute Canada. In particular, it excludes certain research groups at the University to have access to the software. Therefore, we are required by Gaussian to have each of the Gaussian user to sign a Confidentiality Agreement form as provided to us by Gaussian. Inc. Please drop by our office in Engineering, E2-588 to get the form and return it signed.\nSystem specific notes On Grex, Gaussian is limited to single node, SMP jobs and the memory of single node. There is no Linda. The Gaussian code is accessible as a module. The module sets Gaussian\u0026rsquo;s environment variables like GAUSS_SCRDIR (the later, to local node scratch).\nmodule load gaussian/g16.c01  To load the module and access the binaries, you will have first get access as per above. Also, our Gaussian license span is less than Compute Canada\u0026rsquo;s support contract, so there is fewer versions available. Use module spider gaussian to see what is available on Grex.\nAfter a Gaussian module is loaded, the GaussView software also becomes available (provided you have connected with X11 support, perhaps using X2Go) as follows:\ngv  The viewer should not be used to run production calculations on a Grex login nodes. Instead, as for any other production calculations, SLURM jobs should be used as described below.\nUsing Gaussian with SLURM Sample SLURM Script #!/bin/bash #SBATCH --ntasks=1 --cpus-per-task=12 #SBATCH --mem=40gb #SBATCH --time=8:00:00 #SBATCH --job-name=Gauss16-test module load GrexEnv module load gaussian/g16.c01 echo \u0026#34;Starting run at: `date`\u0026#34; which g16 # note that input should have %nproc=12 # and %mem=40gb for the above resurce request. g16 \u0026lt; input.gjf \u0026gt; output.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Simplified job sumbittion A simplified job script sbg16 is available (aftr loading of the g16 module) for automatic generation and sumbission of SLURM Gaussian jobs.\nsbg16 input.gjf -ppn 12 -mem 40000mb -time 8:00:00  Using NBO University of Manitoba has site licenses for NBO6 and NBO7. Corresponding NBO modules would have to be loaded in order to use Gaussian\u0026rsquo;s POP=NBO6 or NBO7 keywords.\nmodule spider nbo  should list available NBO versions with their dependencies.\n"}),a.add({id:48,href:'/grex-docs/docs/grex/software/specific/julia/',title:"Julia",content:"Ho to run Julia jobs? Available Julia versions Presently, binary Julia versions 1.3.0, 1.5.4 and 1.6.1 are available. Use module spider julia to find out other versions.\nInstalling packages We do not maintain centralized versions of Julia packages. User should install Julia modules in their home directory.\nThe command is (in Julia REPL):\nUsing Pkg; Pkg.Add(\u0026quot;My-Package\u0026quot;)   In case of package/version conflicts, remove the packages directory ~/.julia/.\nUsing Julia notebooks It is possible to use IJulia kernels for Jupyter notebooks. A preferrable way of running a Jupyter notebook is SLURM interactive job with salloc command.\n(More details coming soon).\nRunning Julia jobs Julia comes with a large variety of packages. Some of them would use threads; and therefore, have to be ran as SMP jobs with \u0026ndash;cpus-per-task specified. Moreover, you would want to set JULIA_NUM_THREADS environment variable in your job script to be the same as SLURM\u0026rsquo;s number of threads.\nUsing Julia for GPU programming It is possible to use Julia with CUDA Array objects to greatly speed up the Julia computations. For more information, please refer to this link: julia-gpu-programming . However, a suitable \u0026ldquo;CUDA\u0026rdquo; module should be loaded during thee installation of the CUDA Julia packages. And you likely want to be on a GPU node when the Julia GPU code is executed.\n"}),a.add({id:49,href:'/grex-docs/docs/grex/software/specific/lammps/',title:"Lammps",content:"LAMMPS Introduction Text Work in progress \u0026hellip;\n#!/bin/bash #SBATCH --ntasks=16  #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=lmp-tst # Load the module: module load intel/2019.5 ompi/3.1.4 lammps/29Sep21 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp_grex lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; srun ${lmp_exec} \u0026lt; ${lmp_input} \u0026gt; ${lmp_output} echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  "}),a.add({id:50,href:'/grex-docs/docs/grex/software/specific/matlab/',title:"Matlab",content:"Matlab on Grex Introduction MATLAB is a general-purpose high-level programming package for numerical work such as linear algebra, signal processing and other calculations involving matrices or vectors of data. We have a campus license for MATLAB which is used on Grex and other local computing resources. MATLAB is available only for UManitoba users.\nAs most of the Grex software, MATLAB is available as module. The following command will load the latest version available on Grex:\nmodule load uofm/matlab  Then the matlab executable will be in the PATH.\nRunning Matlab It is possible to run MATLAB GUI interactively, for best performance in an X2Go session and terminal. There is no Applications menu shortcut for MATLAB, because it is only in the PATH after the module is loaded from command line. After loading the module, the command will be in the PATH:\nmatlab  For running a MATLAB script in text mode, or a batch script, the following options can be used.\nmatlab -nodisplay -nojvm -nodesktop -nosplash -r your_matlab_script.m  However, each instance, GUI or command line, will consume a license unit. By submitting sufficiently many MATLAB jobs concurrently, it is possibly to exaust entire University\u0026rsquo;s license pool. Thus in most cases, it might make sense to use compiled, standalone MATLAB code runners (MCRs) instead.\nStandalone Matlab runners MATLAB comiler, the mcc command can be used to compile source code (.m files) into a standalone excecutable. There is a couple of important considerations to keep in mind when creating an executable that can be run in the batch-oriented, HPC environment. One is that there is no graphical display attached to your session and the other is that the number of threads used by the standalone application has to be controlled.\nFor example, with code mycode.m a source directory src, with the compiled files being written to a directory called deploy, the following mcc command line (at the Linux shell prompt) could be used:\nmkdir deploy \ncd src \nmcc -R -nodisplay -R -singleCompThread -m -v -w enable -d ../deploy mycode.m \n Note the option -singleCompThread has been included in order to limit the executable to just one computational thread.\nIn the deploy directory, an executable mycode will be created along with a script run_mycode.sh. These two files should be copied to the target machine where the code is to be run.\nExample of SLURM script After the standalone executable mycode and corresponding script run_mycode.sh have been transferred to a directory on the target system on which they will be run, a batch job script needs to be created in the same directory. Here is an example batch job script.\n#!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000mb #SBATCH --time=0-3:00:00 #SBATCH --job-name=Matlab-mcr-job # Choose the MCR directory according to the compiler version used # The one below for uofm/matlab/R2017A MCR=/global/software/matlab/mcr/v93 # If running on Grex, uncomment the following line to set MCR_CACHE_ROOT: module load mcr/mcr echo \u0026#34;Running on host: `hostname`\u0026#34; echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; ./run_mycode.sh $MCR \u0026gt; mycode_${SLURM_JOBID}.out echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  The job is then submitted as any ordinary SLURM job with the sbatch command. See the Running Jobs page for more information. If the above script is called matlab.job, it could be submitted using:\nsbatch matlab.job  The specified --time and total memory (--mem-per-cpu) limits should be adjusted to appropriate values for your particular run.\nAn important part of the above script is the location of the MATLAB Compiler Runtime (MCR) directory. This directory contains files necessary for the standalone application to run. The version of the MCR files specified must match the version of MATLAB used to compile the code (check the link for mathing module and MCR versions).\nLinks (Work in progress)\n"}),a.add({id:51,href:'/grex-docs/docs/grex/software/specific/nwchem/',title:"Nwchem",content:"NWChem Introduction NWChem is a Scalable open-source solution for large scale molecular simulations. NWChem is actively developed by a consortium of developers and maintained by the EMSL located at the Pacific Northwest National Laboratory (PNNL) in Washington State. The code is distributed as open-source under the terms of the Educational Community License version 2.0 (ECL 2.0).\nSystem specific notes On Grex software stack, NWChem is using OpenMPI 3.1 with Intel compilers toolchains. To find out which versions re available, use module spider nwchem .\nFor a version 6.8.1, at the time of writing the following modules have to be loaded:\nmodule load intel/15.0 ompi/3.1.4 nwchem/6.8.1  The NWChem on Grex was built with the ARMCI variant MPI-PR . Thus, NWCHem needs at least One process per node reserved for the data communication. To run a serial job one needs 2 tasks per node. To run a 22 core job over two whole nodes, one has to ask for 2 nodes, 12 tasks per node. Simple number of tasks specification likely wont work because of the chance of having a single-task node allocated by SLURM; so --nodes= --ntask-per-node specification is required!\nSample SLURM Script #!/bin/bash #SBATCH --ntasks-per-node=7 --nodes=2 --cpus-per-task=1 #SBATCH --mem-per-cpu=2000mb #SBATCH --time=0-3:00:00 #SBATCH --job-name=NWchem-dft-test # Adjust the number of tasks, time and memory required. # the above spec is for 12 compute tasks over two nodes. module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1 echo \u0026#34;Starting run at: `date`\u0026#34; which nwchem # Uncomment/Change these in case you want to use custom basis sets export NWCHEM_NWPW_LIBRARY=/global/software/cent7/nwchem/6.8.1-intel15-ompi314/data/libraryps/ export NWCHEM_BASIS_LIBRARY=/global/software/cent7/nwchem/6.8.1-intel15-ompi314/data/libraries/ # In most cases SCRATCH_DIR would be on local nodes scratch # While results are in the same directory export NWCHEM_SCRATCH_DIR=$TMPDIR export NWCHEM_PERMANENT_DIR=`pwd` # Optional memory setting; note that this one or the one in your code # must match the #SBATCH --mem-per-cpu times compute tasks ! export NWCHEM_MEMORY_TOTAL=2000000000 # 24000 MB, double precision words only export MKL_NUM_THREADS=1 srun nwchem dft_feco5.nw \u0026gt; dft_feco5.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Assuming the script above is saved as nwchem.job, it can be sumbitted with:\nsbatch nwchem.job  "}),a.add({id:52,href:'/grex-docs/docs/grex/software/specific/orca/',title:"Orca",content:"ORCA Introduction ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semiempirical methods to DFT to single - and multireference correlated ab initio methods. It can also treat environmental and relativistic effects.\nUser Responsibilities and Access ORCA is a proprietary software, even if it is free it still requires you to agree to the ORCA license conditions. We have installed ORCA on Grex, but to access the binaries each of the ORCA users has to confirm they have accepted the license.\nThe procedure is as follows: first, register at ORCA forum . After the registration is complete, go to ORCA download page, and accept the license conditions. Then contact us (via Compute Canada support for example) quoting the ORCA email and stating that you also would like to access ORCA on Grex.\nSystem specific notes To see the versions installed on Grex and how to load them, please use module spider orca and follow the instructions. Both ORCA-4 and ORCA-5 are available on Grex.\nTo load ORCA-5, use:\nmodule load gcc/4.8 ompi/4.1.1 orca/5.0.2  To load ORCA-4, use:\nmodule load gcc/4.8 ompi/3.1.4 orca/4.2.1  Note:\nThe first realeased version of ORCA-5 (5.0.1) is available on Grex. However, ORCA users should use the versions release after (as for now: 5.0.2 since it addresses few bugs of the two first releases 5.0.0 and 5.0.1).  Using ORCA with SLURM Sample SLURM Script #!/bin/bash #SBATCH --ntasks=8 #SBATCH --mem-per-cpu=2500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=\u0026#34;ORCA-test\u0026#34; # Adjust the number of tasks, memory  # and walltime above as necessary! # Load the OpenMPI and ORCA modules: module load gcc/4.8 ompi/4.1.1 orca/5.0.2 # Assign the input file: ORCA_INPUT_NAME=`ls *.inp | awk -F \u0026#34;.\u0026#34; \u0026#39;{print $1}\u0026#39;` ORCA_RAW_IN=${ORCA_INPUT_NAME}.inp # Specify the output file: ORCA_OUT=${ORCA_INPUT_NAME}.out echo \u0026#34;Current working directory is `pwd`\u0026#34; NUM_PROCS=$SLURM_NTASKS echo \u0026#34;Running on $NUM_PROCSprocessors.\u0026#34; echo \u0026#34;Creating temporary input file ${ORCA_IN}\u0026#34; ORCA_IN=${ORCA_RAW_IN}_${SLURM_JOBID} cp ${ORCA_RAW_IN} ${ORCA_IN} echo \u0026#34;%PAL nprocs $NUM_PROCS\u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; end \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} # The orca command should be called with a full path: echo \u0026#34;Starting run at: `date`\u0026#34; ORCAEXEC=`which orca` ${ORCAEXEC} ${ORCA_IN} \u0026gt; ${ORCA_OUT} echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Assuming the script above is saved as orca.job, it can be sumbitted with:\nsbatch orca.job  Sample SLURM Script for NBO calculations #!/bin/bash #SBATCH --ntasks=32 #SBATCH --mem-per-cpu=4000M #SBATCH --time=7-0:00:00 #SBATCH --job-name=nbo # Load the modules: module load gcc/4.8 ompi/4.1.1 orca/5.0.2 module load nbo/7.0 EBROOTORCA=/global/software/cent7/orca/5.0.2_linux_x86-64_openmpi411 export GENEXE=`which gennbo.i4.exe` export NBOEXE=`which nbo7.i4.exe` # Assign the input file: ORCA_INPUT_NAME=`ls *.inp | awk -F \u0026#34;.\u0026#34; \u0026#39;{print $1}\u0026#39;` ORCA_RAW_IN=${ORCA_INPUT_NAME}.inp # Specify the output file: ORCA_OUT=${ORCA_INPUT_NAME}.out echo \u0026#34;Current working directory is `pwd`\u0026#34; NUM_PROCS=$SLURM_NTASKS echo \u0026#34;Running on $NUM_PROCSprocessors.\u0026#34; echo \u0026#34;Creating temporary input file ${ORCA_IN}\u0026#34; ORCA_IN=${ORCA_RAW_IN}_${SLURM_JOBID} cp ${ORCA_RAW_IN} ${ORCA_IN} echo \u0026#34;%PAL nprocs $NUM_PROCS\u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; end \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} # The orca command should be called with a full path: echo \u0026#34;Starting run at: `date`\u0026#34; ${EBROOTORCA}/orca ${ORCA_IN} \u0026gt; ${ORCA_OUT} echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  "}),a.add({id:53,href:'/grex-docs/docs/grex/software/specific/priroda/',title:"Priroda",content:"Priroda Introduction Priroda is a fast parallel relativistic DFT and ab initio code for molecular modeling, developed by Dr. Dimitri N. Laikov. The code originally implemented fast resolution-of-identity GGA DFT for coulomb and exchange integrals. Later it was extended to provide RI-DFT with hybrid functionals, RI-HF and RI-MP2, and parallel high-level coupled-cluster methods. All these levels of theory can be used together with an efficient all-electron scalar-relativistic method, with small-component bases supplied for all the elements of Periodic Table. The current release of the code also includes a novel NDO-based semiempirical method.\nUser Responsibilities and Access The code is free for academic users, but is not open source. It is distributed on request by the Author, Dr. Dimitri N. Laikov. To access the Priroda code on Grex, the prospective users have to send us (support@tech.alliancecan.ca) a free-form email confirming that they have read and agreed to abide by the following conditions:\nConditions for the Priroda code access on Grex:\n I understand that the Priroda code\u0026rsquo;s ownership and copyright belongs solely to its Author, Dr. Dimitri N. Laikov. I will not incorporate any part of the Priroda code into any other program system, either for sale or for non-profit distribution, without written permission by the Author. I will not copy, distribute or supply the Priroda code for any reason whatsoever to third persons or organizations. Instead, I will direct all the code requests to the Author.   If results obtained with the code are published, I will cite the proper Priroda code references and, when appropriate, the specific methods references, as described in the Priroda code documentation and/or Dr.Laikov\u0026rsquo;s website.   I understand that the Priroda code is provided \u0026ldquo;as is\u0026rdquo; and the author is not assuming any responsibilities or any liabilities that might arise from the usage of the code, whatsoever.   After receiving the email, we will add the user to the wg-prrda UNIX group that is used to control access to the Priroda program, basis sets and docummentaton\nRunning Priroda on Grex The Priroda code is linked against OpenMPI bulit with a GCC compiler. There are several versions of them, and module spider priroda would help to locate the dependencies. As of the time of writing, the following command would load the Priroda version of 2016:\nmodule load gcc/5.2 ompi/3.1.4 priroda/2016  The parallel Priroda executable (called p) will be in the PATH after loading of the module. Its basis sets and/or semiempirical method parameters can be found under $PRIRODA/bin. Documentation and examples are are available under $PRIRODA/doc and $PRIRODA/example, correspondingly.\nThe style of the Priroda input is of free format namelist groups, similar to that of GAMESS-US but more flexible (no limitations inherited from Fortran77). Examples and desctiption of each input group are in the doc and example directories. To invoke the code interactively:\nmpiexec p name.inp name.out  An archive of old Priroda documentation is here Priroda old docs from KNC Using Priroda with SLURM Priroda is MPI-parallelized. The parallel efficiency varies on the method used and the kind of calculation (energies, geometry optimizations or analytical hessians) performed. Pure GGA DFT calculations are quite fast and tightly coupled, and makes sense to use single node with a few tasks per node, or a few nodes, as in example below. RI-MP2 calculations would benefit from more massively parallel calculations, spanning several nodes.\nIt makes no sense to ask more than 4000mb per task.\nSample SLURM Script #!/bin/bash #SBATCH --nodes=1 --ntasks-per-node=6 --mem-per-cpu=2000mb #SBATCH --time=0-2:00:00 #SBATCH --job-name=priroda-test-c60 SCR=$TMPDIR echo \u0026#34;assuming inputs in $SLURM_SUBMIT_DIR\u0026#34; module load gcc/5.2 ompi/3.1.4 priroda/2016 # copy the input file (c60.inp) locally and set the resource requests # and temporary paths. note that the file myfile.inp # should not have a $system .. $end group cp c60.inp $SCR/priroda.inp cd $SCR echo \u0026#39; \u0026#39; \u0026gt;\u0026gt; priroda.inp echo \u0026#39; $system \u0026#39; \u0026gt;\u0026gt; priroda.inp echo \u0026#34; memory=1000 disk=10 path=. \u0026#34; \u0026gt;\u0026gt; priroda.inp echo \u0026#39; $end \u0026#39; \u0026gt;\u0026gt; priroda.inp cat priroda.inp # Copy basis sets locally: cp $PRIRODA/bin/*.in $SCR cp $PRIRODA/bin/*.bas $SCR echo \u0026#34;Start date:`date`\u0026#34; # Actually run the job srun $PRIRODA/bin/p priroda.inp $SLURM_SUBMIT_DIR/c60.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Various scripts and utilities There are some simple scripts and utilities in $PRIRODA/contrib directory. They can be used for conversions of inputs/outputs to and from Molden XYZ format, extraction of the MOs and vibrational frequencies, and restart informations from the Priroda output files.\n"}),a.add({id:54,href:'/grex-docs/docs/grex/software/specific/vasp/',title:"Vasp",content:"VASP Introduction VASP is a massively parallel plane-wave solid state DFT code. On Grex it is available only for the research groups that hold VASP license. To get access, PIs would need to send us a confirmation email from the VASP vendor, detailing status of their license and a list of users allowed to use it.\nSystem specific notes On the Grex local software stack, we have VASP 5 and VASP 6 is using Intel compiler and OpenMPI 3.1. To find out which versions of VASP re available, use module spider vasp .\nFor a version 6.1.2, at the time of writing the following modules have to be loaded:\nmodule load intel/2019.5 ompi/3.1.4\nmodule load vasp/6.1.2\n There are three executables for VASP CPU version: vasp_gam , vasp_ncl , and vasp_std. Refer to the VASP manual as to what these mean. An example VASP SLURM script using the standard version of the VASP binary is below:\n#!/bin/bash #SBATCH --ntasks=16 -cpus-per-task=1 #SBATCH --mem-per-cpu=3400mb #SBATCH --time=0-3:00:00 #SBATCH --job-name=vasp-test # adjust the number of tasks, time and memory required. # the above spec is for 16 compute tasks, using 3400 MB per task . module load intel/2019.5 ompi/3.1.4 module load vasp/6.1.2 echo \u0026#34;Starting run at: `date`\u0026#34; which vasp_std export MKL_NUM_THREADS=1 srun vasp_std \u0026gt; vasp_test.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $?at: `date`\u0026#34;  Assuming the script above is saved as vasp.job, it can be sumbitted with\nsbatch vasp.job  The script assumes that VASP6 inputs (INCAR, POTCAR etc.) are in the same directory as the job script.\n"}),a.add({id:55,href:'/grex-docs/docs/grex/running/contributed-systems/',title:"Contributed systems",content:"Scheduling policies for contributed systems Grex has a few user contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the \u0026ldquo;preferred access\u0026rdquo; is preemption.\nOn the definition of preferential access to HPC systems Preferential access is when you have a non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have. HPC makes access to CPU cores / GPUs / Memory exclusive per job, for the duration of the job (as opposed to time-sharing). Priority is a factor that decides, which job gets to start (and thus exclude other jobs) first if there is a competitive situation (more jobs than free cores).\nThe owner is the owner of the contributed hardware. Others are other users. A partition is a subset of the HPC system’s compute nodes.\nPreemption by partition: the contributed nodes have a SLURM partition on them, allowing the owner to use them, normally, for batch or interactive jobs. The partition is a “preemptor”. There is an overlapping partition, on the same set of the nodes but for the others to use, which is “preemptible”. Jobs in the preemptible partition can be killed after a set “grace period” (1 hour) as the owner\u0026rsquo;s job enter the \u0026ldquo;preemptor\u0026rdquo; partition. If works, pre-empted jobs might be checkpointed rather than killed, but that’s harder to set up. Currently, it is not generally supported. If you have a code that supports checkpoint/restart at the application level, you can get most of the contributed nodes.\nOn Grex, the \u0026ldquo;preemptor\u0026rdquo; partition is named after the name of the owner PI, and the preemptible partitions named similarly but with added -b suffix. Use the --partition= option to submit the jobs with sbatch and salloc commands to select the desired partition.\nTODO: this article is a draft; guidelines for contrib systems are being developed.  "}),a.add({id:56,href:'/grex-docs/',title:"Introduction",content:"Grex Grex is a UManitoba High Performance Computing (HPC) system, first put in production in early 2011 as part of WestGrid consortium. \u0026ldquo;Grex\u0026rdquo; is a Latin name for \u0026ldquo;herd\u0026rdquo; (or maybe \u0026ldquo;flock\u0026rdquo;?). The names of the Grex login nodes (bison , tatanka, aurochs , yak ) also refer to various kinds of bovine animals.\nSince being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators. The old WestGrid documentation, hosted on the WestGrid website became irrelevant after the Grex upgrade, so please visit Grex\u0026rsquo;s New Documentation . Thus, if you are an experienced user in the previous \u0026ldquo;version\u0026rdquo; of Grex, you might benefit from reading this document: Description of Grex changes . If you are a new Grex user, proceed to the quick start guide and documentation right away.\nHardware The orihginal Grex was an SGI Altrix machine, with 312 compute nodes (Xeon 5560, 12 CPU cores and 48 GB of RAM per node) and QDR 40 Gb/s Infiniband network. In 2017, a new Seagate Storage Building Blocks based Lustre filesystem of 418 TB of useful space was added to Grex. In 2020 and 2021, the University added several modern Intel CascadeLake CPU nodes, a few GPU nodes, a new NVME storage for home directories, and EDR Infiniband interconnect. So, the current computing hardware available for general use is as follows:\n 12 [ 40 core Intel CPU ] nodes, 384 GB RAM, EDR 100GB/s IB interconnect 43 [ 52 core Intel 6230R ] nodes, 96 GB RAM, EDR 100GB/s IB interconnect 2 [ 4xV100 NVLINK, 32 core Intel 5218 CPUs ] GPU nodes, 192 GB RAM, FDR 56GB/s IB interconnect Original Grex (slated for decommission in Spring 2022) Xeon 5560, 12 CPU cores, 48 GB of RAM, QDR 40GB/s IB interconnect   There are also several researcher-contributed nodes (CPU and GPU) to Grex which make it a \u0026ldquo;community cluster\u0026rdquo;. The researcher-contributed nodes are available for others on opportunistic basis; the owner groups will preempt the others' workloads.\nGrex\u0026rsquo;s compute nodes have access to two filesystems:\n /home filesystem, NFSv4/RDMA, 15 TB total usable, 100 GB / user quota. /global/scratch filesystem, Lustre, 418 TB total usable, 4 TB / user quota.   There is a 10 GB/s Ethernet connection between Grex and WestGrid\u0026rsquo;s network.\nSoftware Grex is a traditional HPC machine, running CentOS Linux under SLURM resource management system.\nUseful links The Alliance (formerly known as Compute Canada)   Grex Status Page   Grex Documentation   Local Resources at UManitoba    "})})()