[{"id":"0","rootTitleIndex":"0","rootTitle":"Homepage","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/","rootTitleTitle":"Homepage","permalink":"/grex-docs/","permalinkTitle":"Homepage","title":"Homepage","content":"\nHow to use this website? Territory acknowledgement # The University of Manitoba campuses are located on original lands of Anishinaabeg, Cree, Oji-Cree, Dakota and Dene peoples, and on the homeland of the Métis Nation. We respect the Treaties that were made on these territories, we acknowledge the harms and mistakes of the past, and we dedicate ourselves to move forward in partnership with Indigenous communities in a spirit of reconciliation and collaboration.\n"},{"id":"1","rootTitleIndex":"14","rootTitle":"Friendly Organizations","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/friends/","rootTitleTitle":"Homepage / Friendly Organizations","permalink":"/grex-docs/friends/alliancecan/","permalinkTitle":"Homepage / Friendly Organizations / Digital Research Alliance of Canada","title":"Digital Research Alliance of Canada","content":" Introduction # Digital Research Alliance of Canada (the Alliance) is a Canadian National Digital Research Infrastructure (DRI) organization (formerly known as Compute Canada). It provides the eligible researchers with the research computing resources such as several High-performance computing (HPC) systems with a large curated HPC software stack, private OpenStack cloud, and Globus data transfer software.\nThe Alliance also maintains a user authentication service and usage database called CCDB. On Grex, we rely on CCDB for accessing our system. The first step to get started with Grex is to register for an Alliance account at the CCDB website (if you do not already have one).\nThe Digital Research Alliance of Canada on the Web # Digital Research Alliance of Canada official website The Alliance database CCDB The Alliance user documentation Wiki Status page for the Alliance clusters Getting an account with the Alliance # University of Manitoba Faculty members are eligible for getting an account with the Alliance. Once it is obtained, they can manage accounts of their group members in CCDB.\nIf you are a postdoc, a student or an external collaborator, you can apply for an account as part of the Faculty\u0026rsquo;s group. You will need your PI\u0026rsquo;s CCDB role identifier code (CCRI). For more information, please follow this guide.\nThe account will allow you to manage your roles and groups in CCDB, to access National HPC and Cloud machines, as well as few other services such as Globus and NextCloud.\nSince we have switched Grex to CCDB credentials in late 2019, the CCDB account is both necessary and sufficient to access Grex.\nWestGrid accounts are no longer needed, nor working on Grex. Please use CCDB accounts for both Grex and DRAC systems! WestGrid network addresses in the westgrid.ca domain are no longer nor used on Grex. Please use hpc.umanitoba.ca domain! The Alliance\u0026rsquo;s clusters and services # Beluga Cedar Graham Narval Niagara Cloud NextCloud Globus Regional Partners # The regional partners of the Alliance are: BC DRI group and Prairies DRI group, which consists of institutional members belonging to Western Canada (former WestGrid institutions), Compute Ontario, Calcul Quebec and AceNET which is a consortium of universities of Maritime provinces.\nBC DRI group Prairies DRI group Compute Ontario Calcul Quebec AceNET WestGrid ceased its operations since April 1st, 2022. The former WestGrid institutions are now re-organized into two consortia: BC DRI group and Prairies DRI group. University of Manitoba is a member institution of Prairies DRI group, and provides a site for the Alliance support staff that support usage of the National DRI by Manitoba researchers and their collaborators.\nGetting support # The single point of contact for the Alliance (Compute Canada) support is: support@tech.alliancecan.ca\nWhen requesting support, please make sure to include enough information that will help support staff to identify your issue in order to solve it in time. An example of details to include in your email are (but not limited to): user name, name of the cluster, job id if applicable, job script, list of the modules, a path to your files and scripts, \u0026hellip; etc. You may include any other details that may help to understand your issue.\n"},{"id":"2","rootTitleIndex":"10","rootTitle":"Grex changes / software and hardware updates","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/changes/","rootTitleTitle":"Homepage / Grex changes / software and hardware updates","permalink":"/grex-docs/changes/linux-slurm-update/","permalinkTitle":"Homepage / Grex changes / software and hardware updates / Linux/SLURM update project","title":"Linux/SLURM update project","content":" Grex changes: Linux/SLURM update project. # December 10-11, 2019\nIntroduction / Motivation # Grex runs an old version of CentOS 6, which gets unsupported in 2020. The 2.6.x Linux kernel that is shipped with CentOS 6 does not support containerized workloads that require recent kernel features. The Lustre parallel filesystem client had some troubles that we were unable to resolve with the CentOS 6 kernel version as well. Finally, the original Grex resource management software, Torque 2.5 and Moab7 are unable to properly schedule jobs that use newer MPI implementations (OpenMPI 2 and 3), which are increasingly common amongst HPC users. Therefore, using the power outages of October and December 2019, we have embarked on a rather ambitious project of updating the entire Grex OS and software stack and scheduling to CentOS 7 and SLURM. This document outlines the changes and how they will affect Grex users.\nConnecting to Grex # Grex is still using the Westgrid accounting system (Portal/LDAP). To connect to Grex, one needs an active Compute Canada account and a Westgrid consortium account linked to it. You are likely to have one.\nHosts to connect to # During the outage, most of the Grex compute nodes, and all the contributed systems have been reprovisioned to CentOS 7. Public access login nodes, bison.westgrid.ca and tatanka.westgrid.ca (and their DNS alias, grex.westgrid.ca ) give the new CentOS 7.6 / SLURM / LMOD environment.\nA test node, aurochs.westgrid.ca was preserved in the original CentOS 6 state, as well as about 600 cores of the Grex compute nodes. Logging in to aurochs for now allows users access to the original Grex environment (Torque, Moab, Tcl-Modules). We plan to eventually decommission the CentOS 6 partition when the CentOS 7 is debugged and fully in production.\nCommand line access via SSH clients # Because Grex login nodes were reinstalled, SSH clients might give either a warning or an error for not recognizing the Grex host keys. Remove the offending keys from the file ~/.ssh/known_hosts mentioned in the error message.\nGraphical access with X2Go # The new CentOS 7 supports X2Go connections to use GUI applications. However, the GNOME Desktop environment that was default in CentOS 6 is no longer available! Please use either ICEWM or OPENBOX Desktop environment in the X2Go client to connect.\nBecause X2Go is using SSH under the hood to establish the connection, the advice above about ~/.ssh/known_hosts holds: delete the old SSH keys from it if you have connection problems. On Windows, it is often located under C:\\Users\\username\\.ssh\\known_hosts.\nStorage # Lustre storage (/global/scratch) was updated to Lustre 2.10.8 on both server and client sides. We have ran our extensive tests and observed an increase of the write throughputs for large parallel I/O up to 3x. The change should be transparent to Grex users.\nSoftware, Interconnect, LMOD, CC/local stacks # Interconnect and communications libraries # Grex\u0026rsquo;s HPC interconnect hardware is no longer officially supported by commercial MLNX IB Verbs drivers. At the same time, open source projects like RDMA-Core and the new universal interconnect, UCX, almost reached maturity and superior performance. Therefore, for the Grex software update, we have opted for vanilla Kernel drivers for our Infiniband, RDMA-Core for verbs userland libraries and UCX for the communication layer for the new OpenMPI versions, of which we support OpeMPI 3.1.4 (the new default) and 4.0.2 (An experimental, bleeding edge MPI v3 standard implementation that obsoletes many old MPI features). The previous default version OpenMPI 1.6.5 is still supported, and still uses IB Verbs from RDMA-core.\nUsers that have codes directly linked against any IBVerbs or other low level MLNX libraries, or having fixed RPATH to the old OpenMPI binaries will have to recompile their codes!\nSoftware Modules: LMOD # This is a major change. We have made obsolete the tried and tested flat, TCL-based software Modules system in favor of Lmod. Lmod is a new software Module system developed by Robert McLay at TACC. The main difference between Lmod and TCL-mod is that Lmod is built to have a hierarchical module structure: it ensures that no modules of the same “kind” can be loaded simultaneously; that there be no deep module paths like “intel/ompi/1.6.5” or “netcdf/pnetcdf-ompi165-nc433” . Rather, users will load “root” modules first and dependent modules second. That is, instead of TCL-mode’s way on the old system (loading OpenMPI for Intel-14 compilers:\nmodule load intel/14.0.2.144 module load intel/ompi/1.6.5 The new Lmod way would be:\nmodule load intel/14.0.2.144 module load ompi/1.6.5 The hierarchy ensures that only a good ompi/1.6.5 module that corresponds to the previously loaded Intel-14 compilers gets loaded. Note that swapping the compiler modules (Intel to GCC or Intel 14 to Intel 15) results in automatic reload of the dependent modules, if possible. Loading two versions of the same modules simultaneously is no longer possible! This largely removes the need for “module purge” command.\nIn the hierarchical module system, dependent modules are not visible for “module avail” unless their dependencies are loaded. To figure what modules are available use “module spider” instead.\nFor more information and features, visit the official documentation of Lmod and/or Compute Canada documentation for modules.\nWe have tried to preserve the module names and paths closest to the original TCL-modules on Grex, whenever that was possible with the new hierarchy format. Note that the hierarchy is not “complete”: not every combination of software, compilers, and MPI exists on Grex, for practical reasons. Use \u0026ldquo;module spider name-of-the-software/version\u0026rdquo; to check what built variants are there. Send us a request to support@tech.alliancecan.ca if there is any missing software/toolchain combination you may want to use.\nSoftware Modules: Stacks , CVMFS, Defaults # One of the nice features of Lmod is its ability to maintain several software stacks at once. We have used it and now provide the following software stacks modules. The modules are “sticky” which means, one of them is always loaded. (Use module avail to see them). GrexEnv (default), OldEnv and CCEnv .\nGrexEnv is the current Grex default software environment, (mostly) recompiled to CentOS-7. Note that we do not load Intel compilers and OpenMPI modules by default anymore, in contrast to the old environment of Grex. The recompilation is mostly done, but not completely: should you miss a software item, please contact us!\nOldEnv is an Lmod-ized version of the old CentOS-6 modules, should you need it for compatibility reasons. The software (except OpenMPI) is as it was before (not recompiled). It may run on CentOS-7.\nCCEnv is the full Compute Canada software stack, brought to Grex via Cern Virtual Filesystem (CVMFS). We use the “sse3” software stack of Compute Canada because this is the one that suits our older hardware. Note that the default CC environments (StdEnv, nixpkgs) are NOT loaded by default on Grex! In order to access the CC software, they have to be loaded after the CCEnv as follows. (Note that the first load of the CC modules and software items might take a few seconds! It is probably a good practice to first access a CC software binary in a small interactive job to warm the local cache).\nmodule load CCEnv module load StdEnv/2016.4 nixpkgs/16.09 arch/sse3 module avail The CC software stack is documented at Compute Canada wiki: Available_software page. A caveat: it is in general impossible to isolate and containerize high-performance software completely, so not all CC CVMFS software might work on Grex: the most troublesome parts are MPI and CUDA software that rely on low level hardware drivers and direct memory access. Threaded SMP software and serial software we expect to run without issues.\nNote that for Contributed systems it might be beneficial to use a different architecture than the default SSE3, which is available at loading corresponding arch/ module.\nRunning jobs, migration to SLURM # CentOS-7 provides a different method of process isolation (cgroups). It also allows for better support of containers such as Singularity which (hopefully) can now be run in user namespaces. Incidentally, the cgroups are not supported well by any Torque/Moab scheduler version that was compatible with our current Moab software license. Torque is not known to support the new process management interface (PMIX) that is increasingly becoming a standard for MPI libraries either. Therefore, we had little choice but to migrate our batch workload management from Torque to SLURM. It will also make Grex more similar to Compute Canada machines (which are documented here: Running jobs).\nUnlike Torque which is a monolithic, well engineered piece of software that we knew well, SLURM is a very modular, plugin-based ecosystem which is new to us. Therefore, initially we will enable only short walltimes to test our SLURM configuration (48h) before going to full production.\nTorque wrappers # To make life easier for PBS/Torque users, SLURM developers provided most of the Torque commands as wrappers over SLURM commands. So with some luck, you can continue using qsub, qstat, pbsnodes etc. keeping in mind correspondence between Torque’s queues and SLURM partitions. For example:\nsbatch --account=abc-668-aa --partition=compute my.job\nCan be called using Torque syntax as:\nqsub -A abc-668-aa -q compute my.job\nPresently, and unlike old Grex, there is no automatic queue/partition/QOS assignment based on jobs resource request. The option --partition must be selected explicitly for using High Memory and contributed nodes, if you have access to the latter. By default, jobs go into “compute” partition comprised of the original 48 GB, 12 Nehalem CPUs nodes. The command qstat -q now actually lists partitions (which it thinks are queues).\nInteractive Jobs # As before, and as usual for HPC systems, we ask users to limit their work on login nodes to code development and small, infrequent test runs and to use interactive jobs for longer and/or heavier interactive workloads.\nInteractive jobs can be done using SLURM salloc command as well as using qsub -I . The one limitation for the latter is there for graphical jobs: the latest and greatest SLURM 19.05 supports --x11 flags natively for salloc, but does not support yet corresponding qsub -I -X flags for the Torque wrapper. So graphical interactive jobs are only possible with salloc.\nAnother limitation of qsub is the syntax: SLURM does distinguish between --ntasks= for MPI parallel jobs and --cpus-per-task= for SMP threaded jobs; while for qsub it is all the same for its -l nodes=1:ppn= syntax. Therefore, the SMP threaded applications might not be placed correctly with the jobs submitted with qsub.\nBatch Jobs. # There are two changes: need to specify partitions explicitly and short maximal walltimes during initial Grex testing. Resource allocations for Grex RAC 2019-2020 will be implemented in the SLURM scheduler. Two useful commands: sshare (shows your groups fairshare parameters) and seff (shows efficiency of your jobs: CPU and memory usage) might help with effective usage of your allocation.\nIn general, Compute Canada documentation on running SLURM jobs can be followed, obviously with the exception of different core count on Grex nodes. Presently, we schedule on Grex by CPU core (as opposed to by-node) so whole-node jobs do not get any particular prioritization.\nFor the local scratch directories, $TMPDIR should be used in batch scripts on Grex (as opposed to Compute Canada where $SLURM_TMPDIR is defined). Thus, for using software from Compute Canada stack that has references to SLURM_TMPDIR hardcoded in some scripts it relies on (an example being GAMESS-US) the following line should be added on Grex to your job scripts:\nexport SLURM_TMPDIR=$TMPDIR\n"},{"id":"3","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/gaussian/","permalinkTitle":"Homepage / Software Specific Notes / Running Gaussian on Grex","title":"Running Gaussian on Grex","content":" Introduction # Gaussian 16 is a comprehensive suite for electronic structure modeling using ab initio, DFT and semi-empirical methods. A list of Gaussian 16 features can be found here.\nUser Responsibilities and Access # University of Manitoba has a site license for Gaussian 16 and GaussView. However, it comes with certain license limitations, so access to the code is subject to some license conditions.\nSince, as of now, Compute Canada accounts are a superset of Grex accounts, users will want to initiate getting access by sending an email agreeing to Gaussian conditions to support@tech.alliancecan.ca, confirming that you have read and agree to abide by the following conditions, and mentioning that you\u0026rsquo;d also want to access it on Grex:\n1. I am not a member of a research group developing software competitive to Gaussian.\n2. I will not copy the Gaussian software, nor make it available to anyone else.\n3. I will properly acknowledge Gaussian Inc. and Compute Canada in publications.\n4. I will notify Compute Canada of any change in the above acknowledgement.\nIf you are a sponsored user, your sponsor (PI) must also have such a statement on file with us.\nMoreover, the terms of the UManitoba license are actually stricter than for the Alliance (Compute Canada). In particular, it excludes certain research groups at the University to have access to the software. Therefore, we are required by Gaussian to have each of the Gaussian users to sign a Confidentiality Agreement form as provided to us by Gaussian. Inc. Please drop by our office in Engineering, E2-588 to get the form and return it signed.\nSystem specific notes # On Grex, Gaussian is limited to a single node, SMP jobs and the memory of a single node. There is no Linda. The Gaussian code is accessible as a module. The module sets Gaussian\u0026rsquo;s environment variables like GAUSS_SCRDIR (the latter, to local node scratch).\nmodule load gaussian/g16.c01 To load the module and access the binaries, you will first get access as per above. Also, our Gaussian license span is less than Compute Canada\u0026rsquo;s support contract, so there are fewer versions available. Use module spider gaussian to see what is available on Grex.\nAfter a Gaussian module is loaded, the GaussView software also becomes available (provided you have connected with X11 support, perhaps using X2Go) as follows:\ngv The viewer should not be used to run production calculations on Grex login nodes. Instead, as for any other production calculations, SLURM jobs should be used as described below.\nUsing Gaussian with SLURM # Sample SLURM Script # Script example for running Gaussian on Grex run-gaussian.sh\r#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 #SBATCH --mem=40gb #SBATCH --time=8:00:00 #SBATCH --job-name=Gauss16-test module load gaussian/g16.c01 echo \u0026#34;Starting run at: `date`\u0026#34; which g16 # note that input should have %nproc=12 # and %mem=40gb for the above resurce request. g16 \u0026lt; input.gjf \u0026gt; output.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Simplified job submission # A simplified job script sbg16 is available (after loading of the g16 module) for automatic generation and submission of SLURM Gaussian jobs.\nsbg16 input.gjf -ppn 12 -mem 40000mb -time 8:00:00 Using NBO # University of Manitoba has site licenses for NBO6 and NBO7. Corresponding NBO modules would have to be loaded in order to use Gaussian\u0026rsquo;s POP=NBO6 or NBO7 keywords.\nTo list available NBO versions and their dependencies, run the command:\nmodule spider nbo Related links # Gaussian documentation. Gaussian page on Compute Canada wiki. Gaussian error messages. "},{"id":"4","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/julia/","permalinkTitle":"Homepage / Software Specific Notes / Running Julia on Grex","title":"Running Julia on Grex","content":" Introduction # Julia is a programming language that was designed for performance, ease of use and portability. It is available as a module on Grex.\nAvailable Julia versions # Presently, binary Julia versions 1.3.0, 1.5.4 and 1.6.1 are available. Use module spider julia to find out other versions.\nInstalling packages # We do not maintain centralized versions of Julia packages. Users should install Julia modules in their home directory.\nThe command is (in Julia REPL):\nUsing Pkg; Pkg.Add(\u0026#34;My-Package\u0026#34;) In case of package/version conflicts, remove the packages directory ~/.julia/.\nUsing Julia notebooks # It is possible to use IJulia kernels for Jupyter notebooks. A preferable way of running a Jupyter notebook is SLURM interactive job with salloc command.\n(More details coming soon).\nRunning Julia jobs # Julia comes with a large variety of packages. Some of them would use threads; and therefore, have to be run as SMP jobs with \u0026ndash;cpus-per-task specified. Moreover, you would want to set the JULIA_NUM_THREADS environment variable in your job script to be the same as SLURM\u0026rsquo;s number of threads.\nScript example for running Julia on Grex run-julia.sh\r#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=8:00:00 #SBATCH --job-name=Julia-Test # Load the modules: module load julia/1.7.0-bin echo \u0026#34;Starting run at: `date`\u0026#34; julia test-julia.jl echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-julia.sh, it can be submitted with:\nsbatch run-julia.sh For more information, visit the page running jobs on Grex\nUsing Julia for GPU programming # It is possible to use Julia with CUDA Array objects to greatly speed up the Julia computations. For more information, please refer to this link: julia-gpu-programming. However, a suitable \u0026ldquo;CUDA\u0026rdquo; module should be loaded during the installation of the CUDA Julia packages. And you likely want to be on a GPU node when the Julia GPU code is executed.\nRelated links # Julia on the Alliance\u0026rsquo;s clusters Running jobs on Grex "},{"id":"5","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/lammps/","permalinkTitle":"Homepage / Software Specific Notes / Running LAMMPS on Grex","title":"Running LAMMPS on Grex","content":" Introduction # LAMMPS is a classical molecular dynamics code. The name stands for Large-scale Atomic / Molecular Massively Parallel Simulator.\nModules # Multiple versions of LAMMPS were installed on Grex. To see all the available versions, use module spider lammps and follow the instructions.\nAvailable CPU versions: # Version Module Name Supported Packages 29 Sep 21 lammps/29Sep21 * 05 Jun 19 lammps/5Jun19 * 11 Aug 17 lammps/11Aug17 * 05 Nov 16 lammps/5Nov16 * 30 Jul 16 lammps/30jul16 * Available GPU versions: # As for the time when writing this page, there is only one version of LAMMPS with GPU support. It can be loaded using:\nmodule load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 The name of the binary is called lmp_gpu (see the example of script below).\nVersion Module name 24 Mar 22 lammps-gpu/24Mar22 Scripts examples # Serial version # Script example for LAMMPS: Serial version run-lammps-serial.sh\r#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test # Load the modules: module load intel/2019.5 ompi/3.1.4 lammps/29Sep21 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp_grex lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; ${lmp_exec} \u0026lt; ${lmp_input} \u0026gt; ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; MPI version # Script example for LAMMPS: MPI version run-lammps-mpi.sh\r#!/bin/bash #SBATCH --ntasks=16 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test # Load the modules: module load intel/2019.5 ompi/3.1.4 lammps/29Sep21 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp_grex lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; srun ${lmp_exec} \u0026lt; ${lmp_input} \u0026gt; ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; OpenMP version # Hybrid version: MPI and OpenMP # GPU version # Script example for LAMMPS: GPU version run-lammps-mpi-gpu.sh\r#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=gpu #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 #SBATCH --job-name=GPU-Test # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Performance # Related links # LAMMPS website. LAMMPS GitHub LAMMPS online documentation. "},{"id":"6","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/general-linux/","permalinkTitle":"Homepage / Software and Applications / General Linux tools","title":"General Linux tools","content":" Linux tools on Grex # There are a number of general and distro-specific tools on Grex that are worth mentioning here. Such tools are: text editors, image viewers, file managers, \u0026hellip; etc.\nCommand line Text editors # Command line text editors allow you to edit files right on Grex in any terminal session (such as SSH session or an X terminal under X2Go):\nThe (arguably) most popular editor is vi, or vim. It is very powerful, but requires some experience to use it. To exit a vim session, you can use the ZZ key combination (hold shift key + zz), or ESC, :x!. There are many vi tutorials around, for example this one. Another lightweight text-mode editor is nano. It provides a self-explanatory key-combination menu at the bottom of the screen. An online manual can be found here. The Midnight Commander file manager provides a text-mode editor that can be invoked stand-alone as mc -e filename. GUI Text editors # Sometimes it is useful (for example, for copy/paste operations with mouse, between client computer and a remote session) or convenient to have a text editor with a graphical user interface. Note that a most practical way to use this is from X2Go sessions that provide tolerable interaction speeds.\nVi has a GUI counterpart which is accessible as evim command. There are also the following GUI editors: nedit and xfe-xfw.\nImage viewers # There are the following commands that can be used for viewing images: xfe-xfi and nemacs. A simple PDF viewer for X11, xpdf and ghostscript are also available.\n"},{"id":"7","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/matlab/","permalinkTitle":"Homepage / Software Specific Notes / Running MATLAB on Grex","title":"Running MATLAB on Grex","content":" Introduction # MATLAB is a general-purpose high-level programming package for numerical work such as linear algebra, signal processing and other calculations involving matrices or vectors of data. We have a campus license for MATLAB which is used on Grex and other local computing resources. MATLAB is available only for UManitoba users.\nAs with most of the Grex software, MATLAB is available as a module. The following command will load the latest version available on Grex:\nmodule load uofm/matlab Then the matlab executable will be in the PATH.\nAvailable Toolboxes # To see a list of the MATLAB toolboxes available with MATLAB license on Grex, you can use the following command:\nmodule load uofm/matlab matlab -nodisplay -nojvm -batch \u0026#34;ver\u0026#34; Running Matlab # It is possible to run MATLAB GUI interactively, for best performance in an X2Go session, OOD session and a terminal. There is no Applications menu shortcut for MATLAB, because it is only in the PATH after the module is loaded from the command line. After loading the module, the command matlab will be in the PATH.\nFor running a MATLAB script in text mode, or a batch script, the following options can be used:\nmatlab -nodisplay -nojvm -nodesktop -nosplash -r your_matlab_script.m However, each instance, GUI or command line, will consume a license unit. By submitting sufficiently many MATLAB jobs concurrently, there is a possibility to exhaust the entire University\u0026rsquo;s license pool. Thus, in most cases, it might make sense to use compiled, standalone MATLAB code runners (MCRs) instead (please refer to the next section).\nStandalone Matlab runners: MCR # MATLAB compiler, the mcc command can be used to compile a source code (.m file) into a standalone executable. There are a couple of important considerations to keep in mind when creating an executable that can be run in the batch-oriented, HPC environment. One is that there is no graphical display attached to your session and the other is that the number of threads used by the standalone application has to be controlled.\nFor example, with code mycode.m a source directory src, with the compiled files being written to a directory called deploy, the following mcc command line (at the Linux shell prompt) could be used:\nmodule load uofm/matlab mkdir deploy cd src mcc -R -nodisplay -R -singleCompThread -m -v -w enable -d ../deploy mycode.m Note the option -singleCompThread has been included in order to limit the executable to just one computational thread.\nIn the deploy directory, an executable mycode will be created along with a script run_mycode.sh. These two files should be copied to the target machine where the code is going to be run as a batch job.\nExample of SLURM script: MCR # After the standalone executable mycode and corresponding script run_mycode.sh have been transferred to a directory on the target system on which they will be run, a batch job script needs to be created in the same directory. Here is an example batch job script.\nScript example for running MATLAB via MCR run-matlab-mcr.sh\r#!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Matlab-mcr-job #SBATCH --partition=compute # Choose the MCR directory according to the compiler version used. # The one below for uofm/matlab/R2017A MCR=/global/software/matlab/mcr/v93 # If running on Grex, uncomment the following line to set MCR_CACHE_ROOT: module load mcr/mcr echo \u0026#34;Running on host: `hostname`\u0026#34; echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; ./run_mycode.sh $MCR \u0026gt; mycode_${SLURM_JOBID}.out echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; The job is then submitted as any ordinary SLURM job with the sbatch command. See the Running Jobs page for more information. If the above script is called run-matlab-mcr.sh, it could be submitted using:\nsbatch run-matlab-mcr.sh The specified --time and total memory (--mem-per-cpu) limits should be adjusted to appropriate values for your particular run. The option --partition is used to specify the partition to use for running the job. For more information, visit the page running jobs on Grex\nAn important part of the above script is the location of the MATLAB Compiler Runtime (MCR) directory. This directory contains files necessary for the standalone application to run. The version of the MCR files specified must match the version of MATLAB used to compile the code (check the link for matching module and MCR versions).\nMATLAB on the Alliance\u0026rsquo;s clusters # For using MATLAB on the Alliance\u0026rsquo;s clusters, please visit the corresponding MATLAB page. While there is a wide MATLAB license accessible for all users on cedar, beluga and narval, using MATLAB on graham requires access to an external license. UManitoba users could use MATLAB on graham without additional settings.\nRelated links # MATLAB on the Alliance\u0026rsquo;s clusters MATLAB documentation Running jobs on Grex "},{"id":"8","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/nwchem/","permalinkTitle":"Homepage / Software Specific Notes / Running NWChem on Grex","title":"Running NWChem on Grex","content":" Introduction # NWChem is a Scalable open-source solution for large scale molecular simulations. NWChem is actively developed by a consortium of developers and maintained by the EMSL located at the Pacific Northwest National Laboratory (PNNL) in Washington State. The code is distributed as open-source under the terms of the Educational Community License version 2.0 (ECL 2.0).\nSystem specific notes # On the Grex software stack, NWChem is using OpenMPI 3.1 with Intel compilers toolchains. To find out which versions aare available, use module spider nwchem.\nFor a version 6.8.1, at the time of writing the following modules have to be loaded:\nmodule load intel/15.0 ompi/3.1.4 nwchem/6.8.1 The NWChem on Grex was built with the ARMCI variant MPI-PR. Thus, NWCHem needs at least One process per node reserved for data communication. To run a serial job one needs 2 tasks per node. To run a 22-core job over two whole nodes, one has to ask for 2 nodes, 12 tasks per node. Simple number of tasks specification likely won\u0026rsquo;t work because of the chance of having a single-task node allocated by SLURM; so --nodes= --ntask-per-node specification is required.\nScript example for running NWChem on Grex run-nwchem.sh\r#!/bin/bash #SBATCH --ntasks-per-node=7 --nodes=2 --cpus-per-task=1 #SBATCH --mem-per-cpu=2000mb #SBATCH --time=0-3:00:00 #SBATCH --job-name=NWchem-dft-test # Adjust the number of tasks, time and memory required. # the above spec is for 12 compute tasks over two nodes. # Load the modules: module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1 echo \u0026#34;Starting run at: `date`\u0026#34; which nwchem # Uncomment/Change these in case you want to use custom basis sets NWCHEMROOT=/global/software/cent7/nwchem/6.8.1-intel15-ompi314 export NWCHEM_NWPW_LIBRARY=${NWCHEMROOT}/data/libraryps export NWCHEM_BASIS_LIBRARY=${NWCHEMROOT}/data/libraries # In most cases SCRATCH_DIR would be on local nodes scratch # While results are in the same directory export NWCHEM_SCRATCH_DIR=$TMPDIR export NWCHEM_PERMANENT_DIR=`pwd` # Optional memory setting; note that this one or the one in your code # must match the #SBATCH --mem-per-cpu times compute tasks ! export NWCHEM_MEMORY_TOTAL=2000000000 # 24000 MB, double precision words only export MKL_NUM_THREADS=1 srun nwchem dft_feco5.nw \u0026gt; dft_feco5.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-nwchem.sh, it can be submitted with:\nsbatch run-nwchem.sh For more information, visit the page running jobs on Grex\nRelated links # NWChem Running jobs on Grex "},{"id":"9","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/orca/","permalinkTitle":"Homepage / Software Specific Notes / Running ORCA on Grex","title":"Running ORCA on Grex","content":" Introduction # ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semi-empirical methods to DFT to single - and multi-reference correlated ab initio methods. It can also treat environmental and relativistic effects.\nUser Responsibilities and Access # ORCA is a proprietary software, even if it is free it still requires you to agree to the ORCA license conditions. We have installed ORCA on Grex, but to access the binaries, each of the ORCA users has to confirm they have accepted the license terms.\nThe procedure is as follow:\nFirst, register at ORCA forum. You will receive a first email to verify the email address and activate the account. Follow the instructions in that email. After the registration is complete, go to the ORCA download page, and accept the license conditions. You will get a second email stating that the \u0026ldquo;registration for ORCA download and usage has been completed\u0026rdquo;. Then contact us (via the Alliance support for example) quoting the ORCA email and stating that you also would like to access ORCA on Grex. The same procedure is applied to get access to ORCA on the Alliance\u0026rsquo;s clusters.\nSystem specific notes # To see the versions installed on Grex and how to load them, please use module spider orca and follow the instructions. Both ORCA-4 and ORCA-5 are available on Grex.\nTo load ORCA-5, use:\nmodule load gcc/4.8 ompi/4.1.1 orca/5.0.2 To load ORCA-4, use:\nmodule load gcc/4.8 ompi/3.1.4 orca/4.2.1 Note:\nThe first released version of ORCA-5 (5.0.1) is available on Grex. However, ORCA users should use the versions released after (as for now: 5.0.2 since it addresses some bugs of the two first releases 5.0.0 and 5.0.1). We may remove these versions any time. Using ORCA with SLURM # In addition to the different keywords required to run a given simulation, users should make sure to set two additional parameters, like Number of CPUs and maxcore in their input files:\nmaxcore: This option sets the \u0026ldquo;max\u0026rdquo; memory per core. This is the upper limit under ideal conditions where ORCA can (and apparently often does) overshoot this limit. It is recommended to use no more than 75 % of the physical memory available. So, if the base memory is 4 GB per core, one could use 3 GB. The synatxe is as follow: %maxcore 3000 Basically, one can use 75 % of the total memory requested by SLURM divided by number of CPUs asked for.\nNumber of CPUs: ORCA can run in multiple processors with the aid of OpenMPI. All the modules are installed with the recommended OpenMPI version. To run ORCA in parallel, you can simply set the PAL keyword. For instance, a calculation using four processors requires:\n!HF DEF2-SVP PAL4 or 8:\n!HF DEF2-SVP PAL8 For more than eight processors (!PAL8), the explicit %PAL option has to be used:\n!HF DEF2-SVP %PAL NPROCS 16 END When running ORCA calculations in parallel, always use the full path to ORCA:\nOn Grex, you can use: ORCAEXEC=`which orca` ${ORCAEXEC} your-orca-input.in \u0026gt; your-orca-output.txt On the Alliance clusters, the path is defined via an environment variable EBROOTORCA that is set by the module: ${EBROOTORCA}/orca your-orca-input.in \u0026gt; your-orca-output.txt Example on input file # Example of ORCA input file orca-example.inp\r# Benzene RHF Opt Calculation %pal nprocs 8 end ! RHF TightSCF PModel ! opt * xyz 0 1 C 0.000000000000 1.398696930758 0.000000000000 C 0.000000000000 -1.398696930758 0.000000000000 C 1.211265339156 0.699329968382 0.000000000000 C 1.211265339156 -0.699329968382 0.000000000000 C -1.211265339156 0.699329968382 0.000000000000 C -1.211265339156 -0.699329968382 0.000000000000 H 0.000000000000 2.491406946734 0.000000000000 H 0.000000000000 -2.491406946734 0.000000000000 H 2.157597486829 1.245660462400 0.000000000000 H 2.157597486829 -1.245660462400 0.000000000000 H -2.157597486829 1.245660462400 0.000000000000 H -2.157597486829 -1.245660462400 0.000000000000 * Sample Script for running ORCA on Grex # Script example for running ORCA on Grex run-orca-grex.sh\r#!/bin/bash #SBATCH --ntasks=8 #SBATCH --mem-per-cpu=2500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=\u0026#34;ORCA-test\u0026#34; # Adjust the number of tasks, memory walltime above as necessary # Load the modules: module load gcc/4.8 ompi/4.1.1 orca/5.0.2 # Assign the input file: ORCA_INPUT_NAME=`ls *.inp | awk -F \u0026#34;.\u0026#34; \u0026#39;{print $1}\u0026#39;` ORCA_RAW_IN=${ORCA_INPUT_NAME}.inp # Specify the output file: ORCA_OUT=${ORCA_INPUT_NAME}.out echo \u0026#34;Current working directory is `pwd`\u0026#34; NUM_PROCS=$SLURM_NTASKS echo \u0026#34;Running on $NUM_PROCS processors.\u0026#34; echo \u0026#34;Creating temporary input file ${ORCA_IN}\u0026#34; ORCA_IN=${ORCA_RAW_IN}_${SLURM_JOBID} cp ${ORCA_RAW_IN} ${ORCA_IN} echo \u0026#34;%PAL nprocs $NUM_PROCS\u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; end \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} # The orca command should be called with a full path: echo \u0026#34;Starting run at: `date`\u0026#34; ORCAEXEC=`which orca` ${ORCAEXEC} ${ORCA_IN} \u0026gt; ${ORCA_OUT} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-orca-grex.sh, it can be submitted with:\nsbatch run-orca-grex.sh Sample Script for running NBO with ORCA on Grex # The input file should include the keyword NBO.\nScript example for running NBO with ORCA on Grex run-nbo-orca-grex.sh\r#!/bin/bash #SBATCH --ntasks=32 #SBATCH --mem-per-cpu=4000M #SBATCH --time=7-0:00:00 #SBATCH --job-name=nbo # Load the modules: module load gcc/4.8 ompi/4.1.1 orca/5.0.2 module load nbo/7.0 EBROOTORCA=/global/software/cent7/orca/5.0.2_linux_x86-64_openmpi411 export GENEXE=`which gennbo.i4.exe` export NBOEXE=`which nbo7.i4.exe` # Assign the input file: ORCA_INPUT_NAME=`ls *.inp | awk -F \u0026#34;.\u0026#34; \u0026#39;{print $1}\u0026#39;` ORCA_RAW_IN=${ORCA_INPUT_NAME}.inp # Specify the output file: ORCA_OUT=${ORCA_INPUT_NAME}.out echo \u0026#34;Current working directory is `pwd`\u0026#34; NUM_PROCS=$SLURM_NTASKS echo \u0026#34;Running on $NUM_PROCS processors.\u0026#34; echo \u0026#34;Creating temporary input file ${ORCA_IN}\u0026#34; ORCA_IN=${ORCA_RAW_IN}_${SLURM_JOBID} cp ${ORCA_RAW_IN} ${ORCA_IN} echo \u0026#34;%PAL nprocs $NUM_PROCS\u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; end \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} # The orca command should be called with a full path: echo \u0026#34;Starting run at: `date`\u0026#34; ${EBROOTORCA}/orca ${ORCA_IN} \u0026gt; ${ORCA_OUT} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-orca-grex.sh, it can be submitted with:\nsbatch run-nbo-orca-grex.sh For more information, visit the page running jobs on Grex\nRunning ORCA using ${SLURM_TMPDIR} # Related links # ORCA forum ORCA on the Alliance\u0026rsquo;s clusters ORCA input libraries ORCA common problems SCF convergence-issues ORCA tutorial Running jobs on Grex "},{"id":"10","rootTitleIndex":"6","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/slurm-partitions/","permalinkTitle":"Homepage / Running jobs on Grex / Slurm partitions","title":"Slurm partitions","content":" Partitions # The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a \u0026ldquo;partition\u0026rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who \u0026ldquo;owns\u0026rdquo; the hardware as well.\nThere is no fully automatic selection of partitions, other than the default skylake for most of the users, and compute for the short jobs. For the contributors\u0026rsquo; group members, the default partition will be their contributed nodes. Thus, in many cases users have to specify the partition manually when submitting their jobs!\nCurrently, the following partitions are available on Grex:\nGeneral purpose CPU partitions # Partition Nodes CPUs/Node CPUs Mem/Node Notes skylake 42 52 2184 96 Gb CascadeLakeRefresh largemem 12 40 480 384 Gb CascadeLake compute 316 12 3792 48 Gb SSE4.2 compute 4 20 80 32 Gb Avx - 374 - 6536 - - General purpose GPU partitions # Partition Nodes GPU type CPUs/Node Mem/Node Notes gpu 2 4 - V100/32 GB 32 187 Gb AVX512 Contributed GPU partitions # Partition Nodes GPU type CPUs/Node Mem/Node Notes stamps 1 3 4 - V100/16GB 32 187 Gb AVX512 livi 2 1 HGX-2 16xGPU V100/32GB 48 1500 Gb NVSwitch server agro 3 2 AMD Zen 24 250 Gb AMD Preemptible partitions # The following pre-emptible partition are set for general use of the contributed nodes:\nPartition Contributed by stamps-b Prof. R. Stamps livi-b Prof. L. Livi agro-b Faculty of Agriculture The former five partitions (skylake, compute, largemem, test and gpu) are generally accessible. The next three partitions (stamps, livi and agro) are open only to the contributor\u0026rsquo;s groups.\nOn the contributed partitions, the owners\u0026rsquo; group has preferential access. However, users belonging to other groups can submit jobs to one of the pre-emptible partitions (ending with -b) to run on the contributed hardware as long as it is unused, on the condition that their jobs can be preempted (that is, killed) should owners\u0026rsquo; jobs need the hardware. There is a minimum runtime guaranteed to pre-emptible jobs, which is as of now 1 hour. The maximum wall time for the pre-emptible partition is set per partition (and can be seen in the output of the sinfo command). To have a global overview of all partitions on Grex, run the custom script partition-list from your terminal.\nOn the special partition test, oversubscription is enabled in SLURM, to facilitate better turnaround of interactive jobs.\nJobs cannot run on several partitions at the same time; but it is possible to specify more than one partition, like in --partition=compute,skylake, so that the job will be directed by the scheduler to the first partition available.\nJobs will be rejected by the SLURM scheduler if partition\u0026rsquo;s hardware and requested resources do not match (that is, asking for GPUs on compute, largemem or skylake partitions is not possible). So, in some cases, explicitly adding --partition= flag to SLURM job submission is needed.\nJobs that require stamps-b or gpu partitions have to use GPUs, otherwise they will be rejected; this is to prevent of bogging up the precious GPU nodes with CPU-only jobs!\nstamps: GPU nodes contributed by Prof. R. Stamps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nlivi: GPU node contributed by Prof. L. Livi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nagro: GPU node contributed by Faculty of Agriculture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"11","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/priroda/","permalinkTitle":"Homepage / Software Specific Notes / Running Priroda on Grex","title":"Running Priroda on Grex","content":" Introduction # Priroda is a fast parallel relativistic DFT and ab-initio code for molecular modeling, developed by Dr. Dimitri N. Laikov. The code originally implemented fast resolution-of-identity GGA DFT for coulomb and exchange integrals. Later it was extended to provide RI-DFT with hybrid functional, RI-HF and RI-MP2, and parallel high-level coupled-cluster methods. All these levels of theory can be used together with an efficient all-electron scalar-relativistic method, with small-component bases supplied for all the elements of the Periodic Table. The current release of the code also includes a novel NDO-based semi-empirical method.\nUser Responsibilities and Access # The code is free for academic users, but is not open source. It is distributed on request by the Author, Dr. Dimitri N. Laikov.\nTo access the Priroda code on Grex, the prospective users have to send us ( support@tech.alliancecan.ca) a free-form email confirming that they have read and agreed to abide by the following conditions:\nConditions for the Priroda code access on Grex:\nI understand that the Priroda code\u0026rsquo;s ownership and copyright belongs solely to its Author, Dr. Dimitri N. Laikov. I will not incorporate any part of the Priroda code into any other program system, either for sale or for non-profit distribution, without written permission by the Author. I will not copy, distribute or supply the Priroda code for any reason whatsoever to third persons or organizations. Instead, I will direct all the code requests to the Author. If results obtained with the code are published, I will cite the proper Priroda code references and, when appropriate, the specific methods references, as described in the Priroda code documentation and/or Dr. Laikov\u0026rsquo;s website. I understand that the Priroda code is provided \u0026ldquo;as is\u0026rdquo; and the author is not assuming any responsibilities or any liabilities that might arise from the usage of the code, whatsoever. After receiving the email, we will add the user to the wg-prrda UNIX group that is used to control access to the Priroda program, basis sets and documentation.\nRunning Priroda on Grex # The Priroda code is linked against OpenMPI built with a GCC compiler. There are several versions of them, and module spider priroda would help to locate the dependencies. As of the time of writing this documentation, the following command would load the Priroda version of 2016:\nmodule load gcc/5.2 ompi/3.1.4 priroda/2016 The parallel Priroda executable (called p) will be in the PATH after loading of the module. Its basis sets and/or semi-empirical method parameters can be found under $PRIRODA/bin. Documentation and examples are available under $PRIRODA/doc and $PRIRODA/example, correspondingly.\nThe style of the Priroda input is of free format namelist groups, similar to that of GAMESS-US but more flexible (no limitations inherited from Fortran77). Examples and description of each input group are in the doc and example directories. To invoke the code interactively:\nmpiexec p name.inp name.out An archive of old Priroda documentation is here Priroda old docs from KNC\nUsing Priroda with SLURM # Priroda is MPI-parallelized. The parallel efficiency varies on the method used and the kind of calculation (energies, geometry optimizations or analytical hessians) performed. Pure GGA DFT calculations are quite fast and tightly coupled, and it makes sense to use a single node with a few tasks per node, or a few nodes, as in the example below. RI-MP2 calculations would benefit from more massively parallel calculations, spanning several nodes.\nIt makes no sense to ask more than 4000mb per task.\nScript example for running PRIRODA on Grex run-priroda.sh\r#!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=6 #SBATCH --mem-per-cpu=2000M #SBATCH --time=0-2:00:00 #SBATCH --job-name=priroda-test-c60 SCR=$TMPDIR echo \u0026#34;assuming inputs in $SLURM_SUBMIT_DIR\u0026#34; module load gcc/5.2 ompi/3.1.4 priroda/2016 # copy the input file (c60.inp) locally and set the resource requests # and temporary paths. note that the file myfile.inp # should not have a $system .. $end group cp c60.inp $SCR/priroda.inp cd $SCR echo \u0026#39; \u0026#39; \u0026gt;\u0026gt; priroda.inp echo \u0026#39; $system \u0026#39; \u0026gt;\u0026gt; priroda.inp echo \u0026#34; memory=1000 disk=10 path=. \u0026#34; \u0026gt;\u0026gt; priroda.inp echo \u0026#39; $end \u0026#39; \u0026gt;\u0026gt; priroda.inp cat priroda.inp # Copy basis sets locally: cp $PRIRODA/bin/*.in $SCR cp $PRIRODA/bin/*.bas $SCR echo \u0026#34;Start date:`date`\u0026#34; # Actually run the job srun $PRIRODA/bin/p priroda.inp $SLURM_SUBMIT_DIR/c60.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Various scripts and utilities # There are some simple scripts and utilities in the $PRIRODA/contrib directory. They can be used for conversions of inputs/outputs to and from Molden XYZ format, extraction of the MOs and vibrational frequencies, and restart information from the Priroda output files.\nRelated links # Dr. Laikov\u0026rsquo;s website Priroda old docs from KNC "},{"id":"12","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/vasp/","permalinkTitle":"Homepage / Software Specific Notes / Running VASP on Grex","title":"Running VASP on Grex","content":" Introduction # VASP is a massively parallel plane-wave solid state DFT code. On Grex it is available only for the research groups that hold VASP licenses. To get access, PIs would need to send us a confirmation email from the VASP vendor, detailing the status of their license and a list of users allowed to use it.\nSystem specific notes # On the Grex local software stack, we have VASP 5 and VASP 6 using Intel compiler and OpenMPI 3.1. To find out which versions of VASP are available, use module spider vasp .\nFor a version 6.1.2, at the time of writing the following modules have to be loaded:\nmodule load intel/2019.5 ompi/3.1.4 module load vasp/6.1.2` There are three executables for VASP CPU version: vasp_gam , vasp_ncl , and vasp_std. Refer to the VASP manual as to what these mean. An example VASP SLURM script using the standard version of the VASP binary is below:\nThe following script assumes that VASP6 inputs (INCAR, POTCAR etc.) are in the same directory as the job script.\nScript example for running VASP Grex run-vasp.sh\r#!/bin/bash #SBATCH --ntasks=16 #SBATCH -cpus-per-task=1 #SBATCH --mem-per-cpu=3400M #SBATCH --time=0-3:00:00 #SBATCH --job-name=vasp-test # Adjust the number of tasks, time and memory required. # The above spec is for 16 compute tasks, using 3400 MB per task . # Load the modules: module load intel/2019.5 ompi/3.1.4 module load vasp/6.1.2 echo \u0026#34;Starting run at: `date`\u0026#34; which vasp_std export MKL_NUM_THREADS=1 srun vasp_std \u0026gt; vasp_test.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-vasp.sh, it can be submitted with:\nsbatch run-vasp.sh For more information, visit the page running jobs on Grex\nRelated links # VASP manual Running jobs on Grex "},{"id":"13","rootTitleIndex":"4","rootTitle":"Connecting to Grex and Transferring data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex and Transferring data","permalink":"/grex-docs/connecting/ssh/","permalinkTitle":"Homepage / Connecting to Grex and Transferring data / Connecting to Grex with SSH","title":"Connecting to Grex with SSH","content":" SSH # Most of the work on shared HPC computing systems is done via Linux command line / Shell. To connect, in a secure manner, to a remote Linux system, you would like to use SSH protocol. You will need to have:\naccess to the Internet that lets SSH ports open. a user account on Grex (presently, it is a Compute Canada (an Alliance) Account). and an SSH client for your operating system. If you are not sure what your account on Grex is, check Getting Access. You will also need the DNS name of Grex Which is grex.hpc.umanitoba.ca\nSSH clients # Mac OS X # Mac OS X has a built-in OpenSSH command line client. It also has a full-fledged UNIX shell. Therefore, using SSH under Mac OS is not different from Linux. In any terminal, ssh (as well as scp, sftp) just works with one caveat: for the support of X11 tunneling, some of the Mac OS X versions would require the XQuartz package installed.\nssh -XY username@grex.hpc.umanitoba.ca Please remember to change username in the above command with your Compute Canada (or Alliance) user name.\nLinux # Linux provides the command line SSH package, OpenSSH, which is installed by default in most of the Linux distributions. If not, or you are using a very minimal Linux installation, use your package manager to install OpenSSH package. In any terminal window ssh (as well as scp , sftp ) commands should work. To connect to Grex, use:\nssh -XY username@grex.hpc.umanitoba.ca Similarly to the above, please remember to change username in the above command with your Compute Canada (or Alliance) user name.\nSSH keys # You can manage your SSH keys (adding key pairs, editing known_hosts etc.) in the $HOME/.ssh directory. The Alliance (Compute Canada) user documentation has several pages on managing SSH keys and creating SSH tunnels. Note that if ssh keys are set on CCDB, they should work on the Alliance clusters but not on Grex. This feature is not implemented yet on Grex. Instead of CCDB, on Grex one can install ssh keys locally\nWindows # Windows has a very diverse infrastructure for SSH (and Linux support in general). You would like to pick one of the options below and connect to grex.hpc.umanitoba.ca with your Alliance username and password.\nPutty, WinSCP and VCXsrv # The (probably the most popular) free software combination to work under Windows are:\nPuTTy SSH client: download PuTTy WinSCP graphical SFTP client: download WinSCP A free X11 server for Windows: download VCXSrv WinSCP interacts with PuTTy, so you can configure it to open SSH terminal windows from WinSCP client. For X11 forwarding, make sure the \u0026ldquo;X11 tunneling\u0026rdquo; is enabled in PuTTy\u0026rsquo;s session settings, and VCXSrv is running (it sits in the system tray and does nothing unless you start a graphical X11 application).\nThe Alliance wiki has a PuTTY documentation page with some useful screenshots.\nMobaXterm # There is a quite popular package: MobaXterm. It is not open source, but has a limited free version MobaXterm.\nPlease check out the Alliance\u0026rsquo;s documentation on MobaXterm here\nAll Windows versions, CygWin shell # There is a way to use Linux command shell tools under Windows. Cygwin. When OpenSSH package is installed, you can use OpenSSH\u0026rsquo;s command line tools like ssh, scp and sftp as if you were under Linux:\nssh -Y username@grex.hpc.umanitoba.ca Windows 10, WSL subsystem # There is a Linux Subsystem for Windows which allows you running a containerized instance of Linux (Ubuntu, for example) from under Windows 10. Refer to MS documentation on enabling WSL. Then, you will have the same OpenSSH under Linux.\nIt is actually possible to run X11 applications from WSL as well; you would need to get VCXSrv running, on the Windows side, and DISPLAY variable set on the WSL Linux side.\nWindows 10, native OpenSSH package # Actually, some of the Windows 10 versions have OpenSSH as a standalone package. Refer to corresponding MS documentation on enabling OpenSSH. If it works with your version of Windows 10, you should have the OpenSSH command line tools like ssh, scp and sftp in the Windows command line, as if you were under Linux.\nThe original SSH Secure Shell client # The original SSH Secure Shell and Secure FTP client from this website( www.ssh.fi) is now obsolete. It has been unmaintained since 2001 and may not work with the newest SSH keys/encryption mechanisms and does not have any security updates since 2001. We do not support it and advise users to switch to one of the other more modern clients listed above.\nUsing command line # What to do after you connect? You will be facing a Linux shell, most likely BASH. There is plenty of online documentation on how to use it, HPC Carpentries, Compute Canada\u0026rsquo;s SSH documentation page, Bash Guide for Beginners and simple googling for the commands.\nYou would probably like to explore software via Modules, and learn how to submit jobs.\nInternal links # Using modules Running jobs External links # HPC Carpentries The Alliance SSH documentation page Bash Guide for Beginners Linux introduction "},{"id":"14","rootTitleIndex":"6","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/interactive-jobs/","permalinkTitle":"Homepage / Running jobs on Grex / How to run interactive jobs on Grex?","title":"How to run interactive jobs on Grex?","content":" Interactive work # The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:\nedit files compile codes and run short interactive calculations. configure and build programs (limit the number of threads to 4: make -j4) submit and monitor jobs transfer and/or download data run short tests, \u0026hellip; etc. In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].\nTo not affect other user\u0026rsquo;s experience, please be aware that we reserve to ourselves the right to terminate any intensive process running on the login nodes without prior warning. It is very easy to cause resource congestion on a shared Linux server. Therefore, all production calculations should be submitted in the batch mode, using our resource management system, SLURM. It is possible to submit so-called interactive jobs: a job that creates an interactive session but actually runs on dedicated CPUs (and GPUs if needed) on the compute nodes rather than on the login servers (or head nodes). Note that login nodes do not have GPUs.\nSuch mode of running interactive computations ensures that login nodes are not congested. A drawback is that when a cluster is busy, getting an interactive job to start will take some queuing time, just like any other job. So, in practice interactive jobs are to be short and small to be able to utilize backfill-able nodes. This section covers how to run such jobs with SLURM.\nNote that manual SSH connections to the compute nodes without having active running jobs is forbidden on Grex.\nInteractive batch jobs # To request an interactive job, the salloc command should be used. These jobs are not limited to single node jobs; any nodes/tasks/gpus layout can be requested by salloc in the same way as for sbatch directives. However, to minimize queuing time, usually a minimal set of required resources should be used when submitting interactive jobs (less than 3 hours of wall time, less than 4 GB memory per core, \u0026hellip; etc). Because there is no batch file for interactive jobs, all the resource requests should be added as command line options of the salloc command. The same logic of --nodes=, --ntasks-per-node= , --mem= and --cpus-per-task= resources as per batch jobs applies here as well.\nFor a threaded SMP code asking for half a node for two hours:\nsalloc --nodes=1 --ntasks=1 --cpus-per-task=6 --mem=12000M --partition=compute --time=0-2:00:00 For an MPI jobs asking for 48 tasks, irrespectively of the nodes layout:\nsalloc --ntasks=48 --mem-per-task=2000M --partition=compute --time=0-2:00:00 Similar to batch jobs, specifying a partition with --partition= is required. Otherwise, the default partition will be used (as for now, skylake is set as default partition for CPU jobs).\nInteractive GPU jobs # The difference for GPU jobs is that they would have to be directed to a node with GPU hardware:\nThe GPU jobs should run on the nodes that have GPU hardware, which means you\u0026rsquo;d always want to specify --partition=gpu or --partition=stamps-b.\nSLURM on Grex uses the so-called \u0026ldquo;GTRES\u0026rdquo; plugin for scheduling GPU jobs, which means that a request in the form of --gpus=N or --gpus-per-node=N or --gpus-per-task=N is required. Note that both partitions have up to four GPU per node, so asking more than 4 GPUs per node, or per task, is nonsensical. For interactive jobs, it makes more sense to use single GPU in most of the cases.\nFor an interactive session using two hours of one 16 GB V100 GPU, 4 CPUs and 4000MB per cpu:\nsalloc --gpus=1 --cpus-per-task=4 --mem-per-cpu=4000M --time=0-2:00:00 --partition=stamps-b Similarly, for a 32 GB memory V100 GPU:\nsalloc --gpus=1 --cpus-per-task=4 --mem-per-cpu=4000M --time=0-2:00:00 --partition=gpu Graphical jobs # What to do if your interactive job involves a GUI based program? You can SSH to a login node with X11 forwarding enabled, or using X2Go remote desktop, and run it there. It is also possible to forward the X11 connection to compute nodes where your interactive jobs run with --x11 flag to salloc:\nsalloc --ntasks=1 --x11 --mem=4000M To make it work you\u0026rsquo;d want the SSH session login node is also supporting graphics: either through the -Y flag of ssh (or X11 enabled in PuTTy) or by using X2Go. If you are using Mac OS, you will have to install XQuartz to enable X11 forwarding.\nYou may also try to use OpenOnDemand portal on Grex.\nOpenOnDemand portal on Grex: click on the image to read the documentation "},{"id":"15","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/using-modules/","permalinkTitle":"Homepage / Software and Applications / Modules and software stacks","title":"Modules and software stacks","content":" Introduction # External links # The Alliance documentation about using module "},{"id":"16","rootTitleIndex":"6","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/batch-jobs/","permalinkTitle":"Homepage / Running jobs on Grex / Running batch jobs on Grex","title":"Running batch jobs on Grex","content":" Batch jobs # HPC systems usually are clusters of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users\u0026rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called \u0026ldquo;jobs\u0026rdquo; to a \u0026ldquo;batch queue\u0026rdquo;. A subset of the available resources of the HPC machine is allocated to each of the users\u0026rsquo; batch jobs, and they run without any need for user intervention as soon as the resources become available.\nThe job placement, usage monitoring and job accounting are done via a special software, the HPC scheduler. This is an often-under-appreciated automation that makes usage efficient and saves a lot of work on part of the user. However, using HPC is hard in a sense that users have to make an effort in order to figure out what are the available resources on an HPC cluster, and what is the efficient way of requesting the resources for their jobs. Asking for too many resources might be wasteful both in preventing others from using them and in making for a longer queuing time.\nThe resources (\u0026ldquo;tractable resources\u0026rdquo; in SLURM speak) are CPU time, memory, and GPU time. Generic resources can be software licenses, \u0026hellip; etc. Requesting resources is done via command line options to job submission commands sbatch and salloc, or via special comment lines or SLURM directives (starting with #SBATCH) in job scripts. There are also options to control job placement such as partitions.\nThere are default values for the resources which are taken when you do not specify the resource limit. Note that the default values are, as a rule, quite small. On Grex, the default values are set as follow: 3 hours of wall time, 256mb of memory per CPU. In most of the cases, it is better to have an explicit request of an appropriate resource limit rather than using the default.\nWe ask our users to be fair and considerate and do not allow for deliberate waste of resources (such as running serial jobs on more than one CPU core, or running CPU-only calculations on GPU nodes).\nThere are certain scheduling policies in place to prevent the cluster from being swamped by a single user. In particular, the MAXPS / GrpRunMins limit disfavors asking for many CPU cores for long wall time, a MaxCPU limits restricts number of CPU cores used, and there are limits on number of user\u0026rsquo;s jobs in the system and number of array job elements, as described below.\nScheduling policies # The following policies are implemented on Grex:\nThe default wall time is 3 hours (equivalent to: --time=3:00:00 or --time=0-3:00:00). The default amount of memory per processor (--mem-per-cpu=) is 256 mb. Memory limits are enforced, so an accurate estimate of memory resource (either in the form of --mem= or --mem-per-cpu=) should be provided. The maximum wall time is 21 days on compute and skylake partitions, 14 days on largemem partition. The maximum wall time is 3 days on the gpu partition. The maximum wall time is 7 days on the preempted partitions: stamps-b, livi-b and agro-b. The maximum number of processor-minutes for all currently running jobs of a group without a RAC is 4 M. The maximum number of jobs that a user may have queued to run is 4000. The maximum size of an array job is 2000. Users without a RAC award are allowed to simultaneously use up to 400 CPU cores per accounting group. There are limits on the number of GPUs that can be used on contributed hardware (1 GPU per job). Note that you can see some information about the partitions by running the custom script partition-list from your terminal:\npartition-list Typical batch job cases # Any batch job is submitted with sbatch command. Batch jobs are usually shell (BASH, etc.) scripts wrapping around the invocation of a code. The comments on top of the script that start with #SBATCH are interpreted by the SLURM scheduler as options for resource requests:\nDirective Example Description --ntasks= --ntasks=4 Number of tasks (MPI processes) per job. --nodes= --nodes=2 Number of nodes (servers) per job. --ntasks-per-node= --ntasks-per-node=4 Number of tasks (MPI processes) per node. --cpus-per-task= --cpus-per-task=8 Number of threads per task (should not exceed the number of physical cores. --mem-per-cpu= --mem-per-cpu=1500M Memory per task (or thread). --mem= --mem=16000M Memory per node. --gpus= --gpus=1 Number of GPUs per job. --time- --time=0-8:00:00 wall time in format DD-HH:MM:SS --qos= * QOS by name (Not to be used on Grex!). --partition= --partition=compute Partition name: compute, skylake, \u0026hellip; etc (very much used on Grex!). Assuming the name of myfile.slurm (the name or the extension does not matter, it can be called afile.job, otherjob.sh, \u0026hellip; etc.), a job is submitted with the command:\nsbatch myfile.slurm or\nsbatch [+some options] myfile.slurm Some options like --partition=compute could be invoked at submission time.\nRefer to the official SLURM documentation and/or man sbatch for the available options. Below we provide examples for typical cases of SLURM jobs.\nSerial jobs # The simplest kind of job is a serial job when one compute process runs in a sequential fashion. Naturally, such job can utilize only a single CPU core: even large parallel supercomputers as a rule do not parallelize binary codes automatically. So, the CPU request for a serial job is always 1, which is the default; the other resources can be wall time and memory. SLURM has two ways of specifying the later: memory per core (--mem-per-cpu=) and total memory per node (--mem=). It is more logical to use per-core memory always; except in case of the whole-node jobs when special value --mem=0 gives all the available memory for the allocated node. An example script (for 1 CPU, wall time of 30 minutes and a memory of 2500M) is provided below.\nScript template for serial job run-serial-job-template.sh\r#!/bin/bash #SBATCH --time=0-0:30:00 #SBATCH --mem=2500M #SBATCH --job-name=\u0026#34;Serial-Job-Test\u0026#34; # Script for running serial program: your_program echo \u0026#34;Current working directory is `pwd`\u0026#34; # Load modules if needed: echo \u0026#34;Starting run at: `date`\u0026#34; ./your_program \u0026lt;+options or arguments if any\u0026gt; echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; An important special case of serial jobs is high-throughput computing: jobs are serial because they are too short to parallelize them, however there are very many such jobs per research project. The case of embarrassingly parallel computations like some of the Monte Carlo simulations are often High Throughput Computing (HTC).\nSerial jobs that have regularly named inputs and run more than a few minutes each best be specified as a Job array (see below). Serial jobs that are great in numbers, and run less than a few minutes each, better be joined into a task farm running within a single larger job using tools like GLOST, GNU Parallel or a workflow engine like QDO. An example of GLOST job is under the MPI jobs section (see below).\nSMP / threaded / single node jobs # The next kind of job is multi-threaded, shared memory or single-node parallel jobs. Often these jobs are for Symmetric Multiprocessing (SMP) codes that can use more than one CPU on a given node to speed up the calculations. However, SMP/multithreaded jobs rely on some form of inter-process communication (shared memory, \u0026hellip; etc.) that limits them to the CPU cores within just a single server. They cannot scale across multiple compute nodes. Examples are OpenMP, pthreads, Java codes, etc. Gaussian and PSI4 are SMP codes; threaded BLAS/LAPACK routines from MKL (inside NumPY) can utilize multiple threads, \u0026hellip; etc. Note that this kind of programs do not scale very well when increasing the number of threads. We recommend to our users to run a benchmark to see how their programs scale with the number of threads to define the combination or a set of threads for better performance.\nThus, from the point of view of the SMP/threaded jobs resources request, the following considerations are important:\nasking always only a single compute node and one task (--nodes=1 --ntasks=1) job. asking for several CPU cores on it per job, up to the maximum number of CPU cores per node (--cpus-per-task=N) where N should not exceed the total physical cores available on the node. Depending on the partition, you may choose N up to 12 on the compute partition, up to 52 on the skylake partition, up to 40 on the largemem partition, \u0026hellip; etc. making sure that the total memory asked for does not exceed the memory available on the node (refer to the section about node characteristics, hardware for more information). making sure that the code would use exactly the number of CPU cores allocated to the job, to prevent waste or congestion of the resources. In SLURM, it makes a difference whether you ask for parallel tasks (--ntasks) or threads (--cpus-per-task) ; the threads should not be isolated from each other (because they might need to use shared memory!) but the tasks are isolated to each own \u0026ldquo;cgroup\u0026rdquo;.\nAn environment variable ${SLURM_CPUS_PER_TASK} is set in the job, so you can set an appropriate parameter of your code to the same value.\nFor OpenMP, it would be done like:\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} For MKL it is MKL_NUM_THREADS, for Julia --JULIA_NUM_THREADS, for Java -Xfixme parameter.\nScript template for running a job on **compute** partition: using full node run-smp-job-node-template.sh\r#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --nodes=1 #SBATCH --ntask-per-node=1 #SBATCH --cpus-per-task=12 #SBATCH --mem=0 #SBATCH --partition=compute #SBATCH --job-name=\u0026#34;OMP-Job-Test\u0026#34; # An example of an OpenMP threaded job that # takes a whole \u0026#34;old\u0026#34; Grex node for 8 hours. export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; ./your-openmp.x input.dat \u0026gt; output.log echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; Note that the above example requests whole node\u0026rsquo;s memory with --mem=0 because the node is allocated to the job fully due to all the CPUs anyways. It is easier to use the --mem syntax for SMP jobs because typically the memory is shared between threads (i.e., the amount of memory used does not change with the number of SMP threads). Note, however, that the memory request should be reasonably \u0026ldquo;efficient\u0026rdquo; if possible.\nIt is also possible to use a fraction of the node for running OpenMP job. Here is an example asking for 1 task with 4 threads on compute partition:\nScript template for running a job on **compute** partition: using a fraction of the node run-smp-job-partial-node-template.sh\r#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --nodes=1 #SBATCH --ntask-per-node=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=8000M #SBATCH --partition=compute #SBATCH --job-name=\u0026#34;OMP-Job-Test\u0026#34; # An example of an OpenMP threaded job that # takes a whole \u0026#34;old\u0026#34; Grex node for 8 hours. export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; ./your-openmp.x input.dat \u0026gt; output.log echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; GPU jobs # The GPU jobs would usually be similar to SMP/threaded jobs, with the following differences:\nThe GPU jobs should run on the nodes that have GPU hardware, which means you\u0026rsquo;d want always to specify one of the following options: Directive Description --partition=gpu to use the gpu partition. --partition=stamps-b to use the stamps-b partition. --partition=livi-b to use the livi-b partition. --partition=agro-b to use the agro-b partition. SLURM on Grex uses the so-called \u0026ldquo;GTRES\u0026rdquo; plugin for scheduling GPU jobs, which means that the request syntax in the form --gpus=N or --gpus-per-node=N or --gpus-per-task=N is used. How many GPUs to ask for? # Grex, at the moment, does not have GPU-direct MPI enabled, which means that most of the jobs would be single-node. The GPU nodes in either gpu (two nodes, 32GB V100s) or stamps-b (three nodes, 16GB V100s) partition have 4 V100 GPUs, 32 Intel 52xx CPUs and 192GB of CPU memory. There is also the livi-b partition with a large single 16x v100 GPU server. So, asking 1 to 4 GPUs, one node, and 6-8 CPUs per GPU with an appropriate amount of RAM (4-8 Gb) per job would be a good starting point.\nNote that V100 is a fairly large GPU for most of the jobs, and for good utilization of the GPU resources available on Grex, it is a good idea to start with a single GPU, and then try if the code actually is able to saturate it with load. Many codes cannot scale to utilize more than one GPU, and few codes can utilize more than two of them.\nScript template for running ont-guppy on GPU run-guppy-gpu.sh\r#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=stamps-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=6 #SBATCH --mem-per-cpu=6000M #SBATCH --time=0-12:00:00 #SBATCH --job-name=genomics-test # Adjust the resource requests above to your needs. # Example of loading modules, CUDA: module load gcc/4.8 cuda/10.2 export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; nvidia-smi guppy_basecaller -x auto --gpu_runners_per_device 6 -i Fast5 -s GuppyFast5 -c dna_r9.4.1_450bps_hac.cfg echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; The above script (if called say, gpu.job) can be submitted with the usual command:\nsbatch gpu.job Distributed, massively parallel jobs # Parallel jobs that can spawn multiple servers are the most scalable ones, because they are not limited by the number of CPUs or memory per node. Running many parallel tasks across more than one node requires some inter-node communication (which as a rule is slower than shared memory within one server). In HPC, high speed interconnect and specialized RDMA-aware communication libraries make distributed parallel computation very scalable. Grex uses InfiniBand interconnect (IB).\nMost often (but not always), parallel programs are built upon a low-level message passing library called MPI. Refer to the Software section for more information about parallel libraries on Grex. Examples of distributed parallel codes are GAMESS-US, ORCA, LAMMPS, VASP, \u0026hellip; etc.\nThe distributed parallel jobs can be placed across their compute nodes in several ways (i.e., how many parallel tasks per compute node?). Thus, SLURM resource request syntax allows to specify the required layout of nodes/tasks (or nodes/tasks/threads, or even nodes/tasks/GPUs since hybrid MPI+OpenMP and MPI+GPU programs exist). A consideration about the layout is a tradeoff between making the program work faster (and sometimes to work correctly at all) and making the scheduler\u0026rsquo;s work easier.\nA well written MPI software theoretically should not care how the tasks are distributed across how many physical compute nodes. Thus, SLURM\u0026rsquo;s --ntasks= request (similar to the old Torque procs=) specified without --nodes would work and make the scheduling easier.\nA note on process starting:\nSince MPI jobs are distributed, there should be a mechanism to start the compute processes across all of the nodes (or CPUs) allocated for it. The mechanism should know which nodes to use, and how many. Most modern MPI implementations \u0026ldquo;tightly integrate\u0026rdquo; with SLURM, so they will get this information automatically via a Process Management Interface (PMI). SLURM provides its own job starting command called srun. Most MPI implementations also provide their own job spawned commands, usually called mpiexec or mpirun. These are specific to each MPI vendor/kind and not well standardized, and differ in the support of SLURM.\nFor example, OpenMPI (the default, supported MPI implementation) on Grex is compiled against PMIx (3.x, 4.x) or PMI1 (1.6.5). So, it is preferable to use srun instead of mpiexec to kick start the MPI processes, because srun would use PMI.\nFor Intel MPI (another MPI, also available on Grex and required by some of the binary codes, like ANSYS or ADF), srun sometimes may not work, but PMI1 can be used with mpiexec.hydra by setting the following environment variable:\nexport I_PMI_LIBRARY=/opt/slurm/lib/libpmi.so Some examples # Here is an example for running MPI job (in this case, Quantum ESPRESSO) using 32 cores:\nScript template for running QE on 32 CPUs run-qe-mpi-template.sh\r#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --mem-per-cpu=1500M #SBATCH --ntasks=32 #SBATCH --job-name=\u0026#34;QE-Job\u0026#34; # A example of an MPI parallel that # takes 32 cores on Grex for 8 hours. # Load the modules: module load intel/15.0.5.223 ompi/3.1.4 espresso/6.3.1 export OMP_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; srun pw.x -in MyFile.scf.in \u0026gt; Myfile.scf.log echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; However, in practice there are cases when layout should be more restrictive. If the software code assumes equal distribution of processes per node, the request should be --nodes=N --ntasks-per-node=M. A similar case is MPMD codes (Like NWCHem or GAMESS-US or OpenMolcas) that have some of the processes doing computation and some communication functions, and therefore requires at least two tasks running per each node.\nFor some codes, especially for large parallel jobs with intensive communication between tasks there can be performance differences due to memory and interconnect bandwidths, depending on whether the same number of parallel tasks is compacted on few nodes or spread across many of them. Find an example of the job below.\nScript template for running NWChem on 32 cores distributed on 4 nodes run-nwchem-template.sh\r#!/bin/bash #SBATCH --nodes=4 #SBATCH --ntasks-per-node=8 #SBATCH --mem-per-cpu=4000M #SBATCH --job-name=\u0026#34;NWchem-Job\u0026#34; #SBATCH --job-name=NWchem-dft-test # Adjust the number of tasks, time and memory required. # the above spec is for 32 compute tasks over 4 nodes. # Load the modules: module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1 echo \u0026#34;Starting run at: `date`\u0026#34; which nwchem # Uncomment/Change these in case you want to use custom basis sets NWCHEMROOT=/global/software/cent7/nwchem/6.8.1-intel15-ompi314 export NWCHEM_NWPW_LIBRARY=${NWCHEMROOT}/data/libraryps export NWCHEM_BASIS_LIBRARY=${NWCHEMROOT}/data/libraries # In most cases SCRATCH_DIR would be on local nodes scratch # While results are in the same directory export NWCHEM_SCRATCH_DIR=$TMPDIR export NWCHEM_PERMANENT_DIR=`pwd` # Optional memory setting; note that this one or the one in your code # must match the #SBATCH --mem-per-cpu times compute tasks ! export NWCHEM_MEMORY_TOTAL=2000000000 # 24000 MB, double precision words only export MKL_NUM_THREADS=1 srun nwchem dft_feco5.nw \u0026gt; dft_feco5.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; OpenMPI # OpenMPI is the default MPI implementation for Grex (and Compute Canada, now the Alliance). The modules for it on Grex are called ompi . The MPI example scripts above are all OpenMPI based. The old version 1.6.5 is there for compatibility reasons with older software; most users should use 3.1.x or 4.x.x versions. Using srun is recommended in all cases.\nIntel MPI # For applications using IntelMPI (impi modules on Grex, or Intel-MPI based software from Compute Canada CVMFS software stack), a few environment variables have to be set. The following link explains it: Using SLURM with PMI.\nThe JLab documentation example shows an example of SLURM script with IntelMPI.\nOther MPIs # Finally, some canned codes like ANSYS or StatCCM+ would use a vendor-specific MPI implementation that would not tightly integrate with our scheduler\u0026rsquo;s process to CPU core placement. In that case, several whole nodes (that is, with the number of tasks equal to the node\u0026rsquo;s number of CPU cores) should be requested to prevent the impact on other jobs with resource congestion.\nSuch codes will also require a nodelist (machinefile) file obtained from SLURM and provided to them in their own format.\nA custom script slurm_hl2hl.py makes this easier (see CC StarCCM+ or CC ANSYS documentation). The script slurm_hl2hl.py is already available on Grex.\nslurm_hl2hl.py --format STAR-CCM+ \u0026gt; machinefile Job arrays # Job arrays allow for submitting many similar jobs \u0026ldquo;in one blow\u0026rdquo;. It saves users work on job submission, and also makes SLURM scheduler more efficient in scheduling the array jobs because it would know they are the same with respect to size, expected wall time etc.\nArray jobs work most naturally when a single code has to be applied for parameter sweep and/or to a large number of input files that are regularly named, for example as: test1.in, test2.in, \u0026hellip; test99.in\nThen, a single job script with #SBATCH --array=1,99 can be used to submit the 99 jobs.\nIn order to distinguish between the input files, within each of the jobs at run time, you would have to obtain a value for the array index. Which is set by SLURM as ${SLURM_ARRAY_TASK_ID} environment variable. The call to the code on a particular input will then be like:\n./my_code test${SLURM_ARRAY_TASK_ID}.in This way each of the array element jobs can distinguish their own portion of the work to do. A real life example is below; it attempts to run all of the Gaussian standard tests which have names of the format test0001.com, test0002.com, .. test1204.com, etc. Note the printf trick to deal with trailing zeroes in the input names.\nScript template for running job array: case of Gaussian standard tests run-array-job-gauss-tests.sh\r#!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1000MB #SBATCH --job-name=\u0026#34;G16-tests\u0026#34; #SBATCH --array=1-1204 echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Running on `hostname`\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; # Set up the Gaussian environment using the module command: module load gaussian/g16.c01 # Run g16 on an array job element id=`printf \u0026#34;%04d\u0026#34; $SLURM_ARRAY_TASK_ID` v=test${id}.com w=`basename $v .com` g16 \u0026lt; $v \u0026gt; ${w}.${SLURM_JOBID}.out echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; There are limits on how large array jobs can be (see our scheduling policies): the maximal number of elements in job array, as well as the maximal number of jobs that can be submitted by a user.\nUsing CC CVMFS software # As explained in more detail in the software/Modules documentation, we provide Compute Canada\u0026rsquo;s software environment. Most of it can run out of the box by just specifying the corresponding module.\nThere are some caveats:\nSome of the Compute Canada software might have hardcoded environment variables that exist only on these systems. An example is SLURM_TMPDIR. On Grex, add export SLURM_TMPDIR=$TMPDIR to your job scripts.\nIn general, it is hard to containerize HPC. So the software that requires low-level hardware/device drivers access (OpenMPI, CUDA) may have problems when running on non-CC systems. Newer version of OpenMPI (3.1.x) seems to be more portable for using the PMIx job starting mechanism.\n\u0026ldquo;Restricted\u0026rdquo; (commercial) software\u0026rsquo;s binaries are not distributed by Compute Canada CVMFS due to the obvious licensing issues. It has to be installed locally on Grex.\nHaving said that, module load CCEnv gives the right software environment to be run on Grex for a vast majority of threaded and serial software items from CC software stack. See discussion about MPI-parallel jobs below.\nBecause of the distributed nature of CVMFS, it might take time to download a program or library or data file. It would probably make sense to first access it interactively or from an interactive job to warm the CVMFS local cache, to avoid job failures due to the delay.\nBelow is an example of an R serial job that uses quite a few packages from Compute Canada software stack.\nScript template for running R using a module from Compute Canada software stack. run-r-cc-cvmfs-template.sh\r#!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-72:00:00 #SBATCH --job-name=\u0026#34;R-gdal-jags-bench\u0026#34; cd ${SLURM_SUBMIT_DIR} # Load the modules: module load CCEnv module load nixpkgs/16.09 gcc/5.4.0 module load r/3.5.2 jags/4.3.0 geos/3.6.1 gdal/2.2.1 export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; R --vanilla \u0026lt; Benchmark.R \u0026amp;\u0026gt; benchmark.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Users of contributed systems which are newer than the original Grex nodes might want to switch to arch/avx2 or arch/avx512 from the default arch/sse3.\nUsing CC CVMFS software that is MPI-based. # We have found that the recent Compute Canada toolchains that use OpenMPI 3.1.x work on Grex without any changes (that is, with srun). Therefore, for OpenMPI based applications, we recommend to load Compute Canada\u0026rsquo;s software that depends on the recent toolchains, 2018.3 or later (Intel 2018 compilers, GCC 7.3 compilers and openmpi/3.1.2).\nFor example, the module commands below would load the Intel/OpenMPI 3.1.2 toolchain-based environment:\nmodule load CCEnv module load StdEnv/2018.3 Below is an arbitrarily chosen IMB benchmark result for MPI1 on Grex, the sendrecv tests using two processes on two nodes with several MPI implementations (CC means MPI coming from the Compute Canada (now, the Alliance) stack, Grex means compiled locally on Grex).\nYou can see that differences in performance between OpenMPI 3.1.x from CC stack and Grex are minor for this benchmark, even without attempting any local tuning for the CC OpenMPI.\nUsing New 2020 CC CVMFS Stack # Since Spring 2021, Compute Canada has updated the default software stack on their CVMFS distribution to StdEnv/2020 and gentoo. This version will not run on legacy Grex partitions (compute) at all, because it requires AVX2 CPU architecture. It will work as expected on all new GPU and CPU nodes (skylake, largemem, gpu and contributed systems).\nRelated links # SLURM "},{"id":"17","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/code-development/","permalinkTitle":"Homepage / Software and Applications / Code Development on Grex","title":"Code Development on Grex","content":" Introduction # Grex comes with a sizable software stack that contains most of the software development environment for typical HPC applications. This section of the documentation covers the best practices for compiling and building your own software on Grex.\nThe login nodes of Grex can be used to compile codes and to run short interactive and/or test runs. All other jobs must be submitted to the batch system. We do not do as heavy resource limiting on Grex login nodes as, for example, Compute Canada does; so, code development on login nodes is entirely possible. However, it might still make sense to perform some of the code development in interactive jobs, in cases such as (but not limited to):\n(a) the build process and/or tests requires heavy, many-core computations\n(b) you need access to specific hardware not present on the login nodes, such as GPUs and AVX512 CPUs.\nMost of the software on Grex is available through environmental modules. To find a software development tool or a library to build your code against, the module spider command is a good start. The applications software is usually installed by us from sources, into subdirectories under /global/software/cent7\nIt is almost always better to use communication libraries (MPI) provided on Grex rather than building your own, because ensuring tight integration of these libraries with our SLURM scheduler and low-level, interconnect-specific libraries might be tricky.\nGeneral CentOS-7 notes # The base operating system on Grex is CentOS 7.x that comes with its set of development tools. However, due to the philosophy of CentOS, the tools are usually rather old. For example, cmake and git are ancient versions of 2.8, 1.7 respectively. Therefore, even for these basic tools you more likely want to load a module with newest versions of these tools:\nmodule load git module load cmake CentOS also has its system versions of Python, Perl, and GCC compilers. When no modules are loaded, the binaries of these will be available in the PATH. The purpose of these is to make some systems scripts possible, to compile OS packages, drivers and so on.\nWe do not install many packages for the dynamic languages system-wide, because it makes maintaining different versions of them complicated. The same advice applies: use the module spider command to find a version of Perl, Python, R, etc. to suit your needs. The same applies to compiler suites like GCC and Intel.\nWe do install CentOS packages with OS that are:\nbase OS packages necessary for functioning graphical libraries that have many dependencies never change versions that are not critical for performance and/or security. Here are some examples: FLTK, libjpeg, PCRE, Qt4 and Gtk. Login nodes of Grex have many \u0026lsquo;\u0026rsquo;-devel\u0026rsquo;\u0026rsquo; packages installed, while compute nodes do not because we want them lean and quickly re-installable. Therefore, compiling codes that requires \u0026lsquo;\u0026rsquo;-devel\u0026rsquo;\u0026rsquo; base OS packages might fail on compute nodes. Contact us if something like that happens when compiling or running your applications. Finally, because HPC machines are shared systems and users do not have sudo access, following some instructions from a Web page that asks for apt-get install this or yum install that will fail. Rather, module spider should be used to see if the package you want is already installed and available as a module. If not, you can always contact support and ask for help to install the program either under your account or as a module when possible.\nCompilers and Toolchains # Due to the hierarchical nature of our Lmod modules system, compilers and certain core libraries (MPI and CUDA) form toolchains. Normally, you would need to choose a compiler suite (Intel or GCC) and, in case of parallel applications, a MPI library (OpenMPI or IntelMPI). These come in different versions. Also, you\u0026rsquo;d want to know if you want CUDA should your applications be able to utilize GPUs. A combination of compiler/version, MPI/version and possibly CUDA makes a toolchain. Toolchains are mutually exclusive; you cannot mix software items compiled with different toolchains!\nSee Modules for more information.\nThere is no module loaded by default! There will be only the system\u0026rsquo;s GCC-4.8 and no MPI whatsoever. To get started, load a compiler/version. Then, if necessary, an MPI (ompi or impi) and if necessary, CUDA (for which 10.2 is the current version, there is no known reason to use another).\nmodule load intel/2019.5 module load ompi/3.1.4 The above loads Intel compilers and OpenMPI 3.1.4. The example below is for GCC 7 and openmpi 4.1.2.\nmodule load gcc/7.4 module load ompi/4.1.2 The MPI wrappers (mpicc, mpicxx, mpif90, \u0026hellip; etc.) will be set correctly by ompi modules to point to the right compiler.\nIntel compilers suite # At the moment of writing this documentation, the following Intel Compilers Suites are available on Grex:\nIntel 2020.4 : a recent Intel Parallel Studio suite. Intel 2019.5 : a recent Intel Parallel Studio suite. Most software is compiled with it, so use this one if unsure. Intel 2017.8 : a somewhat less recent Intel Parallel studio suite. Intel 15.0 : Legacy, for maintenance of older Grex software. Do not use it for anything new, unless absolutely must. Intel 14.1 : a very old one, for maintenance of older Grex software, and broken. Do not use. It will be removed soon. Intel 12.1 : a very old one, for maintenance of a very old PETSc version. Do not use. The name for the Intel suite modules is intel; module spider intel is the command to find available Intel versions. The latter is left for compatibility with legacy codes. It does not work with systems C++ standard libraries well, so icpc for Intel 12.1 might be dysfunctional. So the intel/12.1 toolchain is actually using GCC 4.8\u0026rsquo;s C++ and C compilers.\nIf unsure, or do not have a special reason otherwise, use Intel 15.0 compilers (icc, icpc, ifort). Intel 15.0 is probably the first Intel compiler to support AVX512 if you are going to use the contributed nodes that have AVX512 architecture.\nThe Intel compilers suite also provides tools and libraries such as MKL (Linear Algebra, FFT, etc.), Intel Performance Primitives (IPP), Intel Threads Building Blocks (TBB), and VTune. Intel MPI as well as MKL for GCC compilers are available as separate modules, should they be needed for use separately.\nGCC compilers suite # At the moment of writing this page, the following GCC compilers are available:\nGCC 11.2 GCC 9.2 GCC 7.4 GCC 5.2 GCC 4.8 The name for GCC is gcc, as in module spider gcc. The GCC 4.8 is a placeholder module; its use is to unload any other modules of the compiler family, to avoid toolchains conflicts. Also, GCC 4.8 is the only multi-lib GCC compiler around; all the others are strictly 64-bit, and thus unable to compile legacy 32-bit programs.\nFor utilizing the AVX512 instructions, probably the best way is to go with the latest GCC compilers (9.2 and 7.4) and and latest MKL. GCC 4.8 does not handle AVX512. Generally Intel compilers outperform GCC, but GCC might have better support for the recent C++11,14,17 standards.\nMPI and Interconnect libraries # The standard distribution of MPI on Grex is OpenMPI. We build most of the software with it. To keep compatibility with the old Grex software stack, we name the modules ompi. MPI modules depend on the compiler they were built with, which means that a compiler module should be loaded first; then the dependent MPI modules will become available as well. Changing the compiler module will trigger automatic MPI module reload. This is how the Lmod hierarchy works now.\nFor a long time Grex was using the interconnect drivers with ibverbs packages from the IB hardware vendor, Mellanox. It is no longer the case: for CentOS-7, we have switched to the vanilla Linux InfiniBand drivers, the open source RDMA-core package, and OpenUCX libraries. The current version of UCX on Grex is 1.6.1. Recent versions of OpenMPI (3.1.x and 4.0.x) do support UCX. Also, our OpenMPI is built with process management interface versions PMI1, PMIx2 and 3, for tight integration with the SLURM scheduler.\nThe current default and recommended version of MPI is OpenMPI 4.1.1. OpenMPI 4.1 works well for new codes but could break old ones. There is an older version, OpenMPI 3.1.4 or 3.1.6 that is more compatible. A very old OpenMPI 1.6.5 exists for compatibility with older software.\nmodule load ompi/3.1.4 There is also IntelMPI, for which the modules are named impi. See the notes on running MPI applications under SLURM here.\nAll MPI modules, be that OpenMPI or Intel, will set MPI compiler wrappers such as mpicc, mpicxx, mpif90 to the compiler suite they were built with. The typical workflow for building parallel programs with MPI would be to first load a compiler module, then an MPI module, and then use the wrapper of C, C++ or Fortran in your makefile or build script.\nIn case a build or configure script does not want to use the wrapper and needs explicit compiler and link options for MPI, OpenMPI wrappers provide the --show option that lists the required command line options. Try for example:\nmpicc \u0026ndash;show\nto print include and library flags to the C compiler to be linked against the currently loaded OpenMPI version.\nLinear Algebra BLAS/LAPACK # It is always a bad idea to use the reference BLAS/LAPACK/CLAPACK from Netlib (or the generic -lblas, -llapack from CentOS or EPEL which also likely is the reference BLAS/LAPACK from Netlib). The physical Computer architecture has much evolved, and is now way different from the logical Computer the human programmer is presented with. Today, it takes careful, manual assembly coding optimization to implement BLAS/LAPACK that performs fast on modern CPUs with their memory hierarchies, instruction prefetching and speculative execution. A vendor-optimized BLAS/LAPACK implementation should always be used. For the Intel/AMD architectures, it is Intel MKL, OpenBLAS, and BLIS.\nAlso, it is worth noting that the linear algebra libraries might come with two versions: one 32-bit array indexes, another full 64-bit. Users must pay attention and link against the proper version for their software (that is, a Fortran code with -i8 or -fdefault-integer-8 would link against 64-bit pointers BLAS).\nMKL # The fastest BLAS/LAPACK implementation from Intel. With Intel compilers, it can be used as a convenient compiler flag, -mkl or if threaded version is not needed, -mkl=sequential.\nWith both Intel and GCC compilers, the MKL libraries can be linked explicitly with compiler/linker options. The base path for MKL includes and libraries is defined as the MKLROOT environment variable. For GCC compilers, module load mkl is needed to add MKLROOT to the environment. There is a command line advisor Website to pick the correct order and libraries. Libraries with the _ilp64 suffix are for 64-bit indexes while _lp64 are for the default, 32-bit indexes.\nNote that when Threaded MKL is used, the number of threads is controlled with the MKL_NUM_THREADS environment variable. On the Grex software stack, it is set by the MKL module to 1 to prevent accidental CPU oversubscription. Redefine it in your SLURM job scripts if you really need threaded MKL execution as follows:\nexport MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK We use the MKL\u0026rsquo;s BLAS and LAPACK for compiling R and Python\u0026rsquo;s NumPY package on Grex, and that\u0026rsquo;s one example when threaded MKL can speed up computations if the code spends significant time in linear algebra routines by using SMP.\nNote that MKL also provides ScaLAPACK and FFTW libraries.\nOpenBLAS # The successor and continuation of the famous GotoBLAS2 library. It contains both BLAS and LAPACK in a single library, libopenblas.a . Use \u0026lsquo;\u0026lsquo;module spider openblas\u0026rsquo;\u0026rsquo; to find available versions for a given compiler suite. We provide both 32-bit and 64-bit indexes versions (and reflect it in the version names, like openblas/0.3.7-i32). The performance of OpenBLAS is close to that of MKL.\nBLIS # Blis is a recent, C++ template based implementation of linear algebra that contains a BLAS interface. On Grex, only 32-bit indexes BLIS is available at the moment. Use \u0026lsquo;\u0026lsquo;module spider blis\u0026rsquo;\u0026rsquo; to see how to load it.\nScaLAPACK # MKL has ScaLAPACK included. Note that it depends on BLACS which in turn depends on an MPI version. MKL comes with support of OpenMPI and IntelMPI for the BLACS layer; it is necessary to pick the right library of them to link against.\nThe command line advisor is helpful for that.\nFast Fourier Transform (FFTW) # FFTW3 is the standard and well performing implementation of FFT. module spider fftw should find it. There is a parallel version of the FFTW3 that depends on MPI it uses, thus to load the fftw module, compiler and MPI modules would have to be loaded first. MKL also provides FFTW bindings, which can be used as follows:\nEither Intel or GCC MKL modules would set the MKLROOT environment variable, and add necessary directories to LD_LIBRARY_PATH. The MKLROOT is handy when using explicit linking against libraries. It can be useful if you want to select a particular compiler (Intel or GCC), pointer width (the corresponding libraries have suffix _lp64 for 32-bit pointers and_ilp64 for 64 bit ones; the later is needed for, for example, Fortran codes with INTEGER*8 array indexes, explicit or set by -i8 compiler option) and kind of MPI library to be used in BLACS (OpenMPI or IntelMPI which both are available on Grex). An example of the linker options to link against sequential, 64 bit pointed version of BLAS, LAPACK for an Intel Fortran code is:\nifort -O2 -i8 main.f -L$MKLROOT/lib/intel64 -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core -lpthread -lm MKL also has FFTW bindings. They have to be enabled separately from the general Intel compilers installation; and therefore, details of the usage might be different between different clusters. On Grex, these libraries are present in two versions: 32-bit pointers (libfftw3xf_intel_lp64) and 64-bit pointers (fftw3xf_intel_ilp64). To link against these FFT libraries, the following include and library options to the compilers can be used (for the _lp64 case):\n-I$MKLROOT/include/fftw -I$MKLROOT/interfaces/fftw3xf -L$MKLROOT/interfaces/fftw3xf -lfftw3xf_intel_lp64 The above line is, admittedly, rather elaborate but give the benefit of compiling and building all of the code with MKL, without the need for maintaining a separate library such as FFTW3.\nHDF5 and NetCDF # Popular hierarchical data formats. Two versions exist on the Grex software stack, one serial and another MPI-dependent version. Which one you load depends whether MPI is loaded.\nTo see the available versions, use:\nmodule spider hdf5 and/or:\nmodule spider netcdf Python # There are Conda python modules and Python built from sources with a variety of the compilers. The conda based modules can be distinguished by the module name. Note that the base OS python should in most cases not be used; rather use a module:\nmodule spider python We do install certain most popular python modules (such as Numpy, Scipy, matplotlib) centrally. pip list and conda list would show the installed modules.\nR # We build R from sources and link against MKL. There are Intel 15 versions of R; however, we find that some packages would only work with GCC-compiled versions of R because they assume GCC or rely on some C++11 features that the older Intel C++ might be lacking. To find available modules for R, use:\nmodule spider \u0026#34;r\u0026#34; Several R packages are installed with the R modules on Grex. Note that it is often the case that R packages are bindings for some other software (JAGS, GEOS, GSL, PROJ, etc.) and require the software or its dynamic libraries to be available at runtime. This means, the modules for the dependencies (JAGS, GEOS, GSL, PROJ) are also to be loaded when R is loaded.\n"},{"id":"18","rootTitleIndex":"4","rootTitle":"Connecting to Grex and Transferring data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex and Transferring data","permalink":"/grex-docs/connecting/x2go/","permalinkTitle":"Homepage / Connecting to Grex and Transferring data / Connecting to Grex via X2Go","title":"Connecting to Grex via X2Go","content":" Graphical user interface access with X2Go # Linux (and UNIX) have a graphical user interface framework, which is called X11, X-Window system. It is possible to use remote X11 applications with the combination of SSH client with X11-tunneling enabled and a local X11-server running. However, this way is often quite slow and painful, especially over WAN networks, where latencies of the network really impair user experience.\nLuckily, there is a solution for this, which is called NX. NX software on client and server sides caches and compresses X11 traffic, leading to a great improvement of the performance of X11 applications. A free NX-based remote desktop solution exists, and is called X2Go.\nOn Grex, we have supported X2Go since 2015; that is, we run an X2Go server on Grex login nodes. So, if you have a valid Grex account, and an X2Go client installed on your local machine, you can connect to Grex and use a remote Linux desktop to run your favorite GUI applications.\nSince X2Go runs over an encrypted SSH connection, it does not require anything else to access Grex. If you have SSH command line access working, and have X2Go client working, it should be enough to get you started.\nX2Go clients and sessions # The X2Go authors provide clients for Mac OS X, Linux and Windows operating systems: download X2Go.\nThere are also alternative X2Go clients (PyHoca CLI and GUI, etc.) that you could try, but we will not cover them here.\nAfter installing the X2Go client, you\u0026rsquo;d need to start it and create a \u0026ldquo;New Session\u0026rdquo; by clicking the corresponding icon.\nFor now, there is no load balancing support for connections: while connecting to the host address grex.hpc.umanitoba.ca will work, session suspend/resume functionality might require specifying connection to a physical Grex login node explicitly, using either of tatanka.hpc.umanitoba.ca or bison.hpc.umanitoba.ca correspondingly in the Host field. (You can also create two sessions, one for tatanka and another for bison.)\nThe same username should be used as for SSH text based connections in the Login field. It is also possible to provide an SSH key instead of the password.\nWhen creating a new Session, a \u0026ldquo;Desktop Environment\u0026rdquo; needs to be selected in the \u0026ldquo;Session Type\u0026rdquo; menu Not all DE\u0026rsquo;s that are listed in this X2Go menu are available on Grex. We support the following Linux Desktop environments:\nOPENBOX: a lightweight DE (Desktop Environment) IceWM: a lightweight DE that looks like Windows95 XFCE4: a full-fledged Linux DE It is also possible to avoid using the desktops altogether and select \u0026ldquo;Published Applications\u0026rdquo; instead following the documentation here; however, most of the Grex applications are only accessible as modules and therefore not present in this menu.\nIn the Media tab, you might want to disable printing and sound support to suppress the corresponding warnings.\nAfter saving the new session, you should be able to connect to Grex with X2Go.\nProblems and Limitations of X2Go # X2Go relies on an older version of NX library that might fail to support newer versions of OpenGL based software.\nExternal links # The Alliance documentation page about X2Go X2Go installation on the X2Go Wiki X2Go FAQ "},{"id":"19","rootTitleIndex":"4","rootTitle":"Connecting to Grex and Transferring data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex and Transferring data","permalink":"/grex-docs/connecting/ood/","permalinkTitle":"Homepage / Connecting to Grex and Transferring data / Connect and Transfer data with OOD","title":"Connect and Transfer data with OOD","content":" OSC OpenOnDemand on Grex # Grex\u0026rsquo;s OOD instance runs on aurochs.hpc.umanitoba.ca . It is available only from UManitoba IP addresses \u0026ndash; that is, your computer should be on the UM Campus network to connect.\nTo connect from outside the UM network, please install and start UManitoba Virtual Private Network. OOD relies on in-browser VNC sessions; so, a modern browser with HTML5 support is required; we recommend Google Chrome or Firefox and its derivatives (Firefox, for example).\nConnect to OOD using UManitoba VPN:\nMake sure Pulse Secure VPN is connected Point your Web browser to https://aurochs.hpc.umanitoba.ca Use your Compute Canada username and password to log in to Grex OOD. Connect via OOD # OpenOndemand login page When connected, you will see the following screen with the current Grex Message-of-the-day (MOTD):\nFile view on OpenOndemand web portal on Grex OOD expects user accounts and directories on Grex to be already created. Thus, new users who want to work with OOD should first connect to Grex normally, via SSH shell at least once, to make the creation of account, directories, and quota complete. Also, OOD creates a state directory under users\u0026rsquo; /home (/home/$USER/ondemand) where it keeps information about running and completed OOD jobs, shells, desktop sessions and such. Deleting the ondemand directory while a job or session is running would likely cause the job or session to fail.\nIt is better to leave the /home/$USER/ondemand directory alone! Transfer data # Internal links # Run applications with OOD External links # "},{"id":"20","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/containers/","permalinkTitle":"Homepage / Software and Applications / Containers for Software","title":"Containers for Software","content":" Introduction # Linux Containers are means to isolate software dependencies from the base Linux operating system. On Grex, we support the Singularity container system, now developed by a company called SyLabs. Several other Linux container engines exist, most notably Docker which is a very popular tool in DevOps community. Presently Docker containers cannot be directly supported on shared HPC systems like Grex. However, with help of Singularity, it is possible to run Docker images from DockerHub, as well as native Singularity images from other repositories, such as SingularityHub and SyLabsCloud.\nUsing Singularity on Grex # Start with module spider singularity; it will list the current version. Due to the nature of container runtime environments, we update Singularity regularly, so the installed version is usually the latest one. Load the module (in the default Grex environment) by the following command:\nmodule load singularity With singularity command, one can list singularity commands and their options:\nmodule load singularity singularity help A brief introduction on getting started with Singularity can be useful to get started. You will not need to install Singularity on Grex since it is already provided as a module.\nTo execute an application within the container, do it in the usual way for that application, but prefix the command with \u0026lsquo;\u0026lsquo;singularity exec image_name.sif\u0026quot;. For example, to run R on an R script, using a container named R-INLA.sif:\nsingularity exec ./R-INLA.sif R --vanilla \u0026lt; myscript.R Quite often, it is useful to provide the containerized application with data residing on a shared HPC filesystem such as /home or /global/scratch. This is done via bind mounts. Normally, the container bind-mounts $HOME, /tmp and the current working directory. On Grex to bind the global Lustre filesystem the following -B option should be used:\nsingularity exec -B /sbb/scratch:/global/scratch ./R-INLA.sif R --vanilla \u0026lt; myscript.R In case you do not want to mount anything to preserve the containers\u0026rsquo; environment from any overlapping of data/code from say $HOME, use the --containall flag.\nSome attention has to be paid to Singularity\u0026rsquo;s local cache and temporary directories. Singularity caches the container images it pulls and Docker layers under $HOME/.singularity. Containers can be large, in tens of gigabytes, and thus they can easily accumulate and exhaust the users\u0026rsquo; storage space quota on $HOME. Thus, users might want to set the SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR variables to some place under their /global/scratch space.\nFor example, to change the location of SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR, one might run:\nmkdir -p /global/scratch/$USER/singularity/{cache,tmp} export SINGULARITY_CACHEDIR=\u0026#34;/global/scratch/$USER/singularity/cache\u0026#34; export SINGULARITY_TMPDIR=\u0026#34;/global/scratch/$USER/singularity/tmp\u0026#34; before building the singularity image.\nGetting and building Singularity images # The commands singularity build and singularity pull would get Singularity images from DockerHub, SingularityHub or SyLabsCloud. Images can also be built from other images, and from recipes. A recipe is a text file that specifies the base image and post-install commands to be performed on it.\nSingularity with GPUs # Use the --nv flag to singularity run/exec/shell commands. Naturally, you should be on a node that has a GPU, in an interactive job. NVIDIA provides many pre-built Docker and Singularity container images on their \u0026ldquo;GPU cloud\u0026rdquo;, together with instructions on how to pull them and to run them. These should work on Grex without much changes.\nOpenScienceGrid CVMFS # We can run Singularity containers distributed with OSG CVMFS which is currently mounted on Grex\u0026rsquo;s CVMFS. The containers are distributed via CVMFS as unpacked directory images. So, the way to access them is to find a directory of interest and point singularity runtime to it. The directories will then be mounted and fetched automatically. The repository starts with /cvmfs/singularity.opensciencegrid.org/. Then you\u0026rsquo;d need an idea from somewhere what you are looking for in the subdirectories of the above-mentioned path. An example (accessing, that is, exploring via singularity shell command, Remoll software distributed through OSG CVMFS by jeffersonlab):\nmodule load singularity singularity shell /cvmfs/singularity.opensciencegrid.org/jeffersonlab/remoll\\:develop A partial description of what is present on OSG CVMFS is available here.\nExternal links # Singularity/Sylabs homepage Singularity documentation (The Alliance documentation) Westgrid Singularity tutorial, a recording can be found here Docker Hub Singularity Hub Sylabs Cloud NVIDIA NGC cloud OSG Helpdesk for Singularity "},{"id":"21","rootTitleIndex":"6","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/using-localdisks/","permalinkTitle":"Homepage / Running jobs on Grex / Using local disks: $SLURM_TMPDIR","title":"Using local disks: $SLURM_TMPDIR","content":" Introduction # "},{"id":"22","rootTitleIndex":"12","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2023/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2023","title":"Workshops - 2023","content":" Spring workshop, 2023 # Below are the slides from the Grex workshop that was held on May 17-19, 2023:\nUpdates on Research Computing Resources: Slides Basics of Linux Shell: Slides Beginner\u0026rsquo;s Introduction to Use HPC Machines: Slides Beginner\u0026rsquo;s HPC Software - Overview: Slides Using OpenOnDemand Web Portal on Grex - Live Demonstration: Slides Using HPC Clusters Efficiently (SLURM, Kinds of jobs): Slides Advanced HPC Software (Containers, CVMFS): Slides Using GPUs on HPC Systems - Live Demonstration: Slides Beginner’s Introduction for Using Cloud Computing (OpenStack): Slides Overview of Further Training and Materials Available: Slides "},{"id":"23","rootTitleIndex":"12","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2022/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2022","title":"Workshops - 2022","content":" Autumn workshop, Oct 2022 # Below are the slides from the Grex workshop that was held on Oct 26-27, 2022:\nProgram and updates: Slides High Performance Computing: Start Guide: Slides High Performance Computing: software environments: Slides OSC OpenOnDemand portal on Grex: Slides Spring workshop, May 2022 # Below are the slides from the Grex workshop that was held on May 4-5, 2022:\nIntroduction to local and National HPC at UManitoba: How to use available software and run jobs efficiently: Slides Introduction to High Performance Computing step by step: From getting an account to running jobs: Slides Using GP GPU compute on Grex: Slides High Performance Computing and software environments: Slides OSC OpenOnDemand portal on Grex: Slides "},{"id":"24","rootTitleIndex":"6","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/contributed-systems/","permalinkTitle":"Homepage / Running jobs on Grex / Scheduling policies and running jobs on Contributed nodes","title":"Scheduling policies and running jobs on Contributed nodes","content":" Scheduling policies for contributed systems # Grex has a few user-contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the \u0026ldquo;preferred access\u0026rdquo; is preemption.\nOn the definition of preferential access to HPC systems # Preferential access is when you have non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have. HPC makes access to CPU cores / GPUs / Memory exclusive per job, for the duration of the job (as opposed to time-sharing). Priority is a factor that decides which job gets to start (and thus exclude other jobs) first if there is a competitive situation (more jobs than free cores).\nThe owner is the owner of the contributed hardware. Others are other users. A partition is a subset of the HPC system’s compute nodes.\nPreemption by partition: the contributed nodes have a SLURM partition on them, allowing the owner to use them, normally, for batch or interactive jobs. The partition is a “preemptor”. There is an overlapping partition, on the same set of the nodes but for the others to use, which is “pre-emptible”. Jobs in the pre-emptible partition can be killed after a set “grace period” (1 hour) as the owner\u0026rsquo;s job enters the \u0026ldquo;preemptor\u0026rdquo; partition. If works, preempted jobs might be check-pointed rather than killed, but that’s harder to set up. Currently, it is not generally supported. If you have a code that supports checkpoint/restart at the application level, you can get most of the contributed nodes.\nOn Grex, the \u0026ldquo;preemptor\u0026rdquo; partition is named after the name of the owner PI, and the pre-emptible partitions named similarly but with added -b suffix. Use the --partition= option to submit the jobs with sbatch and salloc commands to select the desired partition.\nContributed Nodes # As of now, the pre-emptible partitions are:\nPartition Nodes GPUs/Node CPUs/Node Mem/Node Notes stamps-b 1 3 4 32 187 Gb AVX512 livi-b 2 1 16 48 1500 Gb NVSwitch server agro-b 3 2 2 24 250 Gb AMD Partition \u0026ldquo;stamps-b\u0026rdquo; # To submit a GPU job to stamps-b partition, please include the directive:\n#SBATCH --partition=stamps-b in your job script or submit your jobs using:\nsbatch --partition=stamps-b run-gpu-job.sh assuming that run-gpu-job.sh is the name of your job script.\nHere is an example of script for running LAMMPS job on this partition:\nScript example for running LAMMPS on **stamps-b** partition run-lmp-gpu.sh\r#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=stamps-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Partition \u0026ldquo;livi-b\u0026rdquo; # To submit a GPU job to livi-b partition, please include the directive:\n#SBATCH --partition=livi-b in your job script or submit your jobs using:\nsbatch --partition=livi-b run-gpu-job.sh assuming that run-gpu-job.sh is the name of your job script.\nHere is an example of script for running LAMMPS job on this partition:\nScript example for running LAMMPS on **livi-b** partition run-lmp-gpu.sh\r#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=livi-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Partition \u0026ldquo;agro-b\u0026rdquo; # To submit a GPU job to agro-b partition, please include the directive:\n#SBATCH --partition=agro-b in your job script or submit your jobs using:\nsbatch --partition=agro-b run-gpu-job.sh assuming that run-gpu-job.sh is the name of your job script.\nHere is an example of script for running LAMMPS job on this partition:\nScript example for running LAMMPS on **agro-b** partition run-lmp-gpu.sh\r#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=agro-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; stamps-b: GPU [4 - V100/16GB] nodes contributed by Prof. R. Stamps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nlivi-b: GPU [HGX-2 16xGPU V100/32GB] node contributed by Prof. L. Livi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nagro-b: GPU [AMD Zen] node contributed by Faculty of Agriculture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"25","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/cern-vmfs/","permalinkTitle":"Homepage / Software and Applications / CVMFS and the Alliance software stack","title":"CVMFS and the Alliance software stack","content":" Cern VMFS on Grex # CVMFS or CernVM stands for CernVM File System. It provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications.\nPresently, we use CernVMFS (CVMFS) to provide the Alliance\u0026rsquo;s (or Compute Canada\u0026rsquo;s) software stack. We plan to add more publically available CVMFS software repositories such as the one from OpenScienceGrid, in the near future. Note that we can only \u0026ldquo;pull\u0026rdquo; software from these repositories. To actually add or change software, datasets, \u0026hellip; etc., the respective organizations controlling CVMFS repositories should be contacted directly.\nAccess to the CVMFS should be transparent to the Grex users: no action is needed other than loading a software module or setting a path.\nGrex does not have a local CVMFS stratum server. All we do is to cache the software items as they get requested. Thus, there can be a delay associated with pulling a software item from Stratum 1 (Replica Server) for the first time. It usually does not matter for serial programs but parallel codes, that rely on simultaneous process spawning across many nodes, might cause timeout errors. Thus, it is probably a good idea to first access the codes in a small interactive job to warm up the Grex\u0026rsquo;s local CVMFS cache.\nThe Alliance\u0026rsquo;s software stack # The main reason for having CVMFS supported on Grex is to provide Grex users with the software environment as similar as possible with the environment existing on national Alliance\u0026rsquo;s HPC machines. On Grex, the module tree from Compute Canada software stack is not set as default, but has to be loaded with the following commands:\nmodule purge module load CCEnv After the above command, use module spider to search for any software that might be available in the CC software stack. Note that \u0026ldquo;default\u0026rdquo; environments (the StdEnv and nixpkgs modules of the CC stack) are not loaded automatically, unlike on Compute Canada general purpose (GP) clusters. Therefore, it is a good practice to load these modules right away after the CCEnv. The example below loads the Nix package layer that forms the base layer of CC software stack, and then one of the \u0026ldquo;standard environments\u0026rdquo;, in this case based on Intel 2018 and GCC 7.3 compilers, MKL and OpenMPI.\nThere is more than one StdEnv version to choose from.\nmodule load nixpkgs/16.09 module load StdEnv/2018.3 Note that there are several CPU architectures in the CC software stack. They differ in the CPU instruction set used by the compilers, to generate the binary code. The default for legacy systems like Grex is the lowest SSE3 architectures arch/sse3. It ensures that there is no failure on the legacy Grex nodes (which are of NEHALEM, SSE4.2 architecture) due to more recent instructions like AVX, AVX2 and AVX512 that were added by Intel afterwards.\nFor running on Contributed Nodes, that may be of much newer CPU generation, it is better to use the arch/avx512 module and setting RSNT_ARCH=avx512 environment variable in the job scripts.\nSome of the software items on CC software stack might assume certain environment variables set that are not present on Grex; one example is SLURM_TMPDIR. In case your script fails for this reason, the following line could be added to the job script:\nexport SLURM_TMPDIR=$TMPDIR While a majority of CC software stack is built using OpenMPI, some items might be based on IntelMPI. These will require following additional environment variables to be able to integrate with SLURM on Grex:\nexport I_MPI_PMI_LIBRARY=/opt/slurm/lib/libpmi.so export I_MPI_FABRICS_LIST=shm:dapl If a script assumes, or relies on using the mpiexec.hydra launcher, the later might have to be provided with -bootstrap slurm option.\nHow to find software on CC CVMFS # Compute Canada\u0026rsquo;s software building system automatically generates documentation for each item, which is available at the Available Software page. So, the first destination to look for a software item is probably to browse this page. Note that this page covers the default CPU architectures (AVX2, AVX512) of the National systems, and legacy architectures (SSE3, AVX) might not necessarily have each of the software versions and items compiled for them. It is possible to request such versions to be added.\nThe Lmod, module spider, command can be used on Grex to search for modules that are actually available. Note that the CCEnv software stack is not loaded by default; you would have to load it first to enable the spider command to search through the CC software stack:\nmodule purge module load CCEnv module spider mysoftware Then, when finding available software versions and their dependencies, module load command can be used, as described here\nHow to request software added to CC CVMFS # Compute Canada maintains and distributes the software stack as part of its mandate to maintain the National HPC systems. To request a software item installed, the requestor should be part of the Compute Canada system (that is, have an account in CCDB, which is also a prerequisite to have access to Grex. Any CC user can submit such a request to support@tech.alliancecan.ca and notify if a version for non-default CPU architecture such as SSE3 is also necessary to build.\nAn example, R code with dependencies from CC CVMFS stack # A real-world example of using R on Grex, with several dependencies required for the R packages.\nFor dynamic languages like R and Python, the Alliance (formerly known as Compute Canada) does not, in general, provide or manage pre-installed packages. Rather, users are expected to load the base R (Python, Perl, Julia) module and then proceed for the local installation of the required R (or Python, Perl, Julia etc.) packages in their home directories. Check the R documentation and Python documentation.\nScript example for running R using the Alliance\u0026#39;s software stack (CC cvmfs) run-r-cc-cvmfs.sh\r#!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-72:00:00 #SBATCH --job-name=\u0026#34;R-gdal-jags-bench\u0026#34; # Load the modules: module load CCEnv module load nixpkgs/16.09 gcc/5.4.0 module load r/3.5.2 jags/4.3.0 geos/3.6.1 gdal/2.2.1 export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; R --vanilla \u0026lt; Benchmark.R \u0026amp;\u0026gt; benchmark.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Notes on MPI based software from CC Stack # We recommend using a recent environment/toolchain that provides OpenMPI 3.1.x or later, which has a recent PMIx process management interface and supports UCX interconnect libraries that are used on Grex. Earlier versions of OpenMPI might or might not work. With OpenMPI 3.1.x or 4.0.x, srun command should be used in SLURM job scripts on Grex.\nBelow is an example of an MPI job (Intel benchmark) using the StdEnv/2018.3 toolchain (Intel 2018 / GCC 7.3.0 and OpenMPI 3.1.2).\nScript example for running MPI program using the Alliance\u0026#39;s software stack run-mpi-cc-cvmfs.sh\r#!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=2 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-1:00:00 #SBATCH --job-name=\u0026#34;IMB-MPI1-4\u0026#34; # Load the modules: module load CCEnv module load StdEnv/2018.3 module load imb/2019.3 module list echo \u0026#34;Starting run at: `date`\u0026#34; srun IMB-MPI1 \u0026gt; imb-ompi312-2x2.txt echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; If the script above is saved into imb.slurm, it can be submitted as follows:\nsbatch imb.slurm Notes on Code development with CC Stack # Because Compute Canada software stack can only distribute open source software to non-CC systems like Grex, proprietary/restricted software items are omitted. This means that Intel compiler modules, while providing their redistributable parts necessary to run the code compiled with them, will not work to compile new code on Grex. Thus, only GCC compilers and GCC-based toolchains from CC Stack are useful for the local code development on Grex.\nOpenScienceGrid # On Grex, we mount OSG repositories, mainly for Singularity containers provided through OSG. Pointing the singularity to the desired path under /cvmfs/singularity.opensciencegrid.org/ will automatically mount and fetch the required software items. Discovering them is up to the users. See more in our Containers documentation page.\n"},{"id":"26","rootTitleIndex":"4","rootTitle":"Connecting to Grex and Transferring data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex and Transferring data","permalink":"/grex-docs/connecting/data-transfer/","permalinkTitle":"Homepage / Connecting to Grex and Transferring data / Transferring data","title":"Transferring data","content":" Introduction # Globus Online file transfer # Unfortunately, the WestGrid Globus endpoint on Grex had expired. It is not possible to use Globus on Grex as of the time of writing this documentation. However, you can still use Globus to transfer data between the Allaince\u0026rsquo;s (Compute Canada) systems as described here.\nCheck the ESNet website if you are curious about Globus, and why large data transfers over WAN might need specialized networks and software setups.\nOpenSSH tools: scp, sftp # SFTP # On Mac OS and Linux, where OpenSSH client packages are always available, the following command line tools are present: scp, sftp. They work similar to UNIX cp and ftp commands, except that there is a remote target or source.\nSFTP opens a session and then drops the user to a command line, which provides commands like ls, lls, get, put, cd, lcd to navigate the local and remote directories, upload and download files etc.\nsftp someuser@grex.hpc.umanitoba.ca sftp\u0026gt; lls sftp\u0026gt; put myfile.fchk Please replace someuser with your username.\nSCP # SCP behaves like cp. To copy a file myfile.fchk to Grex, from the current directory, into his /global/scratch/, a user would run the following command:\nscp ./myfile.fchk someuser@grex.hpc.umanitoba.ca:/global/scratch/someuser Note that the destination is remote (for it has the form of user@host:/path). More information about file transfer tools exist on Compute Canada documentation\nLFTP tool # LFTP is a multi-protocol file transfer code for Linux, that supports some of the advanced features of Globus, enabling better bandwidth utilization through socket tuning and using multiple streams. On Grex (and between Grex and Compute Canada systems) only SFTP (that is, sftp:// URIs) is supported! So, the minimal syntax for opening a transfer session from Grex to Cedar would be (on Grex):\nlftp sftp://someuser@cedar.computecanada.ca It has a command line interface not unlike the ftp or sftp command line tools, with ls, get, and put commands.\nRSYNC # File transfer clients with GUI # There are many file transfer clients that provide convenient graphical user interface.\nSome examples of the popular file transfer clients are\nWinSCP for Windows. CyberDuck for MacOS X Cross platform FileZilla Client Other GUI clients will work with Grex too, as long as they provide SFTP protocol support.\nTo use such clients, one would need to tell them that SFTP is needed, and to provide the address, which is grex.hpc.umanitoba.ca and your Grex/Alliance username.\nNote that we advise against saving your password in the clients: first, it is less secure, and second, it is easy to store a wrong password. File transfer clients would try to auto-connect automatically, and having a wrong password stored with them will create many failed connection attempts from your client machine, which in turn might temporarily block your IP address from accessing Grex.\nFile transfers with OOD browser GUI # NEW: It is now possible to use OpenOnDemand on aurochs Web interface to download and upload data to and from Grex. Use Files dashboard menu to select a filesystem (currently /home/$USER and /global/scratch/$USER are available), and then Upload and Download buttons.\nThere is a limit of about 10GB to the file transfer sizes with OOD. The OOD interface is, as of now, open for UManitoba IP addresses only (i.e., machines from campus and on UM PulseVPN will work).\nMore information is available on our OOD pages\n"},{"id":"27","rootTitleIndex":"12","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2021/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2021","title":"Workshops - 2021","content":" Autumn workshop, November 2021 # Below are the slides from the Grex workshop that was held on Nov 01-02, 2021:\nIntroduction to HPC: Basics: Steps from getting an account to submitting jobs: Slides Linux (Unix) shell basics: Slides Software on HPC clusters: Slides Scheduling jobs on HPC clusters: How to optimize your jobs and get more from the resources available?: Slides Spring workshop, April 2021 # Below are the slides from the Grex workshop that was held on April 21-22, 2021:\nUpdates about local HPC resources\u0026quot;: Slides Introduction to HPC and available resources: UofM and Compute Canada: Slides Introduction to HPC: SLURM, best practices for Grex: Slides Introduction to Software on HPC clusters: Slides "},{"id":"28","rootTitleIndex":"5","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/data-sizes-and-quota/","permalinkTitle":"Homepage / Storage and Data / Data sizes and quotas","title":"Data sizes and quotas","content":" Data size and quotas # This section explains how to find the actual space and inode usage of your /home/ and /global/scratch allocations on Grex. We limit the size of the data and the number of files that can be stored on these filesystems.\nFile system Type Total space Quota per user /home NFSv4/RDMA 15 TB 100 GB /global/scratch Lustre 418 TB 4 TB /project Lustre 1 PB - To figure out where your current usage stands with the limit, POSIX quota or Lustre\u0026rsquo;s analog, lfs quota, commands can be used.\nNFS quota # The /home/ filesystem is served by NFSv1 and thus supports the standard POSIX quota command. For the current user, it is just:\nquota or\nquota -s The command will result in something like this (note the -s flag added to make units human readable):\n[someuser@bison ~]$ quota -s Disk quotas for user someuser (uid 12345): Filesystem space quota limit grace files quota limit grace 192.168.24.70:/ 249M 100G 105G 4953 500k 1000k The output is a self-explanatory table. There are two values: soft \u0026ldquo;quota\u0026rdquo; and hard \u0026ldquo;limit\u0026rdquo; per each of (space, files) quotas. If you are over soft quota, the value of used resource (space or files) will have a star * to it, and a grace countdown will be shown. Getting over grace period, or over the hard limit prevents you from writing new data or creating new files on the filesystem. If you are over quota on /home, it is time to do some clean up there, or migrate the data-heavy items to /global/scratch where they belong.\nCAVEAT: The Alliance (Compute Canada) breaks the POSIX standard by redefining the quota command in their software stack. So, after loading the CCEnv module on Grex, the quota command may return garbage. For accurate output about your quota, load GrexEnv first before running the command quota or use the command diskusage_report (see below). Lustre quota # The /global/scratch/ filesystem is actually a link to a (new) Lustre filesystem called /sbb/. We have retained the old name for compatibility with the old Lustre filesystem that was used on Grex between 2011 and 2017. Lustre filesystem provides a lfs quota sub-command that requires the name of the filesystem specified. So, for the current user, the command to get current usage {space and number of files}, in the human-readable units, would be as follows:\nlfs quota -h -u $USER /sbb With the output:\n[someuser@bison ~]$ lfs quota -h -u $USER /sbb Disk quotas for usr someuser (uid 12345): Filesystem used quota limit grace files quota limit grace /sbb 622G 2.644T 3.653T - 5070447 6000000 7000000 - Presently we do not enforce group or project quota on Grex.\nIf you are over quota on Lustre /global/scratch filesystem, just like for NFS, there will be a star to the value exceeding the limit, and the grace countdown will be active.\nA wrapper for quota # To make it easier, we have set a custom script with the same name as for the Alliance (Compute Canada) clusters, diskusage_report, that gives both /home and /global/scratch quotas (space and number of files: usage/quota or limits), as in the following example:\n[someuser@bison ~]$ diskusage_report Description (FS) Space (U/Q) # of files (U/Q) /home (someuser) 254M/104G 4953/500k /global/scratch (someuser) 131G/2147G 992k/1000k for more details1, run the command:\ndiskusage_report -dd -vv as in the example:\n[someuser@bison ~]$ diskusage_report -dd -vv + Space and Inode quotas for user: someuser + Date: Thu Feb 24, 2022 Description (FS) Space (U/Q/L) # of files(U/Q/L) /home (someuser) 254M/104G/110G 4953/500k/1000k /global/scratch (someuser) 131G/2147G/3221G 992k/1000k/1100k FS ==\u0026gt; File System (/home, /global/scratch) U ==\u0026gt; Current Usage (Space, Inode) Q ==\u0026gt; Soft Quota (Space, Inode) L ==\u0026gt; Hard Quota (Space, Inode) These options are not implemented on the Alliance clusters.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"29","rootTitleIndex":"1","rootTitle":"Grex: High Performance Computing Cluster at University of Manitoba","rootTitleIcon":"fa-solid fa-user-gear fa-lg","rootTitlePath":"/grex-docs/grex/","rootTitleTitle":"Homepage / Grex: High Performance Computing Cluster at University of Manitoba","permalink":"/grex-docs/grex/","permalinkTitle":"Homepage / Grex: High Performance Computing Cluster at University of Manitoba","title":"Grex: High Performance Computing Cluster at University of Manitoba","content":" Introduction # Grex is a UManitoba High Performance Computing (HPC) system, first put in production in early 2011 as part of WestGrid consortium. \u0026ldquo;Grex\u0026rdquo; is a Latin name for \u0026ldquo;herd\u0026rdquo; (or maybe \u0026ldquo;flock\u0026rdquo;?). The names of the Grex login nodes ( bison, tatanka, aurochs, yak) also refer to various kinds of bovine animals.\nSince being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators.\nIf you are a new Grex user, proceed to the quick start guide and documentation right away.\nHardware # The original Grex was an SGI Altix machine, with 312 compute nodes (Xeon 5560, 12 CPU cores and 48 GB of RAM per node) and QDR 40 Gb/s InfiniBand network. In 2017, a new Seagate Storage Building Blocks (SBB) based Lustre filesystem of 418 TB of useful space was added to Grex. In 2020 and 2021, the University added several modern Intel CascadeLake CPU nodes, a few GPU nodes, a new NVME storage for home directories, and EDR InfiniBand interconnect. On March 2023, a new storage of 1 PB was added to Grex. It is called /project filesystem. The current computing hardware available for general use is as follow:\nLogin nodes # As of Sep 14, 2022, Grex is using UManitoba network. We have decommissioned the old WG and BCNET network that was used for about 11 years. Now, the DNS names use hpc.umanitoba.ca instead of the previous name westgrid.ca\nOn Grex, there are multiple login nodes:\nBison: bison.hpc.umanitoba.ca Tatanka: tatanka.hpc.umanitoba.ca Grex: grex.hpc.umanitoba.ca Yak: yak.hpc.umanitoba.ca (please note that the architecture for this node is avx512). Aurochs: https://aurochs.hpc.umanitoba.ca (only used for OOD and requires VPN if used outside campus network). To login to Grex in the text (bash) mode, connect to grex.hpc.umanitoba.ca using a secure shell client, SSH.\nThe DNS name grex.hpc.umanitoba.ca serves as an alias for two login nodes: bison.hpc.umanitoba.ca and tatanka.hpc.umanitoba.ca .\nIt is also possible to connect via yak.hpc.umanitoba.ca\nCPU nodes # In addition to the original nodes, new skylake nodes have been added to Grex:\nHardware Number of nodes CPUs/Node Mem/Node Network Intel CPU 12 40 384 GB EDR 100GB/s IB interconnect Intel 6230R 42 52 96 GB EDR 100GB/s IB interconnect Intel Xeon 55601 312 12 48 GB QDR 40GB/s IB interconnect Contributed2 4 20 32 GB - GPU nodes # There are also several researcher-contributed nodes (CPU and GPU) to Grex which make it a \u0026ldquo;community cluster\u0026rdquo;. The researcher-contributed nodes are available for others on opportunistic basis; the owner groups will preempt the others\u0026rsquo; workloads.\nHardware Number of nodes GPUs/Node CPUs/node Mem/Node GPU 2 4 32 192 GB 4 [V100-32 GB]3 2 4 32 187 GB 4 [V100-16 GB]4 3 4 32 187 GB 16 [V100-32 GB]5 1 16 48 1500 GB AMD [A30]6 2 2 18 500 GB Storage # Grex\u0026rsquo;s compute nodes have access to three filesystems:\nFile system Type Total space Quota per user /home NFSv4/RDMA 15 TB 100 GB /global/scratch Lustre 418 TB 4 TB /project Lustre 1 PB Allocated per group. In addition to the shared file system, the compute nodes have their own local disks that can be used as temporary storage when running jobs.\nSoftware # Grex is a traditional HPC machine, running CentOS Linux under SLURM resource management system. On Grex, we use different software stacks.\nWeb portals and GUI # In addition to the traditional bash mode (connecting via ssh), users have access to:\nOpenOnDemand: on Grex, it is possible to use OpenOnDemand (OOD for short) to login to Grex and run batch or GUI applications (VNC Desktops, Matlab, Gaussview, Jupyter, \u0026hellip;). For more information, please refer to the page: OpenOnDemand X2Go: for more information, visit the page, connect to Grex via X2Go Useful links # Digital Research Alliance of Canada (the Alliance), formerly known as Compute Canada. the Alliance documentation Grex Status page Grex documentation Local Resources at UManitoba WestGrid ceased its operations since April 1st, 2022. The former WestGrid institutions are now re-organized into two consortia: BC DRI group and Prairies DRI group. Original Grex nodes: slated for decommission in the near furure\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nContributed nodes.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes available for all users (general purpose).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes contributed by Prof. R. Stamps (Department of Physics and Astronomy).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVSwitch server contributed by Prof. L. Livi (Department of Computer Science).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes contributed by Faculty of Agriculture.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"30","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/jupyter-notebook/","permalinkTitle":"Homepage / Software and Applications / How to use jupyter notebooks on Grex?","title":"How to use jupyter notebooks on Grex?","content":" Jupyter on Grex # Jupyter is a Web-interface aimed to support interactive data science and scientific computing. Jupyter supports several dynamic languages, most notably Python, R and Julia. Jupyter offers a metaphor of \u0026ldquo;computational document\u0026rdquo; that combines code, data and visualizations, and can be published or shared with collaborators.\nJupyter can be used either as a simple, individual notebook or as a multi-user Web server/Interactive Development Environment (IDE), such as JupyterHub/JupyterLab. The JupyterHub servers can use a variety of computational back-end configurations: from free-for-all shared workstation to job spawning interfaces to HPC schedulers like SLURM or container workflow systems like Kubernetes.\nThis page lists examples of several ways of accessing jupyter.\nUsing notebooks via SSH tunnel and interactive jobs # Any Python installation that has jupyter notebooks installed can be used for the simple notebook interface. Most often, activity on login nodes of HPC systems is limited, so first an interactive batch job should be started. Then, in the interactive job, users would start a jupyter notebook server and use an SSH tunnel to connect to it from a local Web browser.\nAfter logging on to Grex as usual, issue the following salloc command to start an interactive job:\nsalloc --partition=compute --nodes=1 --ntasks-per-node=2 --time=0-3:00:00 It should give you a command prompt on a compute node. You may change some parameters like partition, time, \u0026hellip; etc to fit your needs. Then, make sure that a Python module is loaded and jupyter is installed, either in the Python or in a virtualenv, let\u0026rsquo;s start a notebook server, using an arbitrary port 8765. If the port is already in use, pick another number.\njupyter-notebook --ip 0.0.0.0 --no-browser --port 8765 If successful, there should be:\nhttp://g333:8675/?token=ae348acfa68edec1001bcef58c9abb402e5c7dd2d8c0a0c9\nor similar, where g333 refers to a compute node it runs, 8675 is a local TCP port and token is an access token.\nNow we have a jupyter notebook server running on the compute node, but how do we access it from our own browser? To that end, we will need an SSH tunnel.\nAssuming a command line SSH client (OpenSSH or MobaXterm command line window), in a new tab or terminal issue the following:\nssh -fNL 8765:g333:8765 youruser@bison.hpc.umanitoba.ca Agan, g333, port 8765 and your user name in the example above should be changed to reflect the actual node and user.\nWhen successful, the SSH command above returns nothing. Keep the terminal window open for as long as you need the tunnel. Now, the final step is to point your browser (Firefox is the best as Chrome might refuse to do plain http://) to the specified port on localhost or 127.0.0.1, as in http://localhost:8765 or http://127.0.0.1:8765 . Use the token as per above to authenticate into the jupyter notebook session, either copying it into the prompt or providing it in the browser address line.\nThe notebook session will be usable for as long as the interactive (salloc) job is valid and both salloc session and the SSH tunnel connections stay alive. This usually is a limitation on how long jupyter notebook calculations can be, in practice.\nThe above-mentioned method will work not only on Grex, but on Compute Canada systems as well.\nUsing notebooks via Grex OOD Web Portal # Grex provides a jupyter server as an OpenOnDemand dashboard application. This is much more convenient than handling SSH tunnels manually. The servers will run as a batch job on Grex compute nodes, so as usual, a choice of SLURM partition will be needed.\nPresently, jupyter for these apps uses an installation of Grex\u0026rsquo;s Python 3.7 module. There are two versions of the app, one for the GCC 7.4 toolchain, another for Intel the 2019.5 toolchain.\nOpenOndemand applications: jupyter Find out more on how to use OOD on the Grex OOD pages\nTo use R, Julia, and different instances or versions of Python, a jupyter notebook kernel needs to be installed by each user in their home directories. Check out the corresponding Compute Canada documentation here .\nOther jupyter instances around # There is a SyZyGy instance umanitoba.syzygy.ca that gives a free-for-all shared JupyterHub for UManitoba users.\nMost of the Alliance\u0026rsquo;s (Compute Canada) HPC machines deployed JupyterHub interfaces: Cedar, Beluga, Narval and Niagara.\n"},{"id":"31","rootTitleIndex":"2","rootTitle":"Quick Start Guide","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/start-guide/","rootTitleTitle":"Homepage / Quick Start Guide","permalink":"/grex-docs/start-guide/","permalinkTitle":"Homepage / Quick Start Guide","title":"Quick Start Guide","content":" Grex # Grex is an UManitoba High Performance Computing (HPC) system, first put in production in early 2011 as part of WestGrid consortium. Now it is owned and operated by the University of Manitoba. Grex is accessible only for UManitoba users and their collaborators.\nA Very Quick Start guide # Create an account on CCDB. You will need an institutional Email address. If you are a sponsored user, you\u0026rsquo;d want to ask your PI for his/her CCRI code {Compute Canada Role Identifier}. For a detailed procedure, visit the page Apply for an account.\nWait for half a day. While waiting, install an SSH client, and SFTP client for your operating system.\nConnect to grex.hpc.umanitoba.ca with SSH, using your username/password from step 1.\nMake a sample job script, call it, for example, sleep-job.sh . The job script is a text file that has a special syntax to be recognized by SLURM. You can use the editor nano , or any other right on the Grex SSH prompt (vim, emacs, pico, \u0026hellip; etc); you can also create the script file on your machine and upload to Grex using your SFTP client or scp.\nScript example for a test job sleep-job.sh\r#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=500M #SBATCH --time=00:01 echo \u0026#34;Starting run at: `date`\u0026#34; echo \u0026#34;Hello world! will sleep for 10 seconds\u0026#34; time sleep 10 echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Submit the script using sbatch command, to the compute partition using: sbatch --partition=compute sleep-job.sh Wait until the job finishes; you can monitor the queues state with the \u0026lsquo;sq\u0026rsquo; command. When the job finishes, a file slurm-NNNN.out should be created in the same directory.\nDownload the output slurm-NNNN.out from grex.hpc.umanitoba.ca to your local machine using your SFTP client.\nCongratulations, you have just ran your first HPC-style batch job. This is the general workflow, more or less; you\u0026rsquo;d just want to substitute the sleep command to something useful, like ./your-code.x your-input.dat .\n"},{"id":"32","rootTitleIndex":"3","rootTitle":"Access and Usage Conditions","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/access/","rootTitleTitle":"Homepage / Access and Usage Conditions","permalink":"/grex-docs/access/","permalinkTitle":"Homepage / Access and Usage Conditions","title":"Access and Usage Conditions","content":" Access Conditions # Grex is open to all researchers at University of Manitoba and their collaborators. The main purpose of the Grex system is Research; it might be used for grad studies courses with a strong research component, for their course-based research.\nThe access and job accounting is by research group; that is, the Principal Investigator (PI)\u0026rsquo;s accounting group gets resource usage of their group members accounted for. Grex\u0026rsquo;s resources (CPU and GPU time, disk space, software licenses) are automatically managed by a batch scheduler, SLURM, according to the University\u0026rsquo;s priorities. There is a process of resource allocation competition (RAC) to get an increased share of Grex resources; however, a \u0026ldquo;default\u0026rdquo; share of the resources is available immediately and free of charge by getting an account.\nIt is expected that Grex accounts and resource allocations are used for the research projects they are requested for.\nOwners of the user-contributed hardware on Grex have preferential access to their hardware, which can only be used by the general community of UM researchers when idle and not reserved.\nGetting an account on Grex # As of the moment, Grex is using the Compute Canada (now the Alliance) account management database ( CCDB). Any eligible Canadian Faculty member can get an Alliance account in the CCDB system. If you are a graduate student, doctoral student, postdoctoral fellow, research assistant, undergraduate student, or a non-research staff member, visiting faculty or external collaborator, you will need to be sponsored in CCDB by a Principal Investigator (PI), i.e. a Faculty member. The PI must register in the CCDB first, and then he/she can sponsor you as a \u0026ldquo;Group Member\u0026rdquo; under his/her account. Once your application for an Alliance account has been approved, you will receive a confirmation email and you can start using the computing and data storage facilities.\nThere are two technical conditions for getting access:\nAn active CCDB account. An active CCDB \u0026ldquo;role\u0026rdquo; affiliated with UManitoba (University of Manitoba). Guidelines of the Acceptable Use # Grex adheres to the Alliance\u0026rsquo;s Privacy and Security policy, and to University of Manitoba IT Security and Privacy policies\nUsers that have Grex account have accepted both.\nIn particular, user\u0026rsquo;s accounts cannot and should not be shared with anyone for whatever reason. Our support staff will never ask for your password. Sharing any account information (login/password or SSH private keys) leads to immediate blocking of the account. UNIX groups and shared project spaces can be used for data sharing. Usage is monitored and statistics are collected automatically, to estimate the researcher\u0026rsquo;s needs and future planning, and to troubleshoot day to day operational issues. Users of Grex should be \u0026ldquo;fair and considerate\u0026rdquo; in their usage of the systems, trying not to allow for unnecessary and/or inefficient use of the resources or interfering with other users\u0026rsquo; work. We reserve to ourselves the right to monitor for inefficient use and ask users to stop their activities if they threaten the general stability of the Grex system. Getting a Resource Allocation on Grex # Similarly, to the Alliance (formerly known as Compute Canada), there is a two-tier system for resource allocation on Grex. Every group gets a \u0026ldquo;Default\u0026rdquo; allocation of Grex resources (computing time and storage space). Groups that need a larger fraction of resources and use Grex intensively, might want to apply for a local Resource Allocation Competition (Grex-RAC).\nGrex\u0026rsquo;s local RAC calls are issued once a year. They are reviewed by a local Advanced Research Computing Committee and may be scaled according to the availability of resources. The instructions and conditions of this year\u0026rsquo;s RAC are provided on the RAC template document (sent via email when the RAC is announced).\n"},{"id":"33","rootTitleIndex":"4","rootTitle":"Connecting to Grex and Transferring data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex and Transferring data","permalink":"/grex-docs/connecting/","permalinkTitle":"Homepage / Connecting to Grex and Transferring data","title":"Connecting to Grex and Transferring data","content":" Connecting to Grex # In order to use almost any HPC system, you would need to be able to somehow connect and log in to it. Also, it would be necessary to be able to transfer data to and from the system. The standard means for these tasks are provided by the SSH protocol.\nTo log in to Grex in the text (or bash) mode, connect to grex.hpc.umanitoba.ca using an SSH (Secure SHELL) client. The DNS name grex.hpc.umanitoba.ca serves as an alias for two login nodes: bison.hpc.umanitoba.ca and tatanka.hpc.umanitoba.ca .\nssh someuser@grex.hpc.umanitoba.ca Since early 2021, a new login node, yak.hpc.umanitoba.ca is available to access and build software that uses new Intel AVX2, AVX512 CPU instructions. Yak is not part of the grex.hpc.umanitoba.ca alias.\nssh someuser@yak.hpc.umanitoba.ca Please remember to replace someuser with your Alliance user name. Transferring Data # Uploading and downloading your data can be done using an SCP/SFTP capable file transfer client. The recommended clients are OpenSSH (providing ssh and scp, sftp command line tools on Linux and Mac OS X) and PuTTY/WinSCP/X-Ming or MobaXterm under Windows. Note that since June 1, 2014, the original \u0026ldquo;SSH Secure Shell\u0026rdquo; Windows SSH/SFTP client is not supported anymore.\nX2Go # Since Dec. 2015, support has been provided for the graphical mode connection to Grex using X2Go.\nX2Go remote desktop clients are available for Windows, Mac OS X and Windows. When creating a new session, please choose either of the supported desktop environments: \u0026ldquo;OPENBOX\u0026rdquo; or \u0026ldquo;ICEWM\u0026rdquo; in the \u0026ldquo;Session type\u0026rdquo; menu. The same login/password should be used as for SSH text based connections.\nOpenOnDemand (OOD) Web Interface # Since October 2021, there is an OpenOnDemand (OOD) Web interface to Grex, available at https://aurochs.hpc.umanitoba.ca from UManitoba IP addresses. OOD provides a way to connect both in text and graphical mode right in the browser, to transfer data between Grex local machines, and to run jobs.\nSee the documentation for more details on how to connect from various clients of operating systems: SSH, X2Go, OOD.\nInternal links # Connecting with SSH\nConnecting with X2Go\nConnect with OOD\nData transfer\n"},{"id":"34","rootTitleIndex":"5","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/data-sharing/","permalinkTitle":"Homepage / Storage and Data / Data sharing","title":"Data sharing","content":" Data sharing # Sharing of accounts login information (like passwords or SSH keys) is strictly forbidden on Grex, as well as on most of the HPC systems. There is a mechanism of data/file sharing that does not require sharing of the accounts. To access each other\u0026rsquo;s data on Grex, the UNIX groups and permissions mechanism can be used as explained below.\nUNIX groups # Each UNIX (or Linux) file or directory is owned by an individual user and also by a group (which may be composed of several users). The permission to access files and directories can be restricted to just the individual owning the file, to the group that owns the file, or access can be unrestricted.\nBy default, each account (username) is set up with an associated UNIX group containing just that single username. So, even if you have set permission for your UNIX group to access files, they are still not being shared with anyone else. You can override the default by using the chmod command to set unrestricted read access to your files. However, if you need more specific control over access, you can ask us to create a special UNIX group containing the usernames of other researchers with whom you want to share data by sending an email to support (support@tech.alliancecan.ca) to ask that a new UNIX group be created for you. Include a list of the users who should be added to that group. One user should be designated as the authority for the group. If a request to join the group is made from someone else, we will ask the designated authority for the permission to add the new researcher to the group. The group name must be of the format wg-xxxxx where xxxxx represents up to five characters. Please indicate your preference for a group name in your email.\nThe group will be set up on the Grex system. This may take a day or two to set up. You will get an email whenever you are added or removed from a UNIX group.\nNow that you have a wg-xxxxx UNIX group created, you can set up the data sharing with it, by setting the permissions as described below.\nThe directory you wish to share should be owned by the group and permitted to the group. For example:\nchgrp -R wg-group dir chmod g+s dir You must ensure that there is access to parent directories as well.\nA directory and all the files in it can be permitted to the group as follows:\nchmod -R g+rX /global/scratch/dirname To set access for the /global/scratch/dirname directory and all its subdirectories. Note the uppercase X in the command. This will set x permissions on the subdirectories (needed for others to list the directories) as well as regular execute permission on executable files.\nIf you want them to allow other members to not only read files in the shared directory \u0026ldquo;dir\u0026rdquo;, but also permit write access to allow them to create and change files in that directory, then all members in the group must add a line:\numask 007\nto the ~/.bashrc or ~/.cshrc file in their respective home directories. Furthermore, you must add write permission to the shared directory itself:\nchmod -R g+rwX dir which would allow read and write access to the directory dir and all its files and subdirectories.\nLinux ACLs # On the Lustre filesystem (/global/scratch/$USER), it is possible to use Linux access control lists (ACLs) which offer more fine-grained control over access than UNIX groups. Compute Canada\u0026rsquo;s Sharing Data documentation might be relevant, with one caution: on Grex, there is no project layout as exists on Compute Canada clusters.\nAn example setting of ACL command, to allow for \u0026ldquo;search\u0026rdquo; access of the top directory to a group wg-abcdf, presumably with some of the directories under it being shared by a UNIX group:\nsetfacl -m g:wg-abcdf:X /global/scratch/$USER External links # Linux permissions\nACLs\n"},{"id":"35","rootTitleIndex":"5","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/","permalinkTitle":"Homepage / Storage and Data","title":"Storage and Data","content":" General Storage Information # As of now, the storage system of Grex consists of the following:\nThe /home NFSv4/RDMA filesystem is served by a very fast NVME disk server. The total size of the filesystem is 15 TB. The quota per-user is 100 GB of space and 500K files.\nThe /global/scratch Lustre filesystem, Seagate SBB, total usable size of 418 TB. It is intended to be used as the high-performance, scalable workspace for active projects. It is not backed up and is not intended for long-time storage of users\u0026rsquo; data that is not actively used. The default quota is 4 TB of space and 1 M files per user and can be increased on request to 10 TB per research group. Larger disk space requires a local RAC application.\nThe /project Lustre filesystem: ias of Sep 2022, an additional storage of 1 PB was added to Grex. There is no backup and it is allocated per group.\nFile system Type Total space Quota/User Number of files /home NFSv4/RDMA 15 TB 100 GB 500 K /global/scratch Lustre 418 TB 4 TB 1 M /project Lustre 1 PB - - The local node storage as defined by the environment variable $TMPDIR is recommended for temporary job data that is not needed after job completes. Grex nodes have SATA local disks of various capacities, leaving 150 Gb, 400 Gb, 800 Gb and 1700 Gb usable space per node, depending on the kind of local disk it has. Most users would want to use /home/USERNAME for code development, source code, scripts, visualization, processed data, \u0026hellip; etc., that do not take much space and benefits for small files I/O. For production data processing, that is, massive I/O tasks from many compute or interactive jobs, /global/scratch/USERNAME should be used. It is often beneficial to place temporary files on the local disk space of the compute nodes, if space permits, so that the jobs do not load Lustre or NFS servers extensively.\nData retention and Backup # Data retention policy as of now conforms to the corresponding of the Alliance (former Compute Canada) policy. Namely, the data will not be kept on Grex indefinitely after the user\u0026rsquo;s account has expired. The data retention period for expired accounts is 1 year. Note that we do not do regular, short term data purges for /global/scratch filesystem. However, data on login and compute nodes local scratch gets purged regularly and automatically.\nBackup: after migration to new /home, the backup temporarily lapsed. There is no backup on any of the Grex filesystems now. We are working on resuming the backup. Data sharing # Sharing of accounts login information (like passwords or SSH keys) is strictly forbidden on Grex, as well as on most of the HPC systems. There is a mechanism of data/file sharing that does not require sharing of the accounts. To access each other\u0026rsquo;s data on Grex, the UNIX groups and permissions mechanism can be used. For more information, please refer to the section sharing data.\nDisclaimer of any responsibility for Data loss # Every effort is made to design and maintain Grex storage systems in a way that they are a reliable storage for researcher\u0026rsquo;s data. However, we (Grex Team, or the University) make no guarantees that any data can be recovered, regardless of where they are stored, even in backed up volumes. Accidents happen, whether caused by human error, hardware or software errors, natural disasters, or any other reason. It is the user\u0026rsquo;s responsibility that the data is protected against possible risks, as appropriate to the value of the data.\nInternal links # Data sizes and Quotas\nData sharing\n"},{"id":"36","rootTitleIndex":"6","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/","permalinkTitle":"Homepage / Running jobs on Grex","title":"Running jobs on Grex","content":" Why running jobs in batch mode? # There are many reasons for adopting a batch mode for running jobs on a cluster. From providing user\u0026rsquo;s computations with fairness, traffic control to prevent resource congestion and resource trashing, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the SLURM batch system since December 2019 with the Linux/SLURM update project.\nAccounting groups # Users belong to \u0026ldquo;accounting groups\u0026rdquo;, led by their Principal Investigators (PIs). The accounting groups on Grex match CCDB roles. By default, a user\u0026rsquo;s jobs are assigned to his primary/default accounting group. But it is possible for a user to belong to more than one accounting group; then the SLURM directive --account= can be used for sbatch or salloc to select a non-default account to run jobs under.\nFor example, a user who belongs to two accounting groups: def-sponsor1 and def-sponsor2 (sponsored by two PIs), can specify which one to use:\n#SBATCH --account=def-sponsor1 or\n#SBATCH --account=def-sponsor2 QOSs # QOS stands for Quality of Service. It is a mechanism to modify scheduler\u0026rsquo;s limits, hardware access policies and modify job priorities and job accounting/billing. Presently, QOS might be used by our Scheduler machinery internally, but not specified by the users. Jobs that specify explicit --qos= will be rejected by the SLURM job submission wrapper.\nLimits and policies # In order to prevent monopolization of the entire cluster by a single user or single accounting group, we enforce a MAXPS like limit; for non-RAC accounts it is set to 4 M CPU-minutes and 400 CPU cores. Accounts that have allocation (RAC) on Grex get higher MAXPS and max CPU cores limits in order to let them utilize their usage targets.\nTo see these limits, one could run the command:\nsacctmgr show assoc where user=$USER or with a specific format for the output:\nexport SACCTMGR_FORMAT=\u0026#34;cluster%9,user%12,account%20,share%5,qos%24,maxjobs%15,grptres%12,grptresrunmin%20\u0026#34; sacctmgr show assoc where user=$USER format=$SACCTMGR_FORMAT Partitions for pre-emptible jobs running on contributed hardware might be further limited, so that they cannot occupy the whole contributed hardware.\nIn cases when Grex is underutilized, but some jobs exist in the queue that can be run if not for the above-mentioned limits, we might relax the limits as a temporary \u0026ldquo;bonus\u0026rdquo;.\nSLURM commands # Naturally, SLURM provides a command line user interface. Some of the most useful commands are listed below.\nExploring the system # The SLURM command that shows the state of nodes and partitions is sinfo:\nExamples:\nsinfo to list the state of all the nodes (idle, down, allocated, mixed) and partitions. sinfo --state=idle to list all idle nodes. sinfo -p compute to list information about a given partition (compute in this case). sinfo -p skylake \u0026ndash;state=idle to list idle nodes on a given partition (skylake in this case). sinfo -R: to list all down nodes. sinfo -R -N -o\u0026quot;%.12N %15T [ %50E ]\u0026quot;|uniq -c: to list all down nodes and print the output in a specific format. sinfo -s \u0026ndash;format=\u0026quot;# %20P %12A %.12l %.11L %.6a %.22C %.10m\u0026quot; to show all the partitions and their characteristics. Submitting jobs # Batch jobs are submitted as follow:\nsbatch [options] myfile.job Interactive jobs are submitted in exactly same way, but they do not need the job script because they will give you an interactive session:\nsalloc [options] For more information about the usage of sbatch and salloc commands, please visit the dedicated sections: interactive and batch jobs.\nThe command sbatch returns a number called JobID and used by SLURM to identify the job in the queuing system.\nThe options are used to specify resources (wall time, tractable resources such as cores and nodes and GPUs) and accounts and QOS and partitions under which the jobs should run, and various other options like specifying whether jobs need X11 GUI (--x11), where its output should go, whether email should be sent when job changes its states and so on. Here is a list of the most frequent options to sbatch command:\nResources (tractable resources in SLURM speak) are CPU time, memory, and GPU time.\nGeneric resources can be software licenses, etc. There are also options to control job placement such as partitions and QOSs.\n--ntasks= (specifies number of tasks (MPI processes) per job) --nodes= (specifies number of nodes (servers) per job) --ntasks-per-node= (specifies number of tasks (MPI processes) per node) --cpus-per-task= (specifies number of threads per task) --mem-per-cpu= (specifies memory per task (or thread?)) --mem= (specifies the memory per node) --gpus= (specifies number of GPUs per job. There are also --gpus-per-XXX and --XXX-per-gpu) --time- (specifies wall time in format DD-HH:MM) --qos= (specifies a QOS by its name (Should not be used on Grex!)) --partition= (specifies a partition by its name (Can be very useful on Grex!)) An example of using some of these options with sbatch and salloc are listed below:\nsbatch --nodes=1 --ntasks-per-node=1 --cpus-per-task=12 --mem=40G --time=0-48:00 gaussian.job and\nsalloc --nodes=1 --ntasks-per-node=4 --mem-per-cpu=4000M --x11 --partition=compute And so on. The options for batch jobs can be either in command line, or (perhaps better) in the special comments in the job file, like:\n#SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=12 #SBATCH --mem=40G #SBATCH --time=0-48:00 #SBATCH --partition=compute Refer to the subsection for batch jobs and interactive jobs for more information, examples of job scripts and how to actually submit jobs.\nMonitoring jobs # Checking on the queued jobs:\nsqueue -u someuser (to see all queued jobs of the user someuser) squeue -u someuser -t R (to see all queued and running jobs of the user someuser) squeue -u someuser -t PD (to see all queued and pending jobs of the user someuser) squeue -A def-sponsor1 (to see all queued jobs for the accounting group def-sponsor1) Without the above parameters, squeue would return all the jobs in the system. There is a shortcut sq for squeue -u $USER\nCancelling jobs:\nscancel JobID (to cancel a job JobID) echo \u0026ldquo;Deleting all the jobs by $USER\u0026rdquo; \u0026amp;\u0026amp; scancel -u $USER (to cancel all your queued jobs at once). echo \u0026ldquo;Deleting all the pending jobs by $USER\u0026rdquo; \u0026amp;\u0026amp; scancel -u $USER \u0026ndash;state=pending (to cancel all your pending jobs at once). Hold and release queued jobs:\nTo put hold some one or more jobs, use:\nscontrol hold JobID (to put on hold the job JobID). scontrol hold JobID01,JobID02,JobID03 (to put on hold the jobs JobID01,JobID02,JobID03). To release them, use:\nscontrol release JobID (to release the job JobID). scontrol release JobID01,JobID02,JobID03 (to release the jobs JobID01,JobID02,JobID03). Checking job efficiency:\nThe command seff is a wrapper around the command sacct that gives a friendly output, like the actual utilization of walltime and memory:\nseff JobID seff -d JobID Note that the output from the seff command is not accurate if the job was not successful.\nChecking resource limits and usage for past and current jobs:\nsacct -j {JobID} -l sacct -u $USER -s {STARTDATE} -e {ENDDATE} -l --parsable2 Getting info on accounts and priorities # Fairshare and accounting information for the accounting group def-someprofessor:\nsshare -l -A def-someprofessor sshare -l -A def-someprofessor --format=\u0026#34;Account,EffectvUsage,LevelFS\u0026#34; sshare -a -l -A def-someprofessor sshare -a -l -A def-someprofessor --format=\u0026#34;Account,User,EffectvUsage,LevelFS\u0026#34; Fairshare and accounting information for a user:\nsshare -l -U --user $USER sshare -l -U --user $USER --format=\u0026#34;Cluster,User,EffectvUsage,FairShare,LevelFS\u0026#34; Limits and settings for an account:\nsacctmgr list assoc account=def-someprofessor format=account,user,qos Internal links # Slurm partitions\nInteractive jobs\nBatch jobs\nUsing local disks\nContributed nodes\nExternal links # SLURM documentation Running jobs on the Alliance (Compute Canada) clusters. References for migrating from PBS to SLURM: ICHEC, HPC-USC Westgrid training materials on SLURM: Scheduling Since the HPC technology is widely used by most universities and National labs, simply googling your SLURM question will likely return a few useful links to their HPC/ARC documentation. Please remember to adapt your script according to the available hardware and software as some SLURM directives and modules are specific to a given cluster. "},{"id":"37","rootTitleIndex":"7","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/","permalinkTitle":"Homepage / Software and Applications","title":"Software and Applications","content":" Software # In the HPC world, software is more often meant as codes that do some scientific or engineering computation, data processing and visualization (as opposed to web services, relational databases, client-server business systems, email and office, \u0026hellip; etc.)\nTools and libraries used to develop HPC software are also software, and have several best practices associated with them. Some of that will be covered below. Without means to provide software to do computations, HPC systems would be rather useless.\nFortunately, most HPC systems, and Grex is no exception, come with a pre-installed, curated software stack. This section covers how to find the installed software, how to access it, and what options you have if some of the software you need is missing.\nHow software is installed and distributed # There are several mechanisms of software installation under Linux. One of them is using Linux software package manager (apt on Ubuntu, yum on Centos, \u0026hellip; etc.) and a binary package repository provided by some third party. These package managers would install a version of the code system wide, into standard OS directories like /usr/bin where it would be immediately available in the system\u0026rsquo;s PATH (PATH is a variable that specifies where the operating systems would look for executable code).\nThis method of installation is often practiced on person\u0026rsquo;s own workstations, because it requires no knowledge other than syntax of the OS package manager. There are however significant drawbacks to it for using on HPC clusters that consists of many compute nodes and are shared by many users:\nNeed of root access to install in the base OS is a systems stability and security threat, and has a potential of users interfering with each other. Package managers as a rule do not keep several versions of the same package; they are geared towards having only the newest one (as in \u0026ldquo;software update\u0026rdquo;), which poses a problem for reproducible research. A package should be installed across all the nodes of the cluster; thus, the installations should be somehow managed. Binary packages in public repos tend to be compiled for generic CPU architectures rather than optimized for a particular system. Thus, in the HPC world, as a rule, only a minimal set of core Linux OS packages is installed by system administrators, and no access to package managers is given to the end users. There are ways to let users have their own Linux OS images through virtualization and containerization technologies (see the containers section) when it is really necessary.\nOn most of the HPC machines, the application software is recompiled from sources and installed into a shared filesystem so that each compute node has access to the same code or program. Multiple versions of a software package can be installed into each own PATH; dependencies between software (such as libraries from one package needed to be accessed by another package) are tracked via a special software called Environmental Modules.\nThe Modules package would manipulate the PATH (and other systems environment variables like LD_LIBRARY_PATH, CPATH and application-specific ones like MKLROOT, HDF5_HOME) on user request, \u0026ldquo;loading\u0026rdquo; and \u0026ldquo;unloading\u0026rdquo; specified software items.\nThe two most popular Modules are the original Tcl Modules and its Lmod rewrite in Lua at TACC. On the new Grex, Lmod is used.\nLmod # The main feature of Lmod is the hierarchical module system to provide a better control of software dependencies. Modules for software items that depend on a particular core module (toolchains: a compiler suite, a MPI library) are only accessible after the core modules are loaded. This prevents situations where conflicts appear when software items built with different toolchains are loaded simultaneously. Lmod will also automatically unload conflicting modules and reload their dependencies should toolchain change. Finally, by manipulating module root paths, it is possible to provide more than one software stack per HPC system. For more information, please refer to the software stacks available on Grex and using modules page.\nHow to find the software with Lmod Modules # A \u0026ldquo;software stack\u0026rdquo; module should be loaded first. On Grex, there are two software stacks, called GrexEnv and CCEnv, standing for the software built on Grex locally and the software environment from the Alliance (Compute Canada), correspondingly. GrexEnv is the only module loaded by default.\nWhen a software stack module is loaded, the module spider command will find a specific software item (for example, GAMESS; note that all the module names are lower-case on Grex and on Compute Canada software stacks) if it exist under that stack:\nmodule spider gamess It might return several versions; then usually a subsequent command with the version is used to determine dependencies required for the software. In case of GAMESS on Grex:\nmodule spider gamess/Sept2019 It will advise to load the following modules: \u0026ldquo;intel/15.0.5.223 impi/5.1.1.109\u0026rdquo;. Then, module load command can be used actually to load the GAMESS environment (note that the dependencies must be loaded first:\nmodule load intel/15.0.5.223 impi/5.1.1.109 module load gamess/Sept2019 For more information about using Lmod modules, please refer to the Alliance (Compute Canada) documentation about using Modules and Lmod user Guide.\nHow and when to install software in your HOME directory # Linux (Unlike some Desktop operating systems) has a concept of user permissions separation. Regular users cannot, unless explicitly permitted, access system\u0026rsquo;s files and files of other users.\nYou can almost always install software without super-user access into your /home/$USER directory. Moreover, you can manage the software with Lmod: Lmod automatically searches for module files under $HOME/modulefiles and adds the modules it discovers there into the modules tree so they can be found by module spider, loaded by module load, etc.\nMost Linux software can be installed from sources using either Autoconf or CMake configuration tools. These will accept --prefix=/home/$USER/my-software/version or -DCMAKE_INSTALL_PREFIX=/home/$USER/my-software/version as arguments. These paths are used for the installation directories where the user have full access.\nSoftware that comes as a binary archive to be unpacked can be simply unpacked into your home directory location. Then, the paths should be set for the software to be found: either by including the environment variable in $HOME/.bashrc or in $HOME/.bash_profile or by creating a specific module in $HOME/modulefiles/my-software/version following Lmod instructions for writing Modules.\nThere exist binary software environments like conda that manage their own tree of binary-everything. These can be used as well, with some caution, because automatically pulling everything might conflict with the same software existing in the HPC environment (Python package paths, MPI libraries, etc.).\nHowever, if a software is really a part of the base OS (something like a graphics Desktop software, etc.), it can be hard to rebuild from sources due to many dependencies. If needed, it may be better if installed centrally or used in a containeri, see Containers documentation.\nInternal links # Linux tools\nUsing modules\nCode development\nContainers\nCVMFS\nJupyter notebooks\nExternal links # Lmod Tcl Modules CMake Autoconf "},{"id":"38","rootTitleIndex":"8","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/","permalinkTitle":"Homepage / Software Specific Notes","title":"Software Specific Notes","content":" Software specific notes # This page refers to the usage of some specific programs installed on Grex, like ORCA, VASP, \u0026hellip; etc.\nSoftware / Applications # Gaussian\nJulia\nLAMMPS\nMATLAB\nNWChem\nORCA\nPriroda\nVASP\nExternal links # Running jobs (on the Alliance\u0026rsquo;s clusters) SLURM documentation. "},{"id":"39","rootTitleIndex":"9","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal","title":"OpenOnDemand, HPC Portal","content":" Introduction # OpenOnDemand or OOD for short, is an open source Web portal for High-Performance computing, developed at Ohio Supercomputing Center. OOD makes it easier for beginner HPC users to access the resources via a Web interface. OOD also allows for interactive, visualization and other Linux Desktop applications to be accessed on HPC systems via a convenient Web user interface.\nSince the end of October 2021, OpenOnDemand version 2 is officially in production on Grex.\nFor more general OOD information, see the OpenOnDemand paper\nOpenOndemand on Grex # Grex\u0026rsquo;s OOD instance runs on aurochs.hpc.umanitoba.ca . It is available only from UManitoba IP addresses \u0026ndash; that is, your computer should be on the UM Campus network to connect.\nTo connect from outside the UM network, please install and start UManitoba Virtual Private Network VPN. OOD relies on in-browser VNC sessions; so, a modern browser with HTML5 support is required; we recommend Google Chrome or Firefox and its derivatives (Firefox, for example).\nConnect to OOD using UManitoba VPN:\nMake sure Pulse Secure VPN is connected Point your Web browser to https://aurochs.hpc.umanitoba.ca Use your Alliance (Compute Canada) username and password to log in to Grex OOD. OpenOndemand login page Once connected, you will see the following screen with the current Grex Message-of-the-day (MOTD):\nFile view on OpenOndemand web portal on Grex OOD expects user accounts and directories on Grex to be already created. Thus, new users who want to work with OOD should first connect to Grex normally, via SSH shell at least once, to make the creation of account, directories, and quota complete. Also, OOD creates a state directory under users\u0026rsquo; /home (/home/$USER/ondemand) where it keeps information about running and completed OOD jobs, shells, desktop sessions and such. Deleting the ondemand directory while a job or session is running would likely cause the job or session to fail.\nIt is better to leave the /home/$USER/ondemand directory alone! Working with files and directories # One of the convenient and useful features of OOD is its Files app that allows you to browse the files and directories across all Grex filesystems: /home, /global/scratch and /project.\nFile view on OpenOndemand web portal on Grex You can also upload your data to Grex using this Web interface. Note that there are limits on uploads on the Web server (a few GBs) and there can be practical limits on download sizes as well due to internet connection speed and stability.\nCustomized OOD apps on Grex # The OOD Dashboard menu, Interactive Apps, shows interactive applications. This is the main feature of OOD, it allows interactive work and visualizations, all in the browser. These applications will run as SLURM Jobs on Grex compute nodes. Users can specify required SLURM resources such as time, number of cores and partitions.\nOpenOndemand applications on Grex As for now, the following applications are supported:\nLinux Desktops in VNC Matlab GUI in VNC GaussView GUI in VNC Jupyter Notebooks servers As with regular SLURM jobs, it is important to specify SLURM partitions for them to start faster. Perhaps the test partition for Desktop is the best place to start interactive Desktop jobs, so it is hardcoded in the Simplified Desktop item.\nThe following links are added to OOD:\nFrom the menu Jobs, there is a link Grex SLURM Queues State that shows a summary of running and pending jobs. It runs a modified version of the script grex-summarize-queue that is accessible from any login node. From the menu Clusters, there is a link Grex SLURM Node State to get a summary of allocated and idle nodes by partition. The same information can be accessed from any login node by running a custom script:slurm-nodes-state "},{"id":"40","rootTitleIndex":"10","rootTitle":"Grex changes / software and hardware updates","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/changes/","rootTitleTitle":"Homepage / Grex changes / software and hardware updates","permalink":"/grex-docs/changes/","permalinkTitle":"Homepage / Grex changes / software and hardware updates","title":"Grex changes / software and hardware updates","content":" Upcoming changes to Grex external network # As you may know, for the last 12 years, Grex has been using Westgrid’s network (IPs and DNS names in .westgrid.ca, like grex.westgrid.ca, aurochs.westgrid.ca).\nThis network is now being decommissioned, and we are migrating Grex’s external connection to UManitoba campus network. Grex’s new domain name will be .hpc.umanitoba.ca . Grex continues to use the same CCDB user account credentials as before (that is, your Alliance or Compute Canada login/password).\nTo minimize the impact on the users, migration will be rolling: we will first migrate some of the login nodes, then the rest of them. As of today, one new login node is available and uses the new UManitoba connection: yak.hpc.umabitoba.ca . The yak login node should be available from both outside UManitoba campus and from the VPN. We would like you to test it by connecting to Grex using:\nssh your-user-name@yak.hpc.umanitoba.ca (Replace your-user-name with your Alliance (Compute Canada) user name)\nPlease note that yak has avx512 CPU architecture and if you have to use it for compiling your codes, they may not run on the \u0026ldquo;compute\u0026rdquo; partition that has older CPUs. Other than that, it should behave as any other old login node. The other nodes (tatanka, bison, aurochs, and the alias grex) will be migrated later this summer. Internal links # Dec 10-11, 2019\nBefore Dec, 2019\n"},{"id":"41","rootTitleIndex":"14","rootTitle":"Friendly Organizations","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/friends/","rootTitleTitle":"Homepage / Friendly Organizations","permalink":"/grex-docs/friends/localit/","permalinkTitle":"Homepage / Friendly Organizations / Local IT Resources","title":"Local IT Resources","content":" Introduction # In addition to the HPC Research Computing facility, there are other IT providers that might relate to research.\nResources provided by IST # IST provides a number of services related to Administrative IT, Teaching, and Research Computing as well.\nFor more information about IST, please visit UM IST Help website and IST service catalog.\nResources provided by the Libraries # Libraries provide a variety of services related to Research Data Management, as well as a GIS service: UM Libraries Research Services.\nFaculties IT representatives # Several Faculties/Departments have local IT representatives that maintain servers, workstation and computing labs.\nFor more information, please contact directly the related services:\nIST UM Libraries Research Services. "},{"id":"42","rootTitleIndex":"11","rootTitle":"Getting Help","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/support/","rootTitleTitle":"Homepage / Getting Help","permalink":"/grex-docs/support/","permalinkTitle":"Homepage / Getting Help","title":"Getting Help","content":" The Alliance support # The single point support contact for the Alliance is support@tech.alliancecan.ca\nEmailing to this address will create a support ticket in the Alliance ticketing system (Help Desk). We support both local (Grex) and National resources through the Alliance support ticketing system. This is the main support contact for our HPC group and it is a preferred method (as compared to contacting an HPC analyst directly). If you use your UManitoba email address (email registered in CCDB) to contact the Alliance support, it will reach us faster because the system will automatically detect it and assign your user name to the generated ticket.\nYou can also open a web interface to the ticketing system OTRS. This requires actually having an Alliance account to access it, so it can be less useful for inquiries on how to get an account.\nTo sum it all up: the main way to reach out for support is: support@tech.alliancecan.ca Secondary support contacts in the Alliance that are useful for particular services:\ncloud@tech.alliancecan.ca for Cloud support. support@frdr-dfdr.ca for Federated Data Repository FRDR. globus@tech.alliancecan.ca for issues related to Compute Canada\u0026rsquo;s Globus file transfer. More information on the Alliance (formerly known as Compute Canada) support can be found here.\nWe support both local (Grex) and National resources through the Alliance (Compute Canada) support ticketing system.\nIST Helpdesk # We have an HPC group in IST\u0026rsquo;s Help Desk system support@umanitoba.ca called Cherwell, so the tickets opened in this system will reach us, especially if you clearly state that they are HPC or Alliance related. More info on the IST Help Desk website.\nIn person training sessions and workshops # You can book an in-person session (face to face meeting or online). We help new users for getting started on the systems we support, or for solving a specific problem that is harder to do over email, or for a consultation about Grex or the resources provided by the Alliance.\nWhere to find us on Fort Garry campus? E2-588 EITC, Fort Garry Campus\nUniversity of Manitoba\nWinnipeg, MB R3T 2N2\nTelephone: 204-474-9625\nHours:\nMonday - Friday\n8:30 am - 4:30 pm\nWe also provide small-group workshops on Research Computing topics when there is enough demand. Contact us if you are interested in having a workshop on a particular topic. Internal links # Workshops and Training Material External links # the Alliance support IST Help Desk WestDRI Training Material the Alliance National Training Calendar "},{"id":"43","rootTitleIndex":"12","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/","permalinkTitle":"Homepage / Workshops and Training Material","title":"Workshops and Training Material","content":" Next workshop: coming soon \u0026hellip; Local workshops # 2023\n2022\n2021\nWestGrid training material # The training material from WestDRI (BC DRI and Prairies DRI groups, formerly known as WestGrid) webinars are available online and classified into different categories:\nGetting started: Link Programming: Link Tools: Link Courses: Link Events: Link Training material from the Alliance and its regional partners # the Alliance ACENET CAC Calcul Quebec Compute Ontario SciNET, new link SharcNET WestDRI National Training calendar "},{"id":"44","rootTitleIndex":"13","rootTitle":"Frequently Asked Questions.","rootTitleIcon":"fa-solid fa-circle-question fa-lg","rootTitlePath":"/grex-docs/faq/","rootTitleTitle":"Homepage / Frequently Asked Questions.","permalink":"/grex-docs/faq/","permalinkTitle":"Homepage / Frequently Asked Questions.","title":"Frequently Asked Questions.","content":" "},{"id":"45","rootTitleIndex":"14","rootTitle":"Friendly Organizations","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/friends/","rootTitleTitle":"Homepage / Friendly Organizations","permalink":"/grex-docs/friends/","permalinkTitle":"Homepage / Friendly Organizations","title":"Friendly Organizations","content":""},{"id":"46","rootTitleIndex":"10","rootTitle":"Grex changes / software and hardware updates","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/changes/","rootTitleTitle":"Homepage / Grex changes / software and hardware updates","permalink":"/grex-docs/changes/changes-before-2020/","permalinkTitle":"Homepage / Grex changes / software and hardware updates / Linux/SLURM update project","title":"Linux/SLURM update project","content":" Grex defunded since April 2, 2018 # Since being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators. The old WestGrid documentation, hosted on the WestGrid website became irrelevant after the Grex upgrade, so please visit Grex’s New Documentation. Thus, if you are an experienced user in the previous “version” of Grex, you might benefit from reading this document: Description of Grex changes.\n"},{"id":"47","rootTitleIndex":"15","rootTitle":"Disclaimer","rootTitleIcon":"fa-solid fa-ban fa-lg","rootTitlePath":"/grex-docs/disclaimer/","rootTitleTitle":"Homepage / Disclaimer","permalink":"/grex-docs/disclaimer/","permalinkTitle":"Homepage / Disclaimer","title":"Disclaimer","content":" This website is a place for technical information related to certain Research Computing resources, maintained for the benefit of the researchers at the University of Manitoba, UofM and their external collaborators. The information, which is technical in nature, represents advice on best practices for using the Research Computing resources (HPC). Parts of the website are preliminary/draft texts released provisionally, in order to speed up the documentation process, and may contain inaccuracies and errors.\nWe advise users to exercise reason when following the advice from these pages. We disclaim responsibility for any harm, loss of data, loss of good-will or whatever negative effects might happen due to reading this documentation.\nThis website does not represent official views, policies, or standards of University of Manitoba, UofM, BC DRI group, Prairies DRI group or the Digital Research Alliance of Canada, DRAC (the Alliance), formerly known as Compute Canada. Please refer to the official sites of the above-mentioned institutions and organizations for more information.\nThis website does not represent official views of any hardware, software or services vendors that might be mentioned on the website\u0026rsquo;s pages.\nWestGrid ceased its operations since April 1st, 2022. The former WestGrid institutions are now re-organized into two consortia: BC DRI group and Prairies DRI group. Compute Canada ceased its operations since April 1st, 2022. The Digital Research Alliance of Canada, the Alliance, is coordinating the national advanced research computing ARC services. "},{"id":"48","rootTitleIndex":"16","rootTitle":"Glossary","rootTitleIcon":"fa-solid fa-sitemap fa-lg","rootTitlePath":"/grex-docs/glossary/","rootTitleTitle":"Homepage / Glossary","permalink":"/grex-docs/glossary/","permalinkTitle":"Homepage / Glossary","title":"Glossary","content":" A # AI :: Artificial Intelligence AVX :: Advanced Vector eXtensions B # BLAS :: Basic Linear Algebra Subprograms C # CPU :: Central Processing Unit D # DFT :: 1. Discrete Fourier Transform, 2. Density Functional Theory E # F # FFT :: Fast Fourier Transform G # GCC :: GNU Compiler Collection GPU :: Graphics Processing Unit GUI :: Graphical User Interface H # HPC :: High-Performance Computing HTTPS :: Hypertext Transfer Protocol Secure HTTP :: Hypertext Transfer Protocol I # I/O :: Input/Output IST :: Information Services and Technology J # K # L # M # MDS :: Metadata Server MDT :: Metadata Target ML :: Machine Learning MPI :: Message-Passing Interface N # NVMe :: Non-Volatile Memory express O # OMP :: Open Multi-Processing OOD :: Open OnDemand OSS :: Object Storage Server OST :: Object Storage Target P # PI :: Principal Investigator Q # R # RAC :: Resource Allocation Competition S # SLURM :: Simple Linux Utility for Resource Management SSH :: Secure SHell T # TLS :: Transport Layer Security U # V # VM :: Virtual Machine W # X # Y # Z # "},{"id":"49","rootTitleIndex":"17","rootTitle":"Grex Documentation - Sitemap","rootTitleIcon":"fa-solid fa-sitemap fa-lg","rootTitlePath":"/grex-docs/sitemap/","rootTitleTitle":"Homepage / Grex Documentation - Sitemap","permalink":"/grex-docs/sitemap/","permalinkTitle":"Homepage / Grex Documentation - Sitemap","title":"Grex Documentation - Sitemap","content":" Unofficial Grex User Guide\nHome\nQuick Start Guide\nAccess and usage conditions\nConnect / Transfer data\nConnecting with SSH\nConnecting with X2Go\nConnect with OOD\nData transfer\nStorage and Data\nData sizes and Quotas\nData sharing\nRunning jobs\nSlurm partitions\nInteractive jobs\nBatch jobs\nUsing local disks\nContributed nodes\nSoftware / Applications\nLinux tools\nUsing modules\nCode development\nContainers\nCVMFS\nJupyter notebooks\nSoftware Specific Notes\nGaussian\nJulia\nLAMMPS\nMATLAB\nNWChem\nORCA\nPriroda\nVASP\nOpenOnDemand\nGrex changes\nDec 10-11, 2019\nBefore Dec, 2019\nGetting Help\nWorkshops\n2023\n2022\n2021\nFAQ\nFriendly Organizations\nThe Alliance (DRAC)\nUManitoba IT resources\nDisclaimer\nGlossary\nSitemap\nDeprecated pages\n"},{"id":"50","rootTitleIndex":"18","rootTitle":"Deprecated pages","rootTitleIcon":"fa-solid fa-ban fa-lg","rootTitlePath":"/grex-docs/deprecated/","rootTitleTitle":"Homepage / Deprecated pages","permalink":"/grex-docs/deprecated/","permalinkTitle":"Homepage / Deprecated pages","title":"Deprecated pages","content":" Upcoming changes to Grex external network # As you may know, for the last 12 years, Grex has been using Westgrid’s network (IPs and DNS names in .westgrid.ca, like grex.westgrid.ca, aurochs.westgrid.ca).\nThis network is now being decommissioned, and we are migrating Grex’s external connection to UManitoba campus network. Grex’s new domain name will be .hpc.umanitoba.ca . Grex continues to use the same CCDB user account credentials as before (that is, your Alliance or Compute Canada login/password).\nTo minimize the impact on the users, migration will be rolling: we will first migrate some of the login nodes, then the rest of them. As of today, one new login node is available and uses the new UManitoba connection: yak.hpc.umabitoba.ca . The yak login node should be available from both outside UManitoba campus and from the VPN. We would like you to test it by connecting to Grex using:\nssh your-user-name@yak.hpc.umanitoba.ca (Replace your-user-name with your Alliance (Compute Canada) user name)\nPlease note that yak has avx512 CPU architecture and if you have to use it for compiling your codes, they may not run on the \u0026ldquo;compute\u0026rdquo; partition that has older CPUs. Other than that, it should behave as any other old login node. The other nodes (tatanka, bison, aurochs, and the alias grex) will be migrated later this summer. Internal links # No data available "}]