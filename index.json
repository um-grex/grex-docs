[{"id":"0","rootTitleIndex":"0","rootTitle":"Homepage","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"/grex-docs/","rootTitleTitle":"Homepage","permalink":"/grex-docs/","permalinkTitle":"Homepage","title":"Homepage","content":" Hardware and software \u0026mdash; Access and Usage Conditions \u0026mdash; Quick Start Guide \u0026mdash; Storage \u0026mdash; Connect and Transfer data \u0026mdash; Software and Applications \u0026mdash; Software notes \u0026mdash; Running jobs \u0026mdash; Workshops and Training \u0026mdash; OpenOnDemand \u0026mdash; How to get help\u0026quot; Traditional Territories Acknowledgement# The University of Manitoba campuses and research spaces are located on original lands of Anishinaabeg, Ininiwak, Anisininewuk, Dakota Oyate, Dene and Inuit, and on the National Homeland of the Red River Métis. We respect the Treaties that were made on these territories, we acknowledge the harms and mistakes of the past, and we dedicate ourselves to move forward in partnership with Indigenous communities in a spirit of Reconciliation and collaboration.\nFor more information, please visit the University of Manitoba Traditional Territories Acknowledgement page .\nHow to use this website? "},{"id":"1","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2025/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2025","title":"Workshops - 2025","content":" High Performance Computing Workshop - Oct 14-17, 2025# Below are the slides and some materials from the in-person workshop that was held on Oct 2025:\nAdditional materials: hands on Oct 14, 2025# Updates and overview of the Digital Alliance of Canada and UManitoba computing resources: Slides Basics of Linux Shell: Slides Data transfer with SSH and with Globus: Slides OpenOnDemand HPC Web portal: File Transfer, Remote Desktop and interactive GUI applications: Slides Oct 15, 2025# Intro to HPC software and Lmod modules tool: Slides Using Jupyter notebooks on HPC Systems: OpenOnDemand and JH Session: Slides Containers in HPC: using Docker/Podman containers: Slides Containers in HPC: using Pyxis: Podman Slides Oct 16, 2025# Running HPC jobs with SLURM scheduler / SLURM topics: optimizing your job throughput on HPC machines: Slides Running HPC Batch jobs with OpenOnDemand portal: Slides Running Python and Jupyter AI / Machine Learning workloads on HPC: Slides Oct 17, 2025# Cloud computing with OpenStack Cloud: IaaS VMs / provisioning a service on the cloud: Slides Using S3 Object Storage on OpenStack Cloud: Slides High Performance Computing Workshop - May 21-23, 2025# Below are the slides and some materials from the in-person workshop that was held on May 2025:\nMay 21, 2025# Introduction to Alliance and Manitoba ARC resources: Slides Introduction to Linux / BASH: Slides Data transfer with SSH and Globus: Slides Intro to HPC software, Lmod: Slides May 22, 2025# OpenOnDemand: Connect, File Transfer, Jobs, Apps: Slides Running jobs with SLURM scheduler: Slides Advanced SLURM topics: Slides Intro to OpenStack: Slides May 23, 2025# Python and Jupyter for GPU/AI/ML on HPC: Slides Additional materials: notebooks and data , LoRa Batch job walktgroough Containers in HPC: Singularity/Apptainer and Podman: Slides: Singularity Podman Various materials# LoRa walkthrough "},{"id":"2","rootTitleIndex":"12","rootTitle":"Friendly Organizations","rootTitleIcon":"fa-solid fa-users fa-lg","rootTitlePath":"/grex-docs/friends/","rootTitleTitle":"Homepage / Friendly Organizations","permalink":"/grex-docs/friends/alliancecan/","permalinkTitle":"Homepage / Friendly Organizations / Digital Research Alliance of Canada","title":"Digital Research Alliance of Canada","content":" Introduction# Digital Research Alliance of Canada (the Alliance) is a Canadian National Digital Research Infrastructure (DRI) organization (formerly known as Compute Canada). It provides the eligible researchers with the research computing resources such as several High-performance computing (HPC) systems with a large curated HPC software stack , private OpenStack cloud, and Globus data transfer software.\nThe Alliance also maintains a user authentication service and usage database called CCDB. On Grex, we rely on CCDB for accessing our system. The first step to get started with Grex is to register for an Alliance account at the CCDB website (if you do not already have one).\nThe Digital Research Alliance of Canada on the Web# Digital Research Alliance of Canada official website Alliance database CCDB Alliance user documentation Wiki Status page for the Alliance clusters Getting an account with the Alliance# University of Manitoba Faculty members are eligible for getting an account with the Alliance. Once it is obtained, they can manage accounts of their group members in CCDB.\nIf you are a postdoc, a student or an external collaborator, you can apply for an account as part of the Faculty\u0026rsquo;s group. You will need your PI\u0026rsquo;s CCDB role identifier code (CCRI). For more information, please follow this guide .\nThe account will allow you to manage your roles and groups in CCDB, to access National HPC and Cloud machines, as well as few other services such as Globus and NextCloud .\nSince we have switched Grex to CCDB credentials in late 2019, the CCDB account is both necessary and sufficient to access Grex.\nWestGrid accounts are no longer needed, nor working on Grex. Please use CCDB accounts for both Grex and DRAC systems! WestGrid network addresses in the westgrid.ca domain are no longer nor used on Grex. Please use hpc.umanitoba.ca domain! Alliance\u0026rsquo;s clusters and services# General purpose clusters:# Beluga (to be decommisionned soon and replaced by Rorqual) Rorqual Cedar (to be decommisionned soon and replaced by Fir) Fir Graham (to be decommisionned soon and replaced by Nibi) Nibi Narval Large parallel clusters:# Niagara (to be decommisionned soon and replaced by Trillium) Trillium Cloud:# Cloud PAICE clusters:# Killarney TamIA Vulcan Other services:# NextCloud Globus The following clusters {Cedar, Graham, Beluga and Niagara} will be decommissoned as soon as the new clusters {Fir, Nibi, Rorqual and Trillium} are fully operational. Gor more information, visit the page: Infrastructure renewal Regional Partners# The regional partners of the Alliance are: BC DRI group and Prairies DRI group, which consists of institutional members belonging to Western Canada (former WestGrid institutions), Compute Ontario, Calcul Quebec and AceNET which is a consortium of universities of Maritime provinces.\nBC DRI group Prairies DRI group Compute Ontario Calcul Quebec AceNET WestGrid ceased operations on April 1st, 2022. The former WestGrid institutions are now re-organized into two consortia: BC DRI group and Prairies DRI group. University of Manitoba is a member institution of Prairies DRI group, and provides a site for the Alliance support staff that support usage of the National DRI by Manitoba researchers and their collaborators.\nGetting support# The single point of contact for Alliance (Compute Canada) support is: support@tech.alliancecan.ca\nWhen requesting support, please make sure to include enough information that will help support staff to identify your issue in order to solve it in time. An example of details to include in your email are (but not limited to): user name, name of the cluster, job id if applicable, job script, list of the modules, a path to your files and scripts, \u0026hellip; etc. You may include any other details that may help to understand your issue.\n"},{"id":"3","rootTitleIndex":"<no value>","rootTitle":"Grex changes / software and hardware updates","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"","rootTitleTitle":"Homepage / Grex changes / software and hardware updates","permalink":"/grex-docs/changes/linux-slurm-update/","permalinkTitle":"Homepage / Grex changes / software and hardware updates / Linux/SLURM update project","title":"Linux/SLURM update project","content":"Grex changes: Linux/SLURM update project.# December 10-11, 2019\nIntroduction / Motivation# Grex runs an old version of CentOS 6, which gets unsupported in 2020. The 2.6.x Linux kernel that is shipped with CentOS 6 does not support containerized workloads that require recent kernel features. The Lustre parallel filesystem client had some troubles that we were unable to resolve with the CentOS 6 kernel version as well. Finally, the original Grex resource management software, Torque 2.5 and Moab7 are unable to properly schedule jobs that use newer MPI implementations (OpenMPI 2 and 3), which are increasingly common amongst HPC users. Therefore, using the power outages of October and December 2019, we have embarked on a rather ambitious project of updating the entire Grex OS and software stack and scheduling to CentOS 7 and SLURM. This document outlines the changes and how they will affect Grex users.\nConnecting to Grex# Grex is still using the Westgrid accounting system (Portal/LDAP). To connect to Grex, one needs an active Compute Canada account and a Westgrid consortium account linked to it. You are likely to have one.\nHosts to connect to# During the outage, most of the Grex compute nodes, and all the contributed systems have been reprovisioned to CentOS 7. Public access login nodes, bison.westgrid.ca and tatanka.westgrid.ca (and their DNS alias, grex.westgrid.ca ) give the new CentOS 7.6 / SLURM / LMOD environment.\nA test node, aurochs.westgrid.ca was preserved in the original CentOS 6 state, as well as about 600 cores of the Grex compute nodes. Logging in to aurochs for now allows users access to the original Grex environment (Torque, Moab, Tcl-Modules). We plan to eventually decommission the CentOS 6 partition when the CentOS 7 is debugged and fully in production.\nCommand line access via SSH clients# Because Grex login nodes were reinstalled, SSH clients might give either a warning or an error for not recognizing the Grex host keys. Remove the offending keys from the file ~/.ssh/known_hosts mentioned in the error message.\nGraphical access with X2Go# The new CentOS 7 supports X2Go connections to use GUI applications. However, the GNOME Desktop environment that was default in CentOS 6 is no longer available! Please use either ICEWM or OPENBOX Desktop environment in the X2Go client to connect.\nBecause X2Go is using SSH under the hood to establish the connection, the advice above about ~/.ssh/known_hosts holds: delete the old SSH keys from it if you have connection problems. On Windows, it is often located under C:\\Users\\username\\.ssh\\known_hosts.\nStorage# Lustre storage (/global/scratch) was updated to Lustre 2.10.8 on both server and client sides. We have ran our extensive tests and observed an increase of the write throughputs for large parallel I/O up to 3x. The change should be transparent to Grex users.\nSoftware, Interconnect, LMOD, CC/local stacks# Interconnect and communications libraries# Grex\u0026rsquo;s HPC interconnect hardware is no longer officially supported by commercial MLNX IB Verbs drivers. At the same time, open source projects like RDMA-Core and the new universal interconnect, UCX, almost reached maturity and superior performance. Therefore, for the Grex software update, we have opted for vanilla Kernel drivers for our Infiniband, RDMA-Core for verbs userland libraries and UCX for the communication layer for the new OpenMPI versions, of which we support OpeMPI 3.1.4 (the new default) and 4.0.2 (An experimental, bleeding edge MPI v3 standard implementation that obsoletes many old MPI features). The previous default version OpenMPI 1.6.5 is still supported, and still uses IB Verbs from RDMA-core.\nUsers that have codes directly linked against any IBVerbs or other low level MLNX libraries, or having fixed RPATH to the old OpenMPI binaries will have to recompile their codes!\nSoftware Modules: LMOD# This is a major change. We have made obsolete the tried and tested flat, TCL-based software Modules system in favor of Lmod. Lmod is a new software Module system developed by Robert McLay at TACC. The main difference between Lmod and TCL-mod is that Lmod is built to have a hierarchical module structure: it ensures that no modules of the same “kind” can be loaded simultaneously; that there be no deep module paths like “intel/ompi/1.6.5” or “netcdf/pnetcdf-ompi165-nc433” . Rather, users will load “root” modules first and dependent modules second. That is, instead of TCL-mode’s way on the old system (loading OpenMPI for Intel-14 compilers:\nmodule load intel/14.0.2.144 module load intel/ompi/1.6.5 The new Lmod way would be:\nmodule load intel/14.0.2.144 module load ompi/1.6.5 The hierarchy ensures that only a good ompi/1.6.5 module that corresponds to the previously loaded Intel-14 compilers gets loaded. Note that swapping the compiler modules (Intel to GCC or Intel 14 to Intel 15) results in automatic reload of the dependent modules, if possible. Loading two versions of the same modules simultaneously is no longer possible! This largely removes the need for “module purge” command.\nIn the hierarchical module system, dependent modules are not visible for “module avail” unless their dependencies are loaded. To figure what modules are available use “module spider” instead.\nFor more information and features, visit the official documentation of Lmod and/or Compute Canada documentation for modules .\nWe have tried to preserve the module names and paths closest to the original TCL-modules on Grex, whenever that was possible with the new hierarchy format. Note that the hierarchy is not “complete”: not every combination of software, compilers, and MPI exists on Grex, for practical reasons. Use \u0026ldquo;module spider name-of-the-software/version\u0026rdquo; to check what built variants are there. Send us a request to support@tech.alliancecan.ca if there is any missing software/toolchain combination you may want to use.\nSoftware Modules: Stacks , CVMFS, Defaults# One of the nice features of Lmod is its ability to maintain several software stacks at once. We have used it and now provide the following software stacks modules. The modules are “sticky” which means, one of them is always loaded. (Use module avail to see them). GrexEnv (default), OldEnv and CCEnv .\nGrexEnv is the current Grex default software environment, (mostly) recompiled to CentOS-7. Note that we do not load Intel compilers and OpenMPI modules by default anymore, in contrast to the old environment of Grex. The recompilation is mostly done, but not completely: should you miss a software item, please contact us!\nOldEnv is an Lmod-ized version of the old CentOS-6 modules, should you need it for compatibility reasons. The software (except OpenMPI) is as it was before (not recompiled). It may run on CentOS-7.\nCCEnv is the full Compute Canada software stack, brought to Grex via Cern Virtual Filesystem (CVMFS). We use the “sse3” software stack of Compute Canada because this is the one that suits our older hardware. Note that the default CC environments (StdEnv, nixpkgs) are NOT loaded by default on Grex! In order to access the CC software, they have to be loaded after the CCEnv as follows. (Note that the first load of the CC modules and software items might take a few seconds! It is probably a good practice to first access a CC software binary in a small interactive job to warm the local cache).\nmodule load CCEnv module load StdEnv/2016.4 nixpkgs/16.09 arch/sse3 module avail The CC software stack is documented at Compute Canada wiki: Available_software page. A caveat: it is in general impossible to isolate and containerize high-performance software completely, so not all CC CVMFS software might work on Grex: the most troublesome parts are MPI and CUDA software that rely on low level hardware drivers and direct memory access. Threaded SMP software and serial software we expect to run without issues.\nNote that for Contributed systems it might be beneficial to use a different architecture than the default SSE3, which is available at loading corresponding arch/ module.\nRunning jobs, migration to SLURM# CentOS-7 provides a different method of process isolation (cgroups). It also allows for better support of containers such as Singularity which (hopefully) can now be run in user namespaces. Incidentally, the cgroups are not supported well by any Torque/Moab scheduler version that was compatible with our current Moab software license. Torque is not known to support the new process management interface (PMIX) that is increasingly becoming a standard for MPI libraries either. Therefore, we had little choice but to migrate our batch workload management from Torque to SLURM. It will also make Grex more similar to Compute Canada machines (which are documented here: Running jobs ).\nUnlike Torque which is a monolithic, well engineered piece of software that we knew well, SLURM is a very modular, plugin-based ecosystem which is new to us. Therefore, initially we will enable only short walltimes to test our SLURM configuration (48h) before going to full production.\nTorque wrappers# To make life easier for PBS/Torque users, SLURM developers provided most of the Torque commands as wrappers over SLURM commands. So with some luck, you can continue using qsub, qstat, pbsnodes etc. keeping in mind correspondence between Torque’s queues and SLURM partitions. For example:\nsbatch --account=abc-668-aa --partition=compute my.job\nCan be called using Torque syntax as:\nqsub -A abc-668-aa -q compute my.job\nPresently, and unlike old Grex, there is no automatic queue/partition/QOS assignment based on jobs resource request. The option --partition must be selected explicitly for using High Memory and contributed nodes, if you have access to the latter. By default, jobs go into “compute” partition comprised of the original 48 GB, 12 Nehalem CPUs nodes. The command qstat -q now actually lists partitions (which it thinks are queues).\nInteractive Jobs# As before, and as usual for HPC systems, we ask users to limit their work on login nodes to code development and small, infrequent test runs and to use interactive jobs for longer and/or heavier interactive workloads.\nInteractive jobs can be done using SLURM salloc command as well as using qsub -I . The one limitation for the latter is there for graphical jobs: the latest and greatest SLURM 19.05 supports --x11 flags natively for salloc, but does not support yet corresponding qsub -I -X flags for the Torque wrapper. So graphical interactive jobs are only possible with salloc.\nAnother limitation of qsub is the syntax: SLURM does distinguish between --ntasks= for MPI parallel jobs and --cpus-per-task= for SMP threaded jobs; while for qsub it is all the same for its -l nodes=1:ppn= syntax. Therefore, the SMP threaded applications might not be placed correctly with the jobs submitted with qsub.\nBatch Jobs.# There are two changes: need to specify partitions explicitly and short maximal walltimes during initial Grex testing. Resource allocations for Grex RAC 2019-2020 will be implemented in the SLURM scheduler. Two useful commands: sshare (shows your groups fairshare parameters) and seff (shows efficiency of your jobs: CPU and memory usage) might help with effective usage of your allocation.\nIn general, Compute Canada documentation on running SLURM jobs can be followed, obviously with the exception of different core count on Grex nodes. Presently, we schedule on Grex by CPU core (as opposed to by-node) so whole-node jobs do not get any particular prioritization.\nFor the local scratch directories, $TMPDIR should be used in batch scripts on Grex (as opposed to Compute Canada where $SLURM_TMPDIR is defined). Thus, for using software from Compute Canada stack that has references to SLURM_TMPDIR hardcoded in some scripts it relies on (an example being GAMESS-US) the following line should be added on Grex to your job scripts:\nexport SLURM_TMPDIR=$TMPDIR\n"},{"id":"4","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/espresso/","permalinkTitle":"Homepage / Software Specific Notes / Running Quantum Espresso on Grex","title":"Running Quantum Espresso on Grex","content":"Introduction# Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft).\nSystem specific notes# On the Grex\u0026rsquo;s default software stack (SBEnv), Espresso is built using a variety of compilers and Open MPI\nTo find out which versions are available, use module spider espresso.\nFor a version 7.3.1, the following modules should be loaded:\nmodule load arch/avx512 intel/2023.2 openmpi/4.1.6 module load espresso/7.3.1 The above module gives access to the Espresso built with traditional Intel compilers and MKL. These would be recommended for compute nodes using Intel AVX512 CPUs.\nFor better efficiency on AMD CPUs (partitions genoa, genlm) a better performance may be achieved by using GCC compilers and AOCL linear algebra packages:\nNote that the module version has +aocl-4.2.0 ; this notation shows that it had been built with AOCL.\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load espresso/7.3.1+aocl-4.2.0 Script example for running Quantum Espresso on Grex run-espresso.sh#!/bin/bash #SBATCH --ntasks=32 --mem-per-cpu=4000mb #SBATCH --time=0-3:00:00 #SBATCH --job-name=ausurf # Adjust the number of tasks, time and memory required. # the above spec is for 32 MPI compute tasks # Load the modules for the Intel version module load SBEnv module load arch/avx512 intel/2023.2 openmpi/4.1.6 module load espresso/7.3.1 ## for AMD partitions --partition=genoa uncomment the modules below: #module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 #module load espresso/7.3.1+aocl-4.2.0 # Unless multithreading is required, it is better to use serial BLAS/LAPACK export OMP_NUM_THREADS=1 export MKL_NUM_THREADS=1 export BLIS_NUM_THREADS=1 which pw.x # lets run the AuSurf example, assuming inputs in the current directory # adjust pw.x options such as -npool and -ndiag according to your system and number of tasks! echo \u0026#34;Starting run at: `date`\u0026#34; srun pw.x -input ausurf.in -npool 2 -ndiag 16 \u0026gt; ausurf.${SLURM.}.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-espresso.sh, it can be submitted with:\nsbatch run-espresso.sh For more information, visit the page running jobs on Grex Running GPU version of Espresso# It is possible to build a version of Quantum Espresso using GPU hardware. Doing so requires the NVidia HPC compilers toolkit which is available on the NGC Cloud. However, an easier way would be to just pull a latest Espresso version from the same NGC cloud using either Singularity or Podman .\nhttps://catalog.ngc.nvidia.com/orgs/hpc/containers/quantum_espresso Related links# Running jobs on Grex NVidia NGC cloud Catalogue "},{"id":"5","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/gaussian/","permalinkTitle":"Homepage / Software Specific Notes / Running Gaussian on Grex","title":"Running Gaussian on Grex","content":"Introduction# Gaussian 16 is a comprehensive suite for electronic structure modeling using ab-initio, DFT and semi-empirical methods. A list of Gaussian 16 features can be found here .\nUser Responsibilities and Access# University of Manitoba has a site license for Gaussian 16 and GaussView. However, it comes with certain license limitations, so access to the code is subject to some license conditions.\nSince, as of now, Compute Canada accounts are a superset of Grex accounts, users will want to initiate getting access by sending an email agreeing to Gaussian conditions to support@tech.alliancecan.ca, confirming that you have read and agree to abide by the following conditions, and mentioning that you\u0026rsquo;d also want to access it on Grex:\n1. I am not a member of a research group developing software competitive to Gaussian.\n2. I will not copy the Gaussian software, nor make it available to anyone else.\n3. I will properly acknowledge Gaussian Inc. and Compute Canada in publications.\n4. I will notify Compute Canada of any change in the above acknowledgement.\nIf you are a sponsored user, your sponsor (PI) must also have such a statement on file with us.\nMoreover, the terms of the UManitoba license are actually stricter than for the Alliance (Compute Canada). In particular, it excludes certain research groups at the University to have access to the software. Therefore, we are required by Gaussian to have each of the Gaussian users to sign a Confidentiality Agreement form as provided to us by Gaussian. Inc. Please drop by our office in Engineering, E2-588 to get the form and return it signed.\nSystem specific notes# On Grex, Gaussian is limited to a single node, SMP jobs and the memory of a single node. There is no Linda. The Gaussian code is accessible as a module. The module sets Gaussian\u0026rsquo;s environment variables like GAUSS_SCRDIR (the latter, to local node scratch).\nmodule load gaussian/g16.c01 To load the module and access the binaries, you will first get access as per above. Also, our Gaussian license span is less than Compute Canada\u0026rsquo;s support contract, so there are fewer versions available. Use module spider gaussian to see what is available on Grex.\nAfter a Gaussian module is loaded, the GaussView software also becomes available. GaussView can be used via OOD . There is already an application that will show up if you have access to Gaussian. Here is a snapshot of Gaussview using OOD application:\nGaussview Application: snapshot The binary is called gv:\ngv The viewer should not be used to run production calculations on Grex login nodes. Instead, as for any other production calculations, SLURM jobs should be used as described below.\nUsing Gaussian with SLURM# Sample SLURM Script# Script example for running Gaussian on Grex run-gaussian.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=32gb #SBATCH --time=8:00:00 #SBATCH --job-name=Gauss16-test module load gaussian/g16.c01 echo \u0026#34;Starting run at: `date`\u0026#34; which g16 # note that input should have %nproc=8 # and %mem=40gb for the above resurce request. g16 \u0026lt; input.gjf \u0026gt; output.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Simplified job submission# A simplified job script sbg16 is available (after loading of the g16 module) for automatic generation and submission of SLURM Gaussian jobs.\nsbg16 input.gjf -ppn 8 -part skylake -mem 40000mb -time 12:00:00 The script takes input file name (must have the .gjf extension), which must be the first argument, and a couple of parameters:\n-ppn N : number of threads. Cannot be more than physical number of threads per node on the selected partition -part name : a SLURM partition to run the job in . -mem MemSpec : total memory per job. The MemSpec can include usual units, without spaces (1000mb, 3gb , etc). -time TimeSpec : walltime requested for the job, in SLURM format (1-00:00 or 24:00:00 give one day). Using NBO# University of Manitoba has site licenses for NBO6 and NBO7. Corresponding NBO modules would have to be loaded in order to use Gaussian\u0026rsquo;s POP=NBO6 or NBO7 keywords.\nTo list available NBO versions and their dependencies, run the command:\nmodule spider nbo Related links# Gaussian documentation. Gaussian page on the Alliance wiki. Gaussian error messages . "},{"id":"6","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/gromacs/","permalinkTitle":"Homepage / Software Specific Notes / Running GROMACS MD package on Grex","title":"Running GROMACS MD package on Grex","content":"Introduction# GROMACS (GROningen MAchine for Chemical Simulations) is a molecular dynamics package primarily designed for simulations of proteins, lipids and nucleic acids. GROMACS is one of the fastest and most popular software packages available and can run on CPUs as well as GPUs.\nSystem specific notes# On the Grex\u0026rsquo;s default software stack (SBEnv), GROMACS is built using a variety of compilers and OpenMPI (4.1 or 5.0 for example).\nTo find out which versions are available, use module spider gromacs. There could be more than one (for example, CPU and GPU) builds available for each GROMACS version as listed by module spider.\nFor a version gromacs/2024.1, at the time of writing the following modules should be loaded for the CPU version:\nmodule load SBEnv module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load gromacs/2024.1 The above module/version gives access to the GROMACS built for compute nodes on GREX, using Intel or AMD AVX512 CPUs.\nThere is also a CUDA GPU version that would be able to use GPU partitions on Grex. It can be made available by loading modules in the following order:\nmodule load SBEnv module load cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6 module load gromacs/2024.1 Note that depending on how GROMACS was built, the main executable may be called either gmx or gmx_mpi ! Plese adjust your scripts accordingly.\nScript example for running GROMACS on Grex run-gromacs.sh#!/bin/bash #SBATCH --nodes=1 # number of nodes #SBATCH --ntasks-per-node=26 # request 26 MPI tasks per node #SBATCH --cpus-per-task=2 # 2 OpenMP threads per MPI task #SBATCH --mem-per-cpu=1000M # memory per CPU (in MB) #SBATCH --time=0-03:00 # time limit (D-HH:MM) #SBATCH --job-name=md-test # Adjust the number of tasks, nodes, threads time and memory required. # the above spec is for 26 MPI compute tasks each spawning 2 threads = 52 cores per node # Load the modules for the CPU version using Grex SBEnv environment module load SBEnv module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load gromacs/2024.1 # If multithreading is required, it has to be passed to the task with OMP_NUM_THREADS export OMP_NUM_THREADS=\u0026#34;${SLURM_CPUS_PER_TASK:-1}\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; srun --cpus-per-task=$OMP_NUM_THREADS gmx_mpi mdrun -deffnm md echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-gromacs.sh, it can be submitted with:\nsbatch run-gromacs.sh For more information about job submission, visit the page running jobs on Grex Related links# Running jobs on Grex Alliance GROMACS documentation provides more use cases and examples that largely apply to Grex as well. "},{"id":"7","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/julia/","permalinkTitle":"Homepage / Software Specific Notes / Running Julia on Grex","title":"Running Julia on Grex","content":"Introduction# Julia is a programming language that was designed for performance, ease of use and portability. It is available as a module on Grex.\nAvailable Julia versions# Presently, binary Julia version julia/1.11.3 is available. Use module spider julia to find out other versions.\nInstalling packages# We do not maintain centralized versions of Julia packages. Users should install Julia modules in their home directory.\nThe command is (in Julia REPL):\nUsing Pkg; Pkg.Add(\u0026#34;My-Package\u0026#34;) In case of package/version conflicts, remove the packages directory ~/.julia/.\nUsing Julia notebooks# It is possible to use IJulia kernels for Jupyter notebooks. A preferable way of running a Jupyter notebook is SLURM interactive job with salloc command.\n(More details coming soon).\nRunning Julia jobs# Julia comes with a large variety of packages. Some of them would use threads; and therefore, have to be run as SMP jobs with \u0026ndash;cpus-per-task specified. Moreover, you would want to set the JULIA_NUM_THREADS environment variable in your job script to be the same as SLURM\u0026rsquo;s number of threads.\nScript example for running Julia on Grex run-julia.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=8:00:00 #SBATCH --job-name=Julia-Test # Load the modules: module load julia/1.10.3 echo \u0026#34;Starting run at: `date`\u0026#34; julia test-julia.jl echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-julia.sh, it can be submitted with:\nsbatch run-julia.sh For more information, visit the page running jobs on Grex Using Julia for GPU programming# It is possible to use Julia with CUDA Array objects to greatly speed up the Julia computations. For more information, please refer to this link: julia-gpu-programming . However, a suitable \u0026ldquo;CUDA\u0026rdquo; module should be loaded during the installation of the CUDA Julia packages. And you likely want to be on a GPU node when the Julia GPU code is executed.\nRelated links# Julia on the Alliance\u0026rsquo;s clusters Running jobs on Grex "},{"id":"8","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/jupyter-notebook/","permalinkTitle":"Homepage / Software Specific Notes / How to use Jupyter notebooks on Grex?","title":"How to use Jupyter notebooks on Grex?","content":"Jupyter on Grex# Jupyter is a Web-interface aimed to support interactive data science and scientific computing. Jupyter supports several dynamic languages, most notably Python, R and Julia. Jupyter offers a metaphor of \u0026ldquo;computational document\u0026rdquo; that combines code, data and visualizations, and can be published or shared with collaborators.\nJupyter can be used either as a simple, individual notebook or as a multi-user Web server/Interactive Development Environment (IDE), such as JupyterHub/JupyterLab. The JupyterHub servers can use a variety of computational back-end configurations: from free-for-all shared workstation to job spawning interfaces to HPC schedulers like SLURM or container workflow systems like Kubernetes.\nThis page lists examples of several ways of accessing Jupyter notebooks.\nUsing notebooks via Grex OOD Web Portal# Grex provides Jupyter Notebooks and/or JupyterLab as an OpenOnDemand dashboard application. This is much more convenient than handling SSH tunnels for Jupyter manually. The servers will run as a batch job on Grex compute nodes, so as usual, a choice of SLURM partition will be needed.\nGrex provides access to Jupyter Notebooks and JupyterLab through an OpenOnDemand server Application. This method is much more convenient than manually handling SSH tunnels for Jupyter.\nNotebooks under OOD would run as batch jobs on Grex compute nodes, and thus have access to any available hardware such as GPUs, as needed. You will need to select a SLURM partition, as well as other resource requests (CPU, GPU, time) when launching a OOD Jupyter session.\nThere are multiple versions of the OOD Jupyter application, that correspond to either local Grex software environment (SBEnv) or the Alliance/ComputeCanada environment (CCEnv, using the current StdEnv version).\nYou can find more information on using OOD in the Grex OOD documentation pages .\nInstalling additional Jupyter Kernels# To run, a Jupyter notebook require a connection to a language \u0026ldquo;kernel\u0026rdquo;. Generally, Jupyter would start only with a Python kernel that came with the jupyter-notebook server. However, there are many more kernels that are available!\nTo use R, Julia, or different instances or versions of Python from Jupyter, you must install a corresponding Jupyter Notebook kernel for each language. Each user must install these kernels individually, within their $HOME directories.\nNote that language kernels must match the software environments they will be used with. For example, install kernels using the local Grex software stack if you plan to use them from the Grex OOD Jupyter App. Python\u0026rsquo;s virtualenv can be helful in isolating the various environements and their dependencies.\nThe kernels are installed under a hidden directory $HOME/.local/share/jupyter/kernels.\nAdding an R kernel# For example, in order to add an R kernel, the following commands can be used:\n#Loading the R module of a required version and its dependencies go here. _R_ will be in the PATH. R -e \u0026#34;install.packages(c(\u0026#39;crayon\u0026#39;, \u0026#39;pbdZMQ\u0026#39;, \u0026#39;devtools\u0026#39;), repos=\u0026#39;http://cran.us.r-project.org\u0026#39;)\u0026#34; R -e \u0026#34;devtools::install_github(paste0(\u0026#39;IRkernel/\u0026#39;, c(\u0026#39;repr\u0026#39;, \u0026#39;IRdisplay\u0026#39;, \u0026#39;IRkernel\u0026#39;)))\u0026#34; R -e \u0026#34;IRkernel::installspec()\u0026#34; Adding a Julia kernel# For Julia, the package IJulia has to be installed: #Loading the Python and Jula module of a required version and its dependencies go here. _julia_ will be in the PATH. echo \u0026#39;Pkg.add(\u0026#34;IJulia\u0026#34;)\u0026#39; | julia python -m ipykernel install --user --name julia --display-name \u0026#34;Julia\u0026#34;\nAdding a Python kernel using virtualenv# Users\u0026rsquo; own Python kernels can be added to Jupyter by using \u0026ldquo;ipykernel\u0026rdquo; command. Because there are numerous versions of Pythons across more than one software stack, and Python modules may depend on a variety of other software (such as CUDA or Boost libraries) is usually a good idea to encapsulate the kernel together with all the Python modules required, in a virtualenv .\nThe example below creates a virtualenv for using Arrow and Torch libraries in Jupyter, using the Alliance\u0026rsquo;s CCEnv softwre stack. Note that using CUDA from CCEnv requires this to be done on a GPU node, using a salloc interactive job.\n# first load all the required modules. The CCEnv stack itself: module load CCEnv module load arch/avx2 module load StdEnv/2023 # then Python Arrow with dependendencies. Scipy-Stack provides most common libraries such as NumPy and SciPy. module load cuda python scipy-stack module load arrow boost r # now we can create a virtualenv, install required modules off CCENv wheelhouse virtualenv ccarrow source ccarrow/bin/activate pip install --upgrade pip pip install transformers pip install datasets pip install evaluate pip install torch pip install scikit-learn pip install ipykernel pip install jupyter # finally, register the virtualenv for the current user python -m ipykernel install --user --name=ccarrow #check the kernels and flush them changes just to be sure; deactivate the nevironment jupyter kernelspec list sync deactivate # now, JupyterLab should show a Python kernel named \u0026#34;ccarrow\u0026#34; in the Launcher To be able to actually start the kernel on a JupyerLab notebook webpage, all the modules (cuda python scipy-stack arrow boost r) must be first loaded. One way of loading the modules is to use the Alliance\u0026rsquo;s \u0026ldquo;Software Module\u0026rdquo; extension, which is available on the left \u0026ldquo;tab\u0026rdquo; of the Jypyter instances that are using CCEnv.\nKeeping track of installed kernels# The kernels are installed under a hidden directory $HOME/.local/share/jupyter/kernels. To list installed kernels, manipulate them etc., there are a few useful commands:\n#Loading the Python module of a required version and its dependencies go here. jupyter kernelspec list The output will be the list of installed kernels. A kernel can be uninstalled using the following command, referring to a kernel name from the list: #Loading the Python module of a required version and its dependencies go here. jupyter kernelspec uninstall my_kernel\nCheck out the corresponding Compute Canada documentation here for more information\nUsing notebooks via SSH tunnel and interactive jobs# Any Python installation with Jupyter Notebooks installed can be used to launch the simple notebook interface. Since activity on HPC login nodes is usually restricted, you should first start an interactive batch job. Within the interactive session, you will start a Jupyter Notebook server and use an SSH tunnel to connect to it from your local web browser.\nAfter logging on to Grex as usual, issue the following salloc command to start an interactive job :\nsalloc --partition=skylake --nodes=1 --ntasks-per-node=2 --time=0-3:00:00 This should give you a command prompt on a compute node. You may adjust parameters like partition, time limit, etc., to fit your needs. Next, ensure that a Python module is loaded and that Jupyter is installed, either within the Python environment or a virtual environment. Then start a notebook server, choosing an arbitrary port number, such as 8765. (If the port is already in use, simply pick another.)\nNote that a desired Python module must be loaded, and corresponding virtualenv must be activated as before accessing the jupyter-notebook command!\njupyter-notebook --ip 0.0.0.0 --no-browser --port 8765 If successful, there should be at least two lines close to the end of the console output:\nhttp://g333:8765/?token=ae348acfa68edec1001bcef58c9abb402e5c7dd2d8c0a0c9 http://127.0.0.1:8765/?token=ae348acfa68edec1001bcef58c9abb402e5c7dd2d8c0a0c9 or similar, where g333 refers to a compute node it runs, 8765 is a local TCP port and the token (the long hash code) is an access/authentication browser token.\nAt this point, you have a Jupyter Notebook server running on the compute node — but it is not yet accessible from your local browser. To access it, you will need to create an SSH tunnel.\nAssumingi you are using a command-line SSH client (OpenSSH or MobaXterm terminal window), opeen a new tab or terminal and run the following command:\nssh -fNL 8765:g333:8765 yourusername@bison.hpc.umanitoba.ca Replace g333, 8765, and yourusername with the correct node name, port, and username.\nIf successful, the SSH command above returns nothing. Keep the terminal window open for as long as you need the SSH tunnel (that is, for the duration of your Jupyter session).\nNote that only one port can be used per server at a time! If the port you selected is already in use (due to another Jupyter Notebook session or an existing SSH tunnel), simply pick a different random port (e.g., 8711) and repeat the procedure.\nThe final step is to open a browser on your client computer (Firefox is the best as Chrome might refuse to do plain http://) and navigate it to the tunnelded port on localhost or 127.0.0.1, as in http://localhost:8765 or http://127.0.0.1:8765 . Use the token as per above to authenticate into the jupyter notebook session, either copying it into the prompt or providing it in the browser address line.\nSimply copying and pasting the Jupyter URL starting from http://127.0.0.1:8765/?token=your-token-goes-here should work! Do not use the URL that starts with the node name (e.g., g333) — it will not work from your local machine.\nAfter this, you should be able to use a Jupyter Notebook, on the compute node, through your local browser.\nThe notebook session will remain active as long as:\nthe interactive job (salloc) session is alive, the Jupyter Notebook server is running, and the SSH tunnel remains open. This method works not only on Grex, but also on the Alliance and most other HPC systems.\nNot using Jupyter notebooks in SLURM jobs# While Jupyter is a great debugging and visualization tool, for heavy production calculations it is almost always a better idea to use batch jobs. However, the Notebooks cannot be executed from Python (or other script languages) directly!\nFortunately, it is possible to convert Jupyter Notebooks (.ipynb format) to a runnable script using Jupyter\u0026rsquo;s nbconvert command.\nFor example, in a Python\u0026rsquo;s Notebook cell:\n!pip install \u0026ndash;no-index nbconvert\n!jupyter nbconvert \u0026ndash;to script my_notebook.ipynb\nOr, in the Jupyter Notebook or JupyterLab GUI, there is an\nOr, in command line (provided corresponding Python and Jupyter modules are loaded first): #Python modules, virtualenv activation commands etc. go here jupyter nbconvert --to script my_notebook.ipynb\nThen, in a SLURM job, the resulting script can be executed with a regular Python (or R, or Julia). Again, after loading the required modules,\n#SLURM headers and modules go here python my_notebook.py Other jupyter instances around# There is a SyZyGy instance umanitoba.syzygy.ca that gives a free-for-all shared SyZyGy JupyterHub for UManitoba users.\nMost of the Alliance\u0026rsquo;s HPC machines deployed JupyterHub interfaces, like Narval and Rorqual. These instances submit Jupyter notebooks as SLURM jobs directly from the JupyterHub interface.\n"},{"id":"9","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/lammps/","permalinkTitle":"Homepage / Software Specific Notes / Running LAMMPS on Grex","title":"Running LAMMPS on Grex","content":"Introduction# LAMMPS is a classical molecular dynamics code. The name stands for Large-scale Atomic / Molecular Massively Parallel Simulator. LAMMPS is distributed by Sandia National Laboratories, a US Department of Energy laboratory.\nModules# On the Grex’s default software stack (SBEnv), LAMMPS was built using a variety of compilers and OpenMPI 4.1\nTo find out which versions are available, use module spider lammps\nAs an example of the version 2024 Aug 29 patch 1, using GCC:\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load lammps/2024-08-29p1 or, another example of an older LAMMPS version using Intel OneAPI compilers:\nmodule load arch/avx512 intel-one/2024.1 openmpi/4.1.6 module load lammps/2021-09-29 There is a GPU version of LAMMPS on Grex that uses the KOKKOS library for GPU interface. It is available as follows:\nmodule load cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6 module load lammps/2024-08-29p1 The above LAMMPS version should be used only on GPU-enabled partitions.\nIt is also possible to load modules from the Alliance software stack after load CCEnv:\nmodule purge module load CCEnv module load arch/avx512 module load StdEnv/2023 module load intel/2023.2.1 openmpi/4.1.5 module load lammps-omp/20230802 Serial version# Script example using a module from SBEnv:\nScript example for LAMMPS: Serial version run-lammps-serial-sbenv.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test # Load the modules: module load arch/avx512 intel-one/2024.1 openmpi/4.1.6 module load lammps/2021-09-29 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; ${lmp_exec} \u0026lt; ${lmp_input} \u0026gt; ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Script example using a module from CCEnv:\nScript example for LAMMPS: Serial version run-lammps-serial-cc.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test # Load the modules: module purge module load CCEnv module load arch/avx512 module load StdEnv/2023 module load intel/2023.2.1 openmpi/4.1.5 module load lammps-omp/20230802 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; ${lmp_exec} \u0026lt; ${lmp_input} \u0026gt; ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; MPI version# Script example using a module from SBEnv:\nScript example for LAMMPS: MPI version run-lammps-mpi-sbenv.sh#!/bin/bash #SBATCH --ntasks=16 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test # Load the modules: module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load lammps/2024-08-29p1 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; srun ${lmp_exec} -in ${lmp_input} -log ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Script example using a module from CCEnv:\nScript example for LAMMPS: MPI version run-lammps-mpi-cc.sh#!/bin/bash #SBATCH --ntasks=16 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test # Load the modules: module purge module load CCEnv module load arch/avx512 module load StdEnv/2023 module load intel/2023.2.1 openmpi/4.1.5 module load lammps-omp/20230802 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; srun ${lmp_exec} -in ${lmp_input} -log ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Script example using a GPU version using KOKKOS from SBEnv:\nScript example for LAMMPS: MPI version run-lammps-mpi-sbenv.sh#!/bin/bash #SBATCH --ntasks=1 --partition=gpu #SBATCH --cpus-per-gpu=8 --gpus=1 #SBATCH --mem=15000M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Lammps-Test-GPU # Load the modules: module load cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6 module load lammps/2024-08-29p1 echo \u0026#34;Starting run at: `date`\u0026#34; lmp_exec=lmp lmp_input=\u0026#34;lammps.in\u0026#34; lmp_output=\u0026#34;lammps_lj_output.txt\u0026#34; # this example uses KOKKOS GPU module, on a single GPU srun ${lmp_exec} -in ${lmp_input} -k on g 1 -sf kk -pk kokkos newton off neigh full -log ${lmp_output} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Related links# LAMMPS website. LAMMPS GitHub LAMMPS online documentation. LAMMPS Tuning LAMMPS from HPC Carpentries "},{"id":"10","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/general-linux/","permalinkTitle":"Homepage / Software and Applications / General Linux tools","title":"General Linux tools","content":"Linux tools on Grex# There are a number of general and distro-specific tools on Grex that are worth mentioning here. Such tools are: text editors, image viewers, file managers, \u0026hellip; etc.\nCommand line Text editors# Command line text editors allow you to edit files right on Grex in any terminal session (such as SSH session or an X terminal under OOD):\nThe (arguably) most popular editor is vi, or vim. It is very powerful, but requires some experience to use it. To exit a vim session, you can use the ZZ key combination (hold shift key + zz), or ESC, :x!. There are many vi tutorials around, for example this one or learn VIM in X minutes . Another lightweight text-mode editor is nano. It provides a self-explanatory key-combination menu at the bottom of the screen. An online manual can be found here . A more modern alternative to \u0026ldquo;nano\u0026rdquo; is micro . On Grex it is available only as module (module load micro) . micro supports syntax coloring for a number of programming languages. The webpage of micro . Midnight Commander file manager provides a text-mode editor that can be invoked stand-alone as mc -e filename, or from within mc by using F4 or Edit menu item on a selected file. GUI Text editors# Vi has a GUI counterpart which is accessible as evim command. There are also the following GUI editors: nedit and xfe-xfw.\nImage viewers# There are the following commands that can be used for viewing images: xfe-xfi. A simple PDF viewer for X11, ghostscript__ is also available.\n"},{"id":"11","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/matlab/","permalinkTitle":"Homepage / Software Specific Notes / Running MATLAB on Grex","title":"Running MATLAB on Grex","content":"Introduction# MATLAB is a general-purpose high-level programming package for numerical work such as linear algebra, signal processing and other calculations involving matrices or vectors of data. We have a campus license for MATLAB which is used on Grex and other local computing resources. As with most of the Grex software, MATLAB is available as a module. The following command will load the latest version available on Grex:\nmodule load matlab Then the matlab executable will be in the PATH.\nTo see all the available versions, use:\nmodule spider matlab Available Toolboxes# To see a list of the MATLAB toolboxes available with MATLAB license on Grex, for a given MATLAB version, use the following command:\nmodule load matlab matlab -nodisplay -nojvm -batch \u0026#34;ver\u0026#34; Running Matlab# It is possible to run MATLAB GUI interactively, for best performance in a remote OOD Desktop session. There is no menu shortcuts for Desktops for MATLAB, so a Terminal window has to be open and the matlab module loaded in it. After loading the module, the command matlab will be in the PATH.\nOOD also provides two versions of Matlab-specifc Interactive Applications: MatlabDesktop and MatlabServer. These apps would start a Matlab user interface directly.\nThe interactive / GUI access is very useful for debugging your Matlab code and for using Matlab to visualize your data. However, for production calculations that may take long time, or significant resources, or a large number of short Matlab tasks, we recommend using Matlab in batch mode. For running a MATLAB script in text mode, and/or a batch script, the following options can be used:\nmodule load matlab # to run a script your_matlab_script.m in batch mode, use -batch with the name only, no extension: matlab -nodisplay -nojvm -nodesktop -nosplash -batch your_matlab_script Here is an example of a SLURM job script to submit a job that uses serial MATLAB in batch mode:\nScript example for running serial MATLAB in batch mode run-matlab-job.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Matlab-Serial #SBATCH --partition=skylake # Load Matlab module/ module load matlab echo \u0026#34;Starting run at: `date`\u0026#34; # This is an example to run a code \u0026#34;matlab-code.m\u0026#34;. # Use the file without the extension \u0026#34;.m\u0026#34; matlab -nodisplay -nojvm -nodesktop -nosplash -singleCompThread -batch \u0026#34;matlab-code\u0026#34; echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Here is an example of a SLURM job script to submit a job that uses parallel (threaded) MATLAB in batch mode:\nScript example for running parallel (threaded) MATLAB in batch mode run-matlab-job-parallel.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Matlab-Parallel #SBATCH --partition=skylake # Load Matlab module/ module load matlab echo \u0026#34;Starting run at: `date`\u0026#34; # This is an example to run a code \u0026#34;matlab-code.m\u0026#34;. # Use the file without the extension \u0026#34;.m\u0026#34; # On this example, the script ois set to use 4 CPUs: # Adjust --cpus-per-task and --mem-per-cpu as needed. matlab -nodisplay -nojvm -nodesktop -nosplash -batch \u0026#34;matlab-code\u0026#34; echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; The option -singleCompThread is used for parallel (threaded) MATLAB. Please remove it if your code is serial. Use it only for parallel MATLAB and adjust the --cpus-per-task parameter as needed. However, when running matlab executable directly, each instance, GUI or command line, will consume a license unit. By submitting sufficiently many MATLAB jobs concurrently, there is a possibility to exhaust the entire University\u0026rsquo;s license pool. Thus, in many cases, it makes sense to use compiled, standalone MATLAB code runners (MCRs) instead (please refer to the MCR section below).\nUsing different BLAS/LAPACK in Matlab# Matlab relies heavily on linear algebra calculations. By default, Matlab would use Intel MKL for BLAS/LAPACK. Since version 2022a, it is possible to configure Matlab to use AMD AOCL libraries (BLIS and FLAME) instead, which may give some performance increase on AMD CPUs.\nmodule load matlab # lets check which linear algebra libraries are in use matlab -nodisplay -nojvm -batch \u0026#34;version -blas, version -lapack\u0026#34; # the answer should be something like \u0026#34;ans =Intel(R) oneAPI Math Kernel Library\u0026#34; # lets change to use AOCL export BLAS_VERSION=libblis-mt.so export LAPACK_VERSION=libflame.so # and check again matlab -nodisplay -nojvm -batch \u0026#34;version -blas, version -lapack\u0026#34; # \u0026#34;ans= AOCL-BLIS..., ans= AOCL-libFLAME... \u0026#34; # When the above environments are set, Matlab would use them instead of MKL More information can be found on this Matlab Knowledge Base Article .\nStandalone Matlab runners: MCR# MATLAB compiler, the mcc command can be used to compile a source code (.m file) into a standalone executable. There are couple of important considerations to keep in mind when creating an executable that can be run in the batch oriented, HPC environment. One is that there is no graphical display attached to your session and the other is that the number of threads used by the standalone application must be controlled.\nFor example, with code mycode.m a source directory src, with the compiled files being written to a directory called deploy, the following mcc command line (at the Linux shell prompt) could be used:\nmodule load matlab mkdir deploy cd src mcc -R -nodisplay -R -singleCompThread -m -v -w enable -d ../deploy mycode.m Note the option -singleCompThread has been included to limit the executable to just one computational thread.\nIn the deploy directory, an executable mycode will be created along with a script run_mycode.sh. These two files should be copied to the target machine where the code is going to be run as a batch job.\nNote that for every MATLAB module, there is a corresponding MCR version. The correspondance between the version is shown on the following table:\nMATLAB module MCR module matlab/R2020B2 mcr/R2020b matlab/R2022A mcr/R2022a matlab/R2023B mcr/R2023b matlab/R2024A mcr/R2024a Example of SLURM script: MCR# After the standalone executable mycode and corresponding script run_mycode.sh have been transferred to a directory on the target system on which they will be run, a batch job script needs to be created in the same directory. Here is an example batch job script.\nScript example for running MATLAB via MCR run-matlab-mcr.sh#!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 #SBATCH --job-name=Matlab-mcr-job #SBATCH --partition=skylake # the mcr version must correspond to Matlab version the code was compiled with! module load mcr/R2023b # Provide the MCR directory according to the compiler version used. # Presently $MCRROOT variable from the mcr module points to it echo \u0026#34;Running on host: `hostname`\u0026#34; echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; ./run_mycode.sh $MCRROOT \u0026gt; mycode_${SLURM_JOBID}.out echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; The job is then submitted as any ordinary SLURM job with the sbatch command. See the Running Jobs page for more information. If the above script is called run-matlab-mcr.sh, it could be submitted using:\nsbatch run-matlab-mcr.sh The specified --time and total memory (--mem-per-cpu) limits should be adjusted to appropriate values for your specific run. The option --partition is used to specify the partition to use for running the job. For more information, visit the page running jobs on Grex An important part of the above script is the location of the MATLAB Compiler Runtime (MCR) directory. This directory contains files necessary for the standalone application to run. The version of the MCR files specified must match the version of MATLAB used to compile the code (check the link for matching module and MCR versions).\nMATLAB on the Alliance\u0026rsquo;s clusters# For using MATLAB on the Alliance\u0026rsquo;s clusters, please visit the corresponding MATLAB page . While there is a wide MATLAB license accessible for all users on Fir , Rorqual and narval , using MATLAB on graham requires access to an external license . UManitoba users could use MATLAB on Nibi without additional settings.\nRelated links# MATLAB on the Alliance\u0026rsquo;s clusters MATLAB documentation Running jobs on Grex "},{"id":"12","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/nwchem/","permalinkTitle":"Homepage / Software Specific Notes / Running NWChem on Grex","title":"Running NWChem on Grex","content":"Introduction# NWChem is a Scalable, massive parallel and open source solution for large scale molecular simulations. NWChem is actively developed by a consortium of developers and maintained by the EMSL located at the Pacific Northwest National Laboratory (PNNL) in Washington State. The code is distributed as open source under the terms of the Educational Community License version 2.0 (ECL 2.0).\nSystem specific notes# To find out which versions of NWChem are available, use module spider nwchem .\nFor a version 7.2.2, and using the local SBEnv software stack, at the time of writing the following modules must be loaded:\nmodule load arch/avx512 intel-one/2024.1 openmpi/4.1.6 module load nwchem/7.2.2 or\nmodule load arch/avx512 aocc/4.2.0 openmpi/4.1.6 module load nwchem/7.2.2+aocl-4.2.0-64 or\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load nwchem/7.2.2+aocl-4.2.0-64 By inspecting the dependencies, you can see that the above versions are different with respect the compilers used to build NWChem: these are IntelOneAPI compiler, GNU GCC v 13, and AMD AOCC 4.2, correspondingly. The latter two versions might be slightly faster on AMD-based partitions (such as genoa, genlm), while the former will be faster on Intel CPUs (skylake and largemem partitions).\nThe NWChem on Grex was built with the ARMCI variant MPI-PR . Thus, NWCHem needs at least One process per node reserved for data communication. To run a serial job, one needs 2 tasks per node. To run a 22-core job over two whole nodes, one should ask for 2 nodes, 12 tasks per node. Simple number of tasks specification likely won\u0026rsquo;t work because of the chance of having a single-task node allocated by SLURM; so --nodes= --ntask-per-node specification is required.\nScript example for running NWChem on Grex run-nwchem.sh#!/bin/bash #SBATCH --ntasks-per-node=7 --nodes=2 --cpus-per-task=1 #SBATCH --mem-per-cpu=2000mb #SBATCH --time=0-3:00:00 #SBATCH --job-name=NWchem-dft-test # Adjust the number of tasks, time and memory required. # the above spec is for 12 compute tasks over two nodes. # Load the modules: module load arch/avx512 intel-one/2024.1 openmpi/4.1.6 module load nwchem/7.2.2 echo \u0026#34;Starting run at: `date`\u0026#34; which nwchem # Uncomment/Change these in case you want to use custom basis sets NWCHEMROOT=${MODULE_NWCHEM_PREFIX} export NWCHEM_NWPW_LIBRARY=${NWCHEMROOT}/data/libraryps export NWCHEM_BASIS_LIBRARY=${NWCHEMROOT}/data/libraries # In most cases SCRATCH_DIR would be on local nodes scratch # While results are in the same directory export NWCHEM_SCRATCH_DIR=$TMPDIR export NWCHEM_PERMANENT_DIR=`pwd` # Optional memory setting; note that this one or the one in your code # must match the #SBATCH --mem-per-cpu times compute tasks ! export NWCHEM_MEMORY_TOTAL=2000000000 # 24000 MB, double precision words only export MKL_NUM_THREADS=1 srun nwchem dft_feco5.nw \u0026gt; dft_feco5.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-nwchem.sh, it can be submitted with:\nsbatch run-nwchem.sh For more information, visit the page running jobs on Grex Related links# NWChem Running jobs on Grex "},{"id":"13","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/orca/","permalinkTitle":"Homepage / Software Specific Notes / Running ORCA on Grex","title":"Running ORCA on Grex","content":"Introduction# ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semi-empirical methods to DFT to single - and multi-reference correlated ab initio methods. It can also treat environmental and relativistic effects.\nUser Responsibilities and Access# ORCA is a proprietary software, even if it is free it still requires you to agree to the ORCA license conditions. We have installed ORCA on Grex, but to access the binaries, each of the ORCA users has to confirm they have accepted the license terms.\nThe procedure is as follow:\nFirst, register at ORCA forum . You will receive a first email to verify the email address and activate the account. Follow the instructions in that email. After the registration is complete, go to the ORCA download page, and accept the license conditions. You will get a second email stating that the \u0026ldquo;registration for ORCA download and usage has been completed\u0026rdquo;. Then contact us (via the Alliance support for example) quoting the ORCA email and stating that you also would like to access ORCA on Grex. The same procedure is applied to get access to ORCA on the Alliance\u0026rsquo;s clusters .\nSystem specific notes# To see the versions installed on Grex and how to load them, please use module spider orca and follow the instructions. Both ORCA-5 and ORCA-6 are available on Grex.\nTo load ORCA-5, use:\nmodule load arch/avx512 intel/2023.2 openmpi/4.1.6 orca/5.0.4 To load ORCA-6, use:\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 orca/6.0.1 Note:\nAfter updating the software stack, we removed the previous versions of ORCA. If needed, we can put back a particular version. However, ORCA users should use the versions released after (as they addresses some bugs of the two first releases like 5.0.0 and 5.0.1). Using ORCA with SLURM# In addition to the different keywords required to run a given simulation, users should make sure to set two additional parameters, like Number of CPUs and maxcore in their input files:\nmaxcore: This option sets the \u0026ldquo;max\u0026rdquo; memory per core. This is the upper limit under ideal conditions where ORCA can (and apparently often does) overshoot this limit. It is recommended to use no more than 75 % of the physical memory available. So, if the base memory is 4 GB per core, one could use 3 GB. The synatxe is as follow: %maxcore 3000 Basically, one can use 75 % of the total memory requested by SLURM divided by number of CPUs asked for.\nNumber of CPUs: ORCA can run in multiple processors with the aid of OpenMPI. All the modules are installed with the recommended OpenMPI version. To run ORCA in parallel, you can simply set the PAL keyword. For instance, a calculation using four processors requires:\n!HF DEF2-SVP PAL4 or 8:\n!HF DEF2-SVP PAL8 For more than eight processors (!PAL8), the explicit %PAL option has to be used:\n!HF DEF2-SVP %PAL NPROCS 16 END When running ORCA calculations in parallel, always use the full path to ORCA:\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 orca/6.0.1 ${MODULE_ORCA_PREFIX}/orca your-orca-input.in \u0026gt; your-orca-output.txt On the Alliance clusters, the path is defined via an environment variable EBROOTORCA that is set by the module. To use ORCA from the Alliance software stack, add the following lines to your script: module purge module load CCEnv module load arch/avx512 module load StdEnv/2023 module load gcc/12.3 openmpi/4.1.5 module load orca/6.0.1 ${EBROOTORCA}/orca your-orca-input.in \u0026gt; your-orca-output.txt Example on input file# Example of ORCA input file orca-example.inp# Benzene RHF Opt Calculation %pal nprocs 8 end ! RHF TightSCF PModel ! opt * xyz 0 1 C 0.000000000000 1.398696930758 0.000000000000 C 0.000000000000 -1.398696930758 0.000000000000 C 1.211265339156 0.699329968382 0.000000000000 C 1.211265339156 -0.699329968382 0.000000000000 C -1.211265339156 0.699329968382 0.000000000000 C -1.211265339156 -0.699329968382 0.000000000000 H 0.000000000000 2.491406946734 0.000000000000 H 0.000000000000 -2.491406946734 0.000000000000 H 2.157597486829 1.245660462400 0.000000000000 H 2.157597486829 -1.245660462400 0.000000000000 H -2.157597486829 1.245660462400 0.000000000000 H -2.157597486829 -1.245660462400 0.000000000000 * Simple script for running ORCA on Grex# This script assumes to have an input file orca.inp. The output is written to a file with the name orca.out. These files could be customized according to the names of your input files and whatever name you want for the output. Before submitting any ORCA job, please make sure to set the same number of CPUs in the input files according to your job script (as discussed above). Here is a simple script to run ORCA on Grex:\nScript example for running ORCA on Grex run-orca-grex-simple.sh#!/bin/bash #SBATCH --ntasks=8 #SBATCH --mem-per-cpu=2500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=\u0026#34;ORCA-test\u0026#34; # Adjust the number of tasks, memory walltime above as necessary # Load the modules: module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load orca/6.0.1 echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Running on $NUM_PROCS processors.\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; ${MODULE_ORCA_PREFIX}/orca orca.inp \u0026gt; orca.out echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Advanced script for running ORCA on Grex# This script assumes that you have one inpu file in the directory from where you submit the job. The script sets the output file based on the name of the input file. For example, using an input file my-orca.inp will generate an output with the name my-orca.out. To avoid wasting resources in case a user forgot to set the number of CPUs in the input file, the script generates a new input file and append the directorive for setting the number of CPUs in the input files, like %PAL NPROCS 8 END if the job script asks for 8 CPUs. Before using the script, please take the time to read it and understand how it works:\nScript example for running ORCA on Grex run-orca-grex.sh#!/bin/bash #SBATCH --ntasks=8 #SBATCH --mem-per-cpu=2500M #SBATCH --time=0-3:00:00 #SBATCH --job-name=\u0026#34;ORCA-test\u0026#34; # Adjust the number of tasks, memory walltime above as necessary # Load the modules: module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load orca/6.0.1 # Assign the input file: ORCA_INPUT_NAME=`ls *.inp | awk -F \u0026#34;.\u0026#34; \u0026#39;{print $1}\u0026#39;` ORCA_RAW_IN=${ORCA_INPUT_NAME}.inp # Specify the output file: ORCA_OUT=${ORCA_INPUT_NAME}.out echo \u0026#34;Current working directory is `pwd`\u0026#34; NUM_PROCS=$SLURM_NTASKS echo \u0026#34;Running on $NUM_PROCS processors.\u0026#34; echo \u0026#34;Creating temporary input file ${ORCA_IN}\u0026#34; ORCA_IN=${ORCA_RAW_IN}_${SLURM_JOBID} cp ${ORCA_RAW_IN} ${ORCA_IN} echo \u0026#34;%PAL nprocs $NUM_PROCS\u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; end \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} # The orca command should be called with a full path: echo \u0026#34;Starting run at: `date`\u0026#34; ${MODULE_ORCA_PREFIX}/orca ${ORCA_IN} \u0026gt; ${ORCA_OUT} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-orca-grex.sh, it can be submitted with:\nsbatch run-orca-grex.sh Sample Script for running NBO with ORCA on Grex# The input file should include the keyword NBO.\nScript example for running NBO with ORCA on Grex run-nbo-orca-grex.sh#!/bin/bash #SBATCH --ntasks=32 #SBATCH --mem-per-cpu=4000M #SBATCH --time=7-0:00:00 #SBATCH --job-name=nbo # Load the modules: module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load orca/6.0.1 module load nbo/nbo7-2021 export GENEXE=`which gennbo.i8.exe` export NBOEXE=`which nbo7.i8.exe` # Assign the input file: ORCA_INPUT_NAME=`ls *.inp | awk -F \u0026#34;.\u0026#34; \u0026#39;{print $1}\u0026#39;` ORCA_RAW_IN=${ORCA_INPUT_NAME}.inp # Specify the output file: ORCA_OUT=${ORCA_INPUT_NAME}.out echo \u0026#34;Current working directory is `pwd`\u0026#34; NUM_PROCS=$SLURM_NTASKS echo \u0026#34;Running on $NUM_PROCS processors.\u0026#34; echo \u0026#34;Creating temporary input file ${ORCA_IN}\u0026#34; ORCA_IN=${ORCA_RAW_IN}_${SLURM_JOBID} cp ${ORCA_RAW_IN} ${ORCA_IN} echo \u0026#34;%PAL nprocs $NUM_PROCS\u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; end \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} echo \u0026#34; \u0026#34; \u0026gt;\u0026gt; ${ORCA_IN} # The orca command should be called with a full path: echo \u0026#34;Starting run at: `date`\u0026#34; ${MODULE_ORCA_PREFIX}/orca ${ORCA_IN} \u0026gt; ${ORCA_OUT} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-orca-grex.sh, it can be submitted with:\nsbatch run-nbo-orca-grex.sh For more information, visit the page running jobs on Grex Related links# ORCA forum ORCA on the Alliance\u0026rsquo;s clusters ORCA input libraries ORCA common problems SCF convergence-issues ORCA tutorial Running jobs on Grex "},{"id":"14","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/r-program/","permalinkTitle":"Homepage / Software Specific Notes / Running R on Grex","title":"Running R on Grex","content":"Introduction# R is a system for statistical computation and graphics. It consists of a language plus a runtime environment with graphics, a debugger, access to certain system functions, and the ability to run programs stored in script files.\nAvailable R versions# Multiple versions of R are available. Use module spider r to find out the different versions of R:\n[~@yak ~]$ module spider r r/4.4.1+aocl-4.2.0 r/4.4.1+mkl-2019.5 r/4.4.1+mkl-2024.1 r/4.5.0+mkl-2024.1 r/4.5.0+openblas-0.3.28 To see how to load a particular version, run the command module spider r/\u0026lt;version\u0026gt;.\nAt the time of updating this page, the version r/4.5.0+mkl-2024.1 is available and can be loaded using:\nmodule load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 The R modules include a set of packages by default. If the package you are looking for is not included in the base modules, you will have to install them locally under your account after loading a specifi version of R. For more information, please have a look to the section Installing packages. The R interpreter can be launched by invoking the command R afyer loading the module:\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 [~@yak ~]$ R R version 4.5.0 (2025-04-11) -- \u0026#34;How About a Twenty-Six\u0026#34; Copyright (C) 2025 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type \u0026#39;license()\u0026#39; or \u0026#39;licence()\u0026#39; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type \u0026#39;contributors()\u0026#39; for more information and \u0026#39;citation()\u0026#39; on how to cite R or R packages in publications. Type \u0026#39;demo()\u0026#39; for some demos, \u0026#39;help()\u0026#39; for on-line help, or \u0026#39;help.start()\u0026#39; for an HTML browser interface to help. Type \u0026#39;q()\u0026#39; to quit R. [Previously saved workspace restored] \u0026gt; To see the packages installed, use the command installed.packages()\nTo run R code my-program.R interactively, use:\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 [~@yak ~]$ Rscript my-program.R The following shows an example of scripts to run R code on Grex using a batch job:\nScript example for running R on Grex run-r-serial.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=8:00:00 #SBATCH --job-name=R-Test # Load the modules: module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 echo \u0026#34;Starting run at: `date`\u0026#34; Rscript my-program.R echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Installing R packages# As mentioned above, the base modules have already some packages. However, users may have to install additional packages depending on what their program uses. Here is a quick example of installing a package called tidyverse unsing the R module r/4.5.0+mkl-2024.1.\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 [~@yak ~]$ R [~@yak ~]$ \u0026gt; Sys.setenv(\u0026#34;DISPLAY\u0026#34;=\u0026#34;:0.0\u0026#34;) [~@yak ~]$ \u0026gt; install.packages(\u0026#34;tidyverse\u0026#34;) The command Sys.setenv(\u0026quot;DISPLAY\u0026quot;=\u0026quot;:0.0\u0026quot;) is not required but it helps for not opening the GUI interface that might be slow over ssh.\nFor the first installation, there are two warning messages that ask questions about the personal library where R packages will be installed:\nWarning in install.packages(\u0026#34;tidyverse\u0026#34;) : \u0026#39;lib = \u0026#34;/home/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/r/4.5.0+mkl-2024.1/lib64/R/library\u0026#34;\u0026#39; is not writable Would you like to use a personal library instead? (yes/No/cancel) yes Would you like to create a personal library ‘/home/$USER/R/x86_64-pc-linux-gnu-library/4.5’ to install packages into? (yes/No/cancel) yes The message refers to a path where the base module is installed. As a user, you do not have write access to this directory. Therefore, the answers to the above questions should be yes and the packages will be installed under a directory /home/$USER/R under your account where you have read and write access. You can add all your packages as described above and they will be stored under the directory /home/$USER/R. Please keep this directory as it is if you want to keep your packages. If deleted, you will have to re-install again all your packages. It is recommended to keep track on the installation process (like list of modules and packages) in case you need to reproduce the installation another time.\nUnder the directory /home/$USER/R/x86_64-pc-linux-gnu-library, a sub directory named with the major version of R is created. For example, 4.5 for all R versions 4.5.x. In the example of installing the package tidyverse, no additional library or module was required. However, on many cases, the installation of R packages requires additional modules. The list is, but not limited to, gsl, netcdf, hdf5, udunits, geos, proj, gdal, tbb, \u0026hellip; etc. Usually, the error message gives a hint to the misssing library. The dependencies used to install any R package are also required to be loaded when running the job.\nAs an example, to install the package rjags, one need to load jags module in addition to R: [~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 [~@yak ~]$ module load jags [~@yak ~]$ R [~@yak ~]$ \u0026gt; Sys.setenv(\u0026#34;DISPLAY\u0026#34;=\u0026#34;:0.0\u0026#34;) [~@yak ~]$ \u0026gt; install.packages(\u0026#34;rjags\u0026#34;)\nPackages from CRAN mirrors# After loading r module and all the required dependencies, the installation of R package from CRAN is invoked by the command install.packages(\u0026ldquo;name of the package\u0026rdquo;) like in the following example for installing rjags. This package requires an additional external module called jags:\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 [~@yak ~]$ module load jags [~@yak ~]$ R [~@yak ~]$ \u0026gt; Sys.setenv(\u0026#34;DISPLAY\u0026#34;=\u0026#34;:0.0\u0026#34;) [~@yak ~]$ \u0026gt; install.packages(\u0026#34;rjags\u0026#34;) It is possible to bundle more than one package in the same command line using:\ninstall.packages(c(\u0026#34;packae1\u0026#34;, \u0026#34;package2\u0026#34;, \u0026#34;package3\u0026#34;)) Packages from GitHub repositories# There are R packages hosted on GitHub and their installation require to first install devtools or remotes . First, one need to add the package devtools or remotes if not installed already and make them available in the R prompt before using them to install other packages.\nHere is an example used to install stampr with devtools:\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 gdal [~@yak ~]$ R [~@yak ~]$ \u0026gt; Sys.setenv(\u0026#34;DISPLAY\u0026#34;=\u0026#34;:0.0\u0026#34;) [~@yak ~]$ \u0026gt; install.packages(\u0026#34;devtools\u0026#34;) [~@yak ~]$ \u0026gt; library(\u0026#34;devtools\u0026#34;) [~@yak ~]$ \u0026gt; devtools::install_github(\u0026#34;jedalong/stampr\u0026#34;) During the installation, you may get questions about updating packages and choose which packages to update. Similar procedure could be used to install a package with ```remotes`:\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 gdal geos [~@yak ~]$ R [~@yak ~]$ \u0026gt; Sys.setenv(\u0026#34;DISPLAY\u0026#34;=\u0026#34;:0.0\u0026#34;) [~@yak ~]$ \u0026gt; install.packages(\u0026#34;remotes\u0026#34;) [~@yak ~]$ \u0026gt; library(\u0026#34;remotes\u0026#34;) [~@yak ~]$ \u0026gt; remotes::install_github(\u0026#34;jedalong/stampr\u0026#34;) Bioconductor packages# The following example shows how to install the packages \u0026ldquo;edgeR\u0026rdquo;, \u0026ldquo;qvalue\u0026rdquo;, \u0026ldquo;GenomicAlignments\u0026rdquo;, \u0026ldquo;GenomicFeatures\u0026rdquo; using BiocManager :\n[~@yak ~]$ module load arch/avx512 gcc/13.2.0 r/4.5.0+mkl-2024.1 [~@yak ~]$ R [~@yak ~]$ \u0026gt; Sys.setenv(\u0026#34;DISPLAY\u0026#34;=\u0026#34;:0.0\u0026#34;) [~@yak ~]$ \u0026gt; if (!require(\u0026#34;BiocManager\u0026#34;, quietly = TRUE)) [~@yak ~]$ + install.packages(\u0026#34;BiocManager\u0026#34;) [~@yak ~]$ \u0026gt; BiocManager::install(c(\u0026#34;edgeR\u0026#34;, \u0026#34;qvalue\u0026#34;, \u0026#34;GenomicAlignments\u0026#34;, \u0026#34;GenomicFeatures\u0026#34;)) External Links# R documentation "},{"id":"15","rootTitleIndex":"7","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-person-running fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/slurm-partitions/","permalinkTitle":"Homepage / Running jobs on Grex / Slurm partitions","title":"Slurm partitions","content":"Partitions# The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a \u0026ldquo;partition\u0026rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who \u0026ldquo;owns\u0026rdquo; the hardware as well.\nThere is no fully automatic selection of partitions, other than the default skylake for most of the users for the short jobs. For the contributors\u0026rsquo; group members, the default partition will be their contributed nodes. Thus, in many cases users have to specify the partition manually when submitting their jobs!\nOn the special partition test, oversubscription is enabled in SLURM, to facilitate better turnaround of interactive jobs.\nJobs cannot run on several partitions at the same time; but it is possible to specify more than one partition, like in --partition=skylake,largemem, so that the job will be directed by the scheduler to the first partition available.\nJobs will be rejected by the SLURM scheduler if partition\u0026rsquo;s hardware and requested resources do not match (that is, asking for GPUs on compute, largemem or skylake partitions is not possible). So, in some cases, explicitly adding --partition= flag to SLURM job submission is needed.\nJobs that require stamps-b or gpu or other GPU-containing partitions, have to use GPUs (with a corresponding TRES flag like --gpus=), otherwise they will be rejected; this is to prevent bogging up the expensive GPU nodes with CPU-only jobs!\nCurrently, the following partitions are available on Grex:\nGeneral purpose CPU partitions# Partition Nodes CPUs/Node CPUs Mem/Node Notes skylake 42 52 2184 187 Gb CascadeLakeRefresh largemem 12 40 480 380 Gb CascadeLake genoa 27 192 5184 750 Gb AMD EPYC 9654 genlm 3 192 576 1500 Gb AMD EPYC 9654 test 1 18 36 512 Gb CascadeLake All CPU partitions support a common subset of the AVX512 architecture. However, AMD EPYC CPUs have Zen4 architecture with extended set of AVX512 commands compared to CascadeLake. Thus host-optimized code compiled on genoa or genlm nodes may throw \u0026lsquo;illegal instruction\u0026rsquo; error on skylake and largemem nodes\nGeneral purpose GPU partitions# Partition Nodes GPU type CPUs/Node Mem/Node Notes gpu 2 4 - V100/32GB 32 187 Gb Intel AVX512 CPU, NVLink lgpu 1 2 - L40s/48GB 64 380 Gb AMD AVX512 CPU Contributed CPU partitions# Partition Nodes CPU type CPUs/Node Mem/Node Notes mcordcpu 1 5 AMD EPYC 9634 84-Core 168 1500 Gb - chrim 4 AMD EPYC 9654 96-Core 192 750 Gb - chrimlm 1 AMD EPYC 9654 96-Core 192 1500 Gb - Contributed GPU partitions# Partition Nodes GPU type CPUs/Node Mem/Node Notes stamps 2 3 4 - V100/16GB 32 187 Gb AVX512 CPU, NVLink livi 3 1 16 -V100/32GB 48 1500 Gb NVSwitch, AVX512 CPU agro 4 2 2 - A30/24GB 24 250 Gb AMD AVX2 CPU mcordgpu 5 2 4 - A30/24GB 32 512 Gb AMD AVX2 CPU Note that newer GPU nodes with NVidia A30 GPUs have an older CPU architecture, up to AVX2 instruction set. Host-optimized code compiled on a CascadeLake or AMD Genoa CPU which have AVX512 instruction set will throw \u0026lsquo;illegal instruction\u0026rsquo; errors on the AVX2 GPU nodes.\nPreemptible partitions# The following preemptible partition are set for general use of the contributed nodes:\nPartition Contributed by stamps-b Prof. R. Stamps livi-b Prof. L. Livi agro-b Faculty of Agriculture genoacpu-b Spans all contributed AMD CPU nodes from Prof M. Cunha Cordeiro and CHRIM mcordgpu-b Prof M. Cunha Cordeiro The following partitions (skylake, largemem, test, gpu, lgpu) are generally accessible. The other partitions (stamps, livi, agro, mcordcpu and mcordgpu, chrim and chrimlm ) are open only to the contributor\u0026rsquo;s groups.\nOn the contributed partitions, the owners\u0026rsquo; group has preferential access. However, users belonging to other groups can submit jobs to one of the preemptible partitions (ending with -b) to run on the contributed hardware as long as it is unused, on the condition that their jobs can be preempted (that is, killed) should owners\u0026rsquo; jobs need the hardware. There is a minimum runtime guaranteed to preemptible jobs, which is as of now 1 hour. The maximum wall time for the preemptible partition is set per partition (and can be seen in the output of the sinfo command). To have a global overview of all partitions on Grex, run the custom script partition-list from your terminal.\nNote that the owners\u0026rsquo; and corresponding preeemptible partitions do overlap! This means, that owners\u0026rsquo; group should not submit their jobs to both of the contributed and the corresponding preemptible partitions, otherwise their jobs may preeempt their other jobs!\nmcordcpu CPU nodes contributed by Prof. Marcos Cunha Cordeiro\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nstamps: GPU nodes contributed by Prof. R. Stamps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nlivi: GPU node contributed by Prof. L. Livi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nagro: GPU node contributed by the Faculty of Agriculture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmcordgpu GPU nodes contributed by Prof. Marcos Cunha Cordeiro\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"16","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/vasp/","permalinkTitle":"Homepage / Software Specific Notes / Running VASP on Grex","title":"Running VASP on Grex","content":"Introduction# VASP is a massively parallel plane-wave solid state DFT code. On Grex, it is available only for the research groups that hold VASP licenses. To get access, PIs would need to send us a confirmation email from the VASP vendor, detailing the status of their license and a list of users allowed to use it.\nSystem specific notes# To find out which versions of VASP are available, use module spider vasp .\nAt the time of reviewing this page, the following versions are avaiable: vasp/6.1.2-sol and vasp/6.3.2-vtst.\nTo load the module vasp/6.1.2-sol use:\nmodule load arch/avx512 intel/2023.2 intelmpi/2021.10 module load vasp/6.1.2-sol To load the module vasp/6.3.2-vtst use:\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load vasp/6.1.2-6.3.2-vtst or\nmodule load arch/avx512 intel/2023.2 intelmpi/2021.10 module load vasp/6.1.2-6.3.2-vtst The first module was build with GCC and the later with Intel compiler.\nThere are three executables for VASP CPU version: vasp_gam , vasp_ncl , and vasp_std. Refer to the VASP manual as to what these mean. An example VASP SLURM script using the standard version of the VASP binary is below:\nThe following script assumes that VASP6 inputs (INCAR, POTCAR etc.) are in the same directory as the job script.\nScript example for running VASP Grex run-vasp.sh#!/bin/bash #SBATCH --ntasks=16 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=3400M #SBATCH --time=0-3:00:00 #SBATCH --job-name=vasp-test # Adjust the number of tasks, time and memory required. # The above spec is for 16 compute tasks, using 3400 MB per task . # Load the modules: module load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load vasp/6.1.2-6.3.2-vtst echo \u0026#34;Starting run at: `date`\u0026#34; which vasp_std export MKL_NUM_THREADS=1 srun vasp_std \u0026gt; vasp_test.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Assuming the script above is saved as run-vasp.sh, it can be submitted with:\nsbatch run-vasp.sh For more information, visit the page running jobs on Grex Related links# VASP manual Running jobs on Grex "},{"id":"17","rootTitleIndex":"4","rootTitle":"Connecting to Grex","rootTitleIcon":"fa-solid fa-plug fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex","permalink":"/grex-docs/connecting/mfa/","permalinkTitle":"Homepage / Connecting to Grex / Using Multi-Factor Authentication (MFA) on Grex","title":"Using Multi-Factor Authentication (MFA) on Grex","content":"Important Notices# As of January 9, 2024 MFA is enforced for all UManitoba/Grex users. We appreciate your attention to this matter and encourage you to enroll at your earliest convenience to ensure a smooth transition. Should you have any questions or require further assistance, please don\u0026rsquo;t hesitate to reach out to your local HPC team. If some of your work relies on unattended connections or automations that may be disrupted by MFA, we ask that you do not enroll yet, and instead contact us for technical support, so that we can work with you on a solution. Please note that enrolling in MFA for Grex, makes MFA active and enforced also on the Alliance National systems as well, and vice versa. If this does not work for you, please contact us . Multi-Factor Authentication# Both Digital Research Alliance of Canada and our UManitoba HPC systems are using a multifactor authentication (MFA) system. MFA adds an additional layer of security to the traditional password-based and SSH keys authentications by requiring a second factor, known as \u0026ldquo;something you have\u0026rdquo;. The Cisco Duo product was chosen as the provider for this additional authentication factor. Grex is using the Duo instance provided by the Alliance. The available additional factors and even the MFA provider are not the same as those used by the University of Manitoba for platforms such as UM Intranet and University MS Office products.\nOn Grex and Alliance systems, the following factors are enabled:\nDuo smartphone app (Android and iOS) Yubico Yubikey cybersecurity USB key device One-time codes (recommended as a backup, available only after one of the above primary factors is already enabled) Enrollment# Enrollment into the Alliance Duo is through CCDB , this enables the MFA requirement on every SSH login on both Grex and Alliance systems .\nEnrollment in MFA is required to access the clusters. You must follow these steps to start using Grex and Alliance systems:\nlogin into CCDB with your credentials from the top menu choose \u0026ldquo;My Account\u0026rdquo; =\u0026gt; \u0026ldquo;Multifactor Authentication Management\u0026rdquo; register a new device OPTIONAL BUT RECOMMENDED at the bottom of the same page, use the \u0026ldquo;Generate 10 codes\u0026rdquo; button to generate 10 rescue codes (you must print/save these codes in a safe location and never disclose them to anyone) additional information on the Alliance official wiki The following images should better explain the previous steps.\nData transfer on MFA-enabled systems# MobaXTerm# Upgrade to version 23.1 or more recent Test the connection If problems persist, try a combination of the following:\nUse a keypair instead of password Use \u0026ldquo;SCP (normal speed)\u0026rdquo; instead of \u0026ldquo;SFTP\u0026rdquo; (under \u0026ldquo;Advanced SSH settings\u0026rdquo;) WinSCP# Upgrade to version 6.1.1 or more recent Use a keypair instead of password Use the SFTP app insead of SSH browser app. Test the connection CyberDuck# CyberDuck supports MFA for SFTP out of the box.\nUpgrade to version 9 or newer. Optionally, provide SSH private key instead of password PyCharm# Use a keypair instead of password Test the connection FileZilla# FileZilla will ask the password and second factor each time a transfer is initiated, because by default transfers use independent connections which are closed automatically after some idle time.\nTo avoid entering the password and second factor multiple times, you can limit the number of connections to each site to \u0026ldquo;1\u0026rdquo; in \u0026ldquo;Site Manager\u0026rdquo; =\u0026gt; \u0026ldquo;Transfer Settings tab\u0026rdquo;, note that then you\u0026rsquo;ll lose the ability to browse the server during transfers.\nLaunch FileZilla and select \u0026ldquo;Site Manager\u0026rdquo; From the \u0026ldquo;Site Manager\u0026rdquo;, create a new site (or edit an existing one) On the \u0026ldquo;General\u0026rdquo; tab, specify the following: Protocol: \u0026ldquo;SFTP – SSH File Transfer Protocol\u0026rdquo; Host: the cluster login hostname Logon Type: \u0026ldquo;Interactive\u0026rdquo; User: your CCDB username On the \u0026ldquo;Transfer Settings\u0026rdquo; tab, specify the following: Limit number of simultaneous connections: checked Maximum number of connections: 1 Select \u0026ldquo;OK\u0026rdquo; to save the connection Test the connection Ubuntu Nautilus File Manager# Nautilus uses GVFS to allow browsing remote locations through ssh, but it doesn\u0026rsquo;t support MFA login.\nAs a workaround, you can use ssh connection multiplexing, with a (not) well-known location for the control socket:\nFor GVFS versions 1.47 and later: $ ssh -fNMS \u0026#34;$XDG_RUNTIME_DIR/gvfsd-sftp/%C\u0026#34; ccdb_username@grex_login_hostname $ gio mount sftp://ccdb_username@grex_login_hostname -- USE NAUTILUS TO BROWSE REMOTE FILES -- For GVFS before version 1.47 (The “echo” command must be execute only once): $ echo -e \u0026#39;Host *\\n\\tControlPath ~/.ssh/S.%r@%h:%p\u0026#39; \u0026gt;\u0026gt; ~/.ssh/config $ ssh -fNM ccdb_username@grex_login_hostname $ gio mount sftp://ccdb_username@grex_login_hostname -- USE NAUTILUS TO BROWSE REMOTE FILES -- Note that if the master connection gets closed for any reason (e.g.: timeout, lost connection, \u0026hellip;), then it must be restarted manually to keep browsing the remote location.\nConfiguring OpenSSH clients for longer MFA sessions# For the users of OpenSSH clients on Linux, MacOS, and Windows (see SSH ) it is possible to reduce the number of second-factor challenges by caching them between sessions for a period of time. This can be configured in the OpenSSH client\u0026rsquo;s user config as follows.\nEdit or create the file $HOME/.ssh/config that contains a config similar to this :\nHost yak.hpc.umanitoba.ca ControlPath ~/.ssh/cm-%r@%h:%p ControlMaster auto ControlPersist 10m The example above uses the Yak login node of Grex; replace it with other hostnames as required, or add sections for more than one host if needed.\n"},{"id":"18","rootTitleIndex":"4","rootTitle":"Connecting to Grex","rootTitleIcon":"fa-solid fa-plug fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex","permalink":"/grex-docs/connecting/ssh/","permalinkTitle":"Homepage / Connecting to Grex / Connecting to Grex with SSH","title":"Connecting to Grex with SSH","content":"SSH# Most of the work on shared HPC computing systems is done via Linux command line / Shell. To connect, in a secure manner, to a remote Linux system, you would like to use SSH protocol. You will need to have:\naccess to the Internet that lets SSH ports open. a user account on Grex (presently, it is a Compute Canada (an Alliance) Account). and an SSH client for your operating system. If you are not sure what your account on Grex is, check Getting Access . You will also need the DNS name of Grex Which is grex.hpc.umanitoba.ca\nSSH clients# Mac OS X# Mac OS X has a built-in OpenSSH command line client. It also has a full-fledged UNIX shell. Therefore, using SSH under Mac OS is not different from Linux. In any terminal, ssh (as well as scp, sftp) just works with one caveat: for the support of X11 tunneling, some of the Mac OS X versions would require the XQuartz package installed.\nssh -XY username@grex.hpc.umanitoba.ca Please remember to change username in the above command with your Alliance ( CCDB ) user name.\nLinux# Linux provides the command line SSH package, OpenSSH, which is installed by default in most of the Linux distributions. If not, or you are using a very minimal Linux installation, use your package manager to install the OpenSSH package. In any terminal window ssh (as well as scp , sftp ) commands should work. To connect to Grex, use:\nssh -XY username@grex.hpc.umanitoba.ca Similarly to the above, please remember to change username in the above command with your Alliance ( CCDB ) user name.\nSSH keys# To avoid entering passwords and to increase the security of your SSH connections, you can use SSH key-based authentication on Grex and the Alliance systems. In key-based authentication, a pair of cryptographic keys (a private key and a public key) is used. The public key can be shared with anyone, but the private key is secret and should be kept securely, only on your client machine.\nTo connect to an HPC machine, the system must have your public key. The preferred method for providing this is to deposit your public key (or keys) in the Alliance\u0026rsquo;s CCDB. The Alliance (Compute Canada) user documentation has several pages on managing SSH keys .\nYou can also manage your SSH keys (adding key pairs, editing known_hosts, etc.) locally on an HPC system in the $HOME/.ssh directory. However, centralized key management via CCDB is the recommended approach. Additionally, for using robot nodes for automation workloads, SSH keys via CCDB are required, as password authentication and local SSH keys will not work.\nWindows# Windows has a very diverse infrastructure for SSH (and Linux support in general). You would like to pick one of the options below and connect to grex.hpc.umanitoba.ca with your Alliance username and password.\nPutty, WinSCP and VCXsrv# The (probably the most popular) free software combination to work under Windows are:\nPuTTy SSH client: download PuTTy WinSCP graphical SFTP client: download WinSCP A free X11 server for Windows: download VCXSrv WinSCP interacts with PuTTy, so you can configure it to open SSH terminal windows from the WinSCP client. For X11 forwarding, make sure the \u0026ldquo;X11 tunneling\u0026rdquo; is enabled in PuTTy\u0026rsquo;s session settings, and VCXSrv is running (it sits in the system tray and does nothing unless you start a graphical X11 application).\nThe Alliance wiki has a PuTTY documentation page with some useful screenshots.\nMobaXterm# There is a quite popular package: MobaXterm. It is not open source, but has a limited free version MobaXterm .\nPlease check out the Alliance\u0026rsquo;s documentation on MobaXterm here All Windows versions, CygWin shell# There is a way to use Linux command shell tools under Windows. Cygwin . When OpenSSH package is installed, you can use OpenSSH\u0026rsquo;s command line tools like ssh, scp and sftp as if you were under Linux:\nssh -Y username@grex.hpc.umanitoba.ca Windows 10+, WSL subsystem# There is a Linux Subsystem for Windows which allows you to run a containerized instance of Linux (Ubuntu, for example) from under Windows ver. 10 and later. Refer to MS documentation on enabling WSL . Then, you will have the same OpenSSH under Linux.\nIt is actually possible to run X11 applications from WSL as well; you would need to get VCXSrv running, on the Windows side, and DISPLAY variable set on the WSL Linux side.\nWindows 10+, native OpenSSH package# Actually, some of the Windows 10 and later versions have OpenSSH as a standalone package. Refer to corresponding MS documentation on enabling OpenSSH . If it works with your version of Windows 10, you should have the OpenSSH command line tools like ssh, scp and sftp in the Windows command line, as if you were under Linux.\nThe original SSH Secure Shell client# The original SSH Secure Shell and Secure FTP client from this website(www.ssh.fi ) is now obsolete. It has been unmaintained since 2001 and may not work with the newest SSH keys/encryption mechanisms and does not have any security updates since 2001. We do not support it and advise users to switch to one of the other more modern clients listed above.\nUsing command line# What to do after you connect? You will be facing a Linux shell, most likely BASH. There is plenty of online documentation on how to use it, HPC Carpentries , Compute Canada\u0026rsquo;s SSH documentation page , Bash Guide for Beginners and simple googling for the commands.\nYou would probably like to explore software via Modules , and learn how to submit jobs .\nInternal links# Using modules Running jobs External links# HPC Carpentries The Alliance SSH documentation page Bash Guide for Beginners Linux introduction "},{"id":"19","rootTitleIndex":"7","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-person-running fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/interactive-jobs/","permalinkTitle":"Homepage / Running jobs on Grex / How to run interactive jobs on Grex?","title":"How to run interactive jobs on Grex?","content":"Interactive work# The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:\nedit files compile codes and run short interactive calculations. configure and build programs (limit the number of threads to 4: make -j4) submit and monitor jobs transfer and/or download data run short tests, \u0026hellip; etc. In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].\nTo not affect other user\u0026rsquo;s experience, please be aware that we reserve to ourselves the right to terminate any intensive process running on the login nodes without prior warning. It is very easy to cause resource congestion on a shared Linux server. Therefore, all production calculations should be submitted in the batch mode, using our resource management system, SLURM. It is possible to submit so-called interactive jobs: a job that creates an interactive session but actually runs on dedicated CPUs (and GPUs if needed) on the compute nodes rather than on the login servers (or head nodes). Note that login nodes do not have GPUs.\nSuch mode of running interactive computations ensures that login nodes are not congested. A drawback is that when a cluster is busy, getting an interactive job to start will take some queuing time, just like any other job. So, in practice interactive jobs are to be short and small to be able to utilize backfill-able nodes. This section covers how to run such jobs with SLURM.\nNote that manual SSH connections to the compute nodes without having active running jobs is forbidden on Grex.\nInteractive batch jobs# To request an interactive job, the salloc command should be used. These jobs are not limited to a single node jobs; any nodes/tasks/gpus layout can be requested by salloc in the same way as for sbatch directives. However, to minimize queuing time, usually a minimal set of required resources should be used when submitting interactive jobs (less than 3 hours of wall time, less than 4 GB memory per core, \u0026hellip; etc). Because there is no batch file for interactive jobs, all the resource requests should be added as command line options of the salloc command. The same logic of --nodes=, --ntasks-per-node= , --mem= and --cpus-per-task= resources as per batch jobs applies here as well.\nFor a threaded SMP code asking for 6 cores for two hours:\nsalloc --nodes=1 --ntasks=1 --cpus-per-task=6 --mem=12000M --partition=skylake --time=0-2:00:00 For an MPI jobs asking for 48 tasks, irrespectively of the nodes layout:\nsalloc --ntasks=48 --mem-per-cpu=2000M --partition=skylake --time=0-2:00:00 Similar to batch jobs , specifying a partition with --partition= is required. For more information, see the page .\nInteractive GPU jobs# The difference for GPU jobs is that they would have to be directed to a node with GPU hardware:\nThe GPU jobs should run on the nodes that have GPU hardware, which means you\u0026rsquo;d always want to specify --partition=gpu or --partition=stamps-b.\nSLURM on Grex uses the so-called \u0026ldquo;GTRES\u0026rdquo; plugin for scheduling GPU jobs, which means that a request in the form of --gpus=N or --gpus-per-node=N or --gpus-per-task=N is required. Note that both partitions have up to four GPU per node, so asking more than 4 GPUs per node, or per task, is nonsensical. For interactive jobs, it makes more sense to use ia single GPU in most of the cases.\nFor an interactive session using two hours of one 16 GB V100 GPU, 4 CPUs and 4000MB per cpu:\nsalloc --gpus=1 --cpus-per-task=4 --mem-per-cpu=4000M --time=0-2:00:00 --partition=stamps-b Similarly, for a 32 GB memory V100 GPU:\nsalloc --gpus=1 --cpus-per-task=4 --mem-per-cpu=4000M --time=0-2:00:00 --partition=gpu Graphical jobs# What to do if your interactive job involves a GUI based program? You can SSH to a login node with X11 forwarding enabled, and run it there. It is also possible to forward the X11 connection to compute nodes where your interactive jobs run with --x11 flag to salloc:\nsalloc --ntasks=1 --x11 --mem=4000M To make it work you\u0026rsquo;d want the SSH session login node is also supporting graphics: either through the -Y flag of ssh (or X11 enabled in PuTTy). If you are using Mac OS, you will have to install XQuartz to enable X11 forwarding.\nYou may also try to use OpenOnDemand portal on Grex.\nOpenOnDemand portal on Grex: click on the image to read the documentation "},{"id":"20","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/using-modules/","permalinkTitle":"Homepage / Software and Applications / Modules and software stacks","title":"Modules and software stacks","content":"Introduction# On a Linux server or a Linux desktop, software can be installed in one of the standard locations, such as /usr/bin. This is where most of the system-level software binaries can be found. For custom user-built software, it is a good practice to install it separately from the standard location, to avoid potential conflicts and make changes and uni-installation possible. One of the common locations would be under /usr/local/, as in /usr/local/My_Custom_software/ , or under /opt (/opt/My_Other_custom_software).\nThere is a Linux/UNIX Filesystem Hierarchy Standard (FHS) that describes the conventions on where the files are.\nThe Linux operating system \u0026ldquo;finds\u0026rdquo; the software executable (binaries and/or scripts) using a special environment variable called PATH. By default, the variable has all the standard locations like /usr/bin already. Custom software installed under other locations usually requires modifying the PATH variable to include these specific locations. Other variables might be required for a software to work, LD_LIBRARY_PATH for example, shows where to look for dynamic or static libraries. For the source code libraries variables, like CPATH and CXXFLAGS would tell where to find so-called header files and modules.\nWhile it is possible for a Linux user to edit the PATH and related environment variables in their login script or application scripts by hand, it is an error-prone process. A typo in PATH may render the whole system binaries inaccessible, for example. Another problem is the maintenance of multiple versions of the same package, as these would have the same names for binaries and no good result would happen if more than one version is in the same PATH.\nTo solve the problem, a tool called \u0026ldquo;Modules\u0026rdquo; was developed in the HPC ecosystem:\nModules allow for clean and dynamic modification of the user\u0026rsquo;s environment (environment variables like PATH, LD_LIBRARY_PATH, CPATH, and almost anything else) in a Linux user session. Modules can be \u0026ldquo;loaded\u0026rdquo; and \u0026ldquo;unloaded\u0026rdquo;, modifying the environment in a clean and atomic way. Modules can also be used to find available software and to control software dependencies in a hierarchical way. Modules on HPC machines are usually provided system-wide for all users; but individual users can create their own private modules to be used along with the system software stacks. There are two main implementations of Modules: the original TCL Modules and the hierarchical Lmod Modules developed at TACC. Grex and the Alliance systems are using Lmod. That is, modules are files written in a special syntax in either TCL or Lua languages. There is a command, module, that works on the module files.\nUsing Modules on HPC machines# First check if the module command exists and works.\n# This command should work: module help Many HPC systems load some modules automatically, by default. To list the modules that are loaded, use:\n# This command might return some modules, or # return nothing if there is no default modules. module list All modules can be unloaded with the purge commmand, except the modules that are \u0026ldquo;sticky\u0026rdquo; and would refuse to unload, unless you add the option --force.\nmodule purge The most useful command for which Modules exist is load. The command loads module into the current environment. The unload commands does the opposite action.\n# Lets have a clean envronment: module purge # We want minimap2 for our Genomics work. Is it in the PATH? which minimap2 # Nothing, lets load the module: module load minimap2 module list # Now there should be a working minimap2 executable in the PATH: which minimap2 minimap2 --help # If we unload the module, we should have no minimap2 in our environment: module unload minimap2 which minimap2 # Nothing. How do we know if there is a minimap2 module, and it is the software/version we need?\nThere is a command module avail that shows modules currently available to load, and commands module whatis and module help to (hopefully) provide a useful description of a given module file.\nmodule list module help minimap2 module avail Finding software using modules: Hierarchies# On most HPC systems, using Lmod command module avail is less useful because of the existence of a module hierarchy. Lmod enforces a hierarchy of modules based on module dependencies. A typical dependency for a code is a compiler suite/version and an MPI implementation used to compile or build the program.\nTo avoid software conflicts, it is necessary that the same compiler/version (runtime, dynamic libraries) the code is built with, is loaded at the time of the execution of the code. Module hierarchies hide the software modules for which the dependencies are not yet loaded. Some of the modules make the \u0026ldquo;Core\u0026rdquo; level of the hierarchy and they do not require dependencies.\nOne of the kinds of modules which are usually at \u0026ldquo;Core\u0026rdquo; level are, naturally, compilers and compiler toolchains. Another kind may be \u0026ldquo;CPU architecture\u0026rdquo;.\nTo find the modules in a hierarchy of modules, the module spider command is provided. Usually, it takes two invocations of module spider, first only with a software name to find available versions, and then with software name/version to find the required dependencies.\n# Lets find a module for NWchem: module avail nwchem # Nothing, now run module spider: module spider nwchem # Returns a number of versions, including 7.2.2 module spider nwchem/7.2.2 # Knowing the dependencies, we can load them module load arch/avx512 intel-one/2024.1 openmpi/4.1.6 # Now that dependencies are loaded, the module for NWchem is visible module avail nwchem module whatis nwchem module load nwchem/7.2.2 # can start using nwchem on an input file, let it be siosi8.nw which nwchem mpiexec -np 4 nwchem siosi8.nw Switching between the Software stacks# On some HPC systems, there exists more than one Software stack, and the stacks would form separate module hierarchies.\nOften it is used by systems that have a local and an external software stack. An example of the external software stack is the Alliance/ComputeCanada software stack (CCEnv).\nOn Grex, there are two main software stacks: SBEnv (the default) and CCEnv (the optional Alliance software stack).\nThe command module spider would find software only within the software stack. The stack itself is a module, and it sits above any Core level in the hierarchy. It is necessary first to purge all the loaded modules and load the software stack, or environment. The stack modules (SBEnv, CCEnv) are sticky and cannot be purged. They are mutually exclusive (i.e., two stacks cannot be loaded at the same time).\nmodule purge module list # By default the above on Grex shows SBEnv: module help CCEnv module help SBEnv # Lets switch to CCEnv, the ComputeCanada software stack: module load CCEnv # we can proceed load \u0026#34;standard environments\u0026#34;; # on Grex it is best to use the latest one. Note the avx512 architecture. module load arch/avx512 StdEnv/2023 # Is there an NWchem module? module spider nwchem # etc. At this point, equipped with module spider and the knowledge of the available software stacks, the reader should be well equipped to find any centrally available software on a given HPC system.\nIn some cases, it may help to get more information about a given module, like the path to the installation directory and the location of the libraries and headers. If that\u0026rsquo;s the case, the command module show could be used. In the following example, we load boost and show the variavles that points to the installation directory:\nmodule load arch/avx512 gcc/13.2.0 boost/1.85.0 module show boost/1.85.0 ---------------------------------------------------------------------------------------------------------------------------------------- /global/software/alma8/sb/modules/arch-avx512-gcc-13.2.0/boost/1.85.0: ---------------------------------------------------------------------------------------------------------------------------------------- prepend_path(\u0026#34;CMAKE_PREFIX_PATH\u0026#34;,\u0026#34;/global/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/boost/1.85.0\u0026#34;) prepend_path(\u0026#34;CPATH\u0026#34;,\u0026#34;/global/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/boost/1.85.0/include\u0026#34;) prepend_path(\u0026#34;LIBRARY_PATH\u0026#34;,\u0026#34;/global/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/boost/1.85.0/lib\u0026#34;) prepend_path(\u0026#34;LD_LIBRARY_PATH\u0026#34;,\u0026#34;/global/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/boost/1.85.0/lib\u0026#34;) setenv(\u0026#34;MODULE_BOOST_PREFIX\u0026#34;,\u0026#34;/global/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/boost/1.85.0\u0026#34;) whatis(\u0026#34;Description: Boost provides free peer-reviewed portable C++ source libraries, emphasizing libraries that work well with the C++ Standard Library\u0026#34;) whatis(\u0026#34;Homepage: http://www.boost.org\u0026#34;) setenv(\u0026#34;BOOST_ROOT\u0026#34;,\u0026#34;/global/software/alma8/sb/opt/arch-avx512-gcc-13.2.0/boost/1.85.0\u0026#34;) help([[ Boost provides free peer-reviewed portable C++ source libraries, emphasizing libraries that work well with the C++ Standard Library. Boost libraries are intended to be widely useful, and usable across a broad spectrum of applications. The Boost license encourages both commercial and non-commercial use. Homepage: http://www.boost.org ]]) From the above example, the variables BOOST_ROOT and MODULE_BOOST_PREFIX point to the installation directory. If needed to add the path to your Makefile (for example when compiling another program), this variable can be used. The variables could be used to add the location to the libraries, $MODULE_BOOST_PREFIX/lib, or headers, $MODULE_BOOST_PREFIX/include, \u0026hellip; etc. Note that the names of the variables may change from one cluster to another depending on the package manaer used to build the modules. Under the default standard environment on Grex, SBEnv, the name of the variable for each program is in the form, MODULE_NAMEOFTHEPROGRAM_PREFIX while it is in the form of EBROOT followed by NAMEOFTHEPROGRAM when using CCEnv. It is recommended to use module show to figure out what variables are used when using a given software stack on any cluster.\nAs a summary, here are the most used commands when using modules:\nmodule list: to list the modules that are loaded. module avail: to list the modules that currently available to load. module spider : to see how to load a given module. module load \u0026lt;module names/version \u0026hellip;\u0026gt;: to load a given set of modules \u0026hellip;. module show : to print more information about the module. module purge: to purge or unload all the modules loaded previously. Internal links# Available software External links# What is PATH? How to view and modify it The Alliance documentation about using Modules Lmod User Guide Traditional/Original TCL Modules "},{"id":"21","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/python-ai/","permalinkTitle":"Homepage / Software Specific Notes / Using Python for ML on Grex","title":"Using Python for ML on Grex","content":"Introduction# Python is a dynamic language with many optional Library \u0026ldquo;modules\u0026rdquo; available. Moreover, Python is often used as a \u0026ldquo;glue\u0026rdquo; language for interacting with tools and libraries written in other languages (C/C++, Fortran, CUDA, etc.). This makes maintenance of Python software difficult. Not only do Python and libraries need to be of the right versions, but also other software they depend on should be of the same versions that have been used to build the corresponding packages.\nSome of the mechanisms to handle these dependency problems are:\nconda (including everything, most of Linux with all the software binaries in Python repo). virtualenv / pip. packaging Python modules as HPC software Modules. using Linux containers like Docker or Singularity with all the software packed up by the software developers. The conda offers an easy way to package up all dependencies and often is liked by the users. Many conda repositories exist. Sometimes conda is the only way to run a particularly badly maintained Python package. However, it has important drawbacks such as packaging up all the dependencies which uses a lot of disk space, and provide for conflicts with HPC and Linux environments. Thus, conda can be used on HPC machines like Grex at the user\u0026rsquo;s risk. In particular, we suggest against any automatic \u0026ldquo;activation\u0026rdquo; of conda environments in users\u0026rsquo; startup scripts (like ~/.bashrc).\nNote that as of 2024, Anaconda owners strictened their licensing policy. We do not provide any system-wide conda installations on Grex. In case users want to continue using conda, they must be sure that they have a proper Anaconda license to do so. Note also that the same applies for mamba which would use the same conda software channels. The virtualenv, while similar to conda in that it would isolate the Python dependencies, is more HPC-friendly because it allows for using HPC modules together with Python pip to control dependencies per particular software item. ComputeCanada / The Alliance has chosen to provide for the CCEnv just basic Python as a Module, and let users use virtualenv for each workflow they would like. ComputeCanada / The Alliance provides repackaged Python \u0026ldquo;wheels\u0026rdquo; to work properly with CCEnv. pip install from CCEnv would use these wheels first. Using _\u0026ndash;index-url _ that would point to other sources of the \u0026ldquo;wheels\u0026rdquo; can lead to problems and broken installations.\nAdding each and every package with pip is time-consuming. CCEnv provides \u0026ldquo;modules\u0026rdquo; for a most common combination of fixed module versions of NumPy, SciPy, Matplotlib, etc., so-called scipy-stack modules.\nExamples of Use Cases# Example 1: using OpenAI shape-e in CCEnv virtualenv# Let us put the above into practice, by using an old OpenAI ML model, shap-e that can generate 3D objects in the form of .ply meshes, from a text. The example below will use the Alliance software environment, virtualenv, and the \u0026ldquo;shap-e\u0026rdquo; model fetched from GitHub.\n# Lets get an interactive job on a GPU node using 1 GPU for 2 hours salloc --gpus=1 --partition=stamps-b,agro-b,mcordcpu-b --cpus-per-task=2 --mem=32gb --time=0-2:00 Note the prompt changed to user@gXYZ; you should be on a compute node with GPU now. You can try nvidia-smi command to see the GPU model, driver version etc.\n# Let\u0026#39;s switch to CCEnv 2023 enviroment by loading required modules! module load CCEnv module load arch/avx512 StdEnv/2023 # Let\u0026#39;s load CUDA, cuDNN, python and Scipy stack modules needed to run the model. #The module versions are current as of Jul 2024 module load cuda/12.2 cudnn/9.2.1.18 module load python/3.11.5 scipy-stack/2024a which python We now have a python executable in the PATH, ready to set up a virtualenv with all the dependencies required by \u0026ldquo;shap-e\u0026rdquo;. These are Torch, Torchaudio, Torchvision, and PyTorch3d. It is often a challenge to provide matching versions for these ML libraries, so some trial and error with the \u0026ldquo;wheels\u0026rdquo; of the same or close enough versions available on CCEnv is usually required.\n# Lets start a virtualenv called shapee virtualenv shapee source shapee/bin/activate Note the prompt changes again to something like \u0026ldquo;(shapee) [user@gXYZ]\u0026rdquo;. We are now in a new Python environment and can proceed with installing Python dependencies and the ML model using pip.\n# These versions work in 2024 pip install jupyter nbconvert jupyter_contrib_nbextensions pip install torchvision==0.16.2 torchaudio==2.1.1 pytorch3d==0.7.5 git clone https://github.com/openai/shap-e \u0026amp;\u0026amp; cd shap-e \u0026amp;\u0026amp; pip install -e . cd .. If the above commands had passed without the errors, we should be able to use the model. A standalone example (made from the Jupyter notebook provided in the GitHub repo) should be able to run with Python in this _virtualenv and generate the .ply mesh files.\nPython script example for shap-e shap-e-test.py# https://github.com/openai/shap-e/blob/main/shap_e/examples/sample_text_to_3d.ipynb converted to Python # import torch from shap_e.diffusion.sample import sample_latents from shap_e.diffusion.gaussian_diffusion import diffusion_from_config from shap_e.models.download import load_model, load_config from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) print(\u0026#34;found device %s\\n\u0026#34; % device) xm = load_model(\u0026#39;transmitter\u0026#39;, device=device) model = load_model(\u0026#39;text300M\u0026#39;, device=device) diffusion = diffusion_from_config(load_config(\u0026#39;diffusion\u0026#39;)) batch_size = 4 guidance_scale = 15.0 prompt = \u0026#34;a shark\u0026#34; latents = sample_latents( batch_size=batch_size, model=model, diffusion=diffusion, guidance_scale=guidance_scale, model_kwargs=dict(texts=[prompt] * batch_size), progress=True, clip_denoised=True, use_fp16=True, use_karras=True, karras_steps=64, sigma_min=1e-3, sigma_max=160, s_churn=0, ) # Interactive display skipped, lets generate mesh files # Example of saving the latents as meshes. from shap_e.util.notebooks import decode_latent_mesh for i, latent in enumerate(latents): t = decode_latent_mesh(xm, latent).tri_mesh() with open(f\u0026#39;example_mesh_{i}.ply\u0026#39;, \u0026#39;wb\u0026#39;) as f: t.write_ply(f) with open(f\u0026#39;example_mesh_{i}.obj\u0026#39;, \u0026#39;w\u0026#39;) as f: t.write_obj(f) print(\u0026#34;All done\u0026#34;) python shap-e-test.py It also is possible to start a Jupyter Notebook server while being in the same environment and use the model interactively, with example Notebooks provided under shap-e/examples.\nThe above scenario is done using interactive salloc job on a GPU node. Interactive sessions are very useful for initializing Python environments and troubleshooting.\nFor long-running tasks, it is best to run the ML workloads under SLURM batch mode. The same modules must be loaded inside the SLURM job script, and the same virtualenv must be \u0026ldquo;activated\u0026rdquo; inside the job script as well, before running Python. An example job script for running the same Python code as a batch job:\nSLURM job script to run the above shap-e Python example run-python-gpu.sh#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=stamps-b,mcordgpu-b,livi-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=32gb #SBATCH --time=0-2:00:00 #SBATCH --job-name=python-shape-e # Adjust the resource requests above to your needs. # Example of loading modules, CUDA and Python from CCEnv: # modules for CCEnv Python module load CCEnv module load arch/avx512 StdEnv/2023 module load cuda/12.2 cudnn/9.2.1.18 module load python/3.11.5 scipy-stack/2024a # activate the existing virtualenv with Torch, Shap-e source shapee/bin/activate # actually run the code/model python shap-e-test.py echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; # end of the script The script can be submitted as usual, with the sbatch command:\nsbatch run-python-gpu.sh Example 2: using manga-image-translator with Singularity# Containers are another popular way of managing Python dependencies. In this example, let us try to translate Manga captions using AI code and models from manga-image-translator . We will need the software and the AI models it is using. While it is possible to build the software from GitHub sources with pip/virtualenv, it can be tricky and time consuming. The authors of the repository have provided a container image on DockerHub . We will be using the image on Grex with Singularity/ Apptainer. Singularity can pull and run many (but not every of them!) Docker images that are simple enough. Inspection of the Docker recipe here shows that the image is simple because it does not have any USER command. Moreover, we see that there is an ENTRYPOINT defined:\nENTRYPOINT [\u0026ldquo;python\u0026rdquo;, \u0026ldquo;-m\u0026rdquo;, \u0026ldquo;manga_translator\u0026rdquo;]\nKnowing the entry point is useful for running the same image with singularity exec. Let us first try to download and convert the Docker image into a new Singularity image in the SISF format.\n# Lets use singularity to pull the container from DockerHub module load singularity singularity pull docker://zyddnys/manga-image-translator:main The SISF image is large (can be 12 to 15GB). The software will be further downloading a couple of 10s of GBs of the LLM models. To avoid running into disk quota issues, it makes sense to do the work on /project filesystem rather than under /home!\nTo try running the container on a GPU, we would need to run an interactive session on a GPU node.\n# Lets get an interactive job on a GPU node using 1 GPU for 2 hours. Note that we\u0026#39;d need more memory for LLama models. salloc --gpus=1 --partition=stamps-b,agro-b,mcordcpu-b --cpus-per-task=4 --mem=100gb --time=0-2:00 There is a documentation on the GitHub site on how to run the models. Naturally it describes how to run it with Docker! But we can understand from it that the code expects two directories with the input and output bind-mounted into the container: /app/source and /app-source-translated. Docker is using -v flag to bind-mount \u0026ldquo;volumes\u0026rdquo;. Singularity is more humble and just bind-mounts directories with -B flag. Most of other Docker flags can be omitted, except for the GPU related flag that has to be replaced with \u0026ndash;nv for Singularity).\nLet us not forget what the \u0026ldquo;entrypoint\u0026rdquo; was! And what were the options required for the Python code itself as distinct to Sing/Docker options to execute the container.\n# Create source and source-translated directories mkdir source mkdir source-translated # The example Comics image for AI to work on, from the Github pages of the manga translator. # Please make sure to follow all the applicable Copyrigth laws when using pictures from the Internet. cd source \u0026amp;\u0026amp; wget https://user-images.githubusercontent.com/31543482/232265479-a15c43b5-0f00-489c-9b04-5dfbcd48c432.png cd .. # lets try to run the container, finally singularity exec --nv -B `pwd`/source:/app/source -B `pwd`/source-translated:/app/translated manga-image-translator_main.sif python -m manga_translator --use-gpu -l ENG -i source -v --translator=sugoi -m batch --overwrite --manga2eng The command above fails! The error mentions something about a directory \u0026ldquo;/app/models/translatiors\u0026rdquo;. The reason is that unlike Docker, Singularity containers are read-only and cannot download or create any new massive data into the existing image. To work around the issue, we would need one more directory, \u0026ldquo;/app/models/translators\u0026rdquo; bind mounted for the LLM models it tries to download. Also, adding \u0026ndash;writable-tmpfs for ephemeral temporary filesystems is often needed for Python codes in Singularity.\n## the directories and test image should already exist from the above example. Lets add translators and bind-mount it to /app/models! mkdir translators singularity exec --writable-tmpfs --nv -B `pwd`/translators:/app/models/translators -B `pwd`/source:/app/source -B `pwd`/source-translated:/app/translated manga-image-translator_main.sif python -m manga_translator --use-gpu -l ENG -i source -v --translator=sugoi -m batch --overwrite --manga2eng When the command finishes, the ./source-translated would contain a image with Japanese text translated to English.\nExample 3: using manga-image-translator with Podman# Singularity is the preferred container engine on our shared HPC systems. However, there are cases when Singularity/Apptainer SISF images would not run Docker images correctly. In these cases, root-less Podman can be used. Both the Alliance\u0026rsquo;s CCEnv and Grex\u0026rsquo;s SBEnv provide a module for Podman. Using Podman is very similar to using Docker in that Podman supports same command line options. However, use of Podman in cases when Singularity fails is also more advanced and it would need additional options for user namespace and user/group ID mappings. In general, our support of Podman is as of now considered experimental.\nIn our example, we would not need these advanced namespaces options because the manga-image_translator is well built to work in either Singularity or Docker.\nTo try running the container on a GPU, we would need to run an interactive session on a GPU node.\n# Lets get an interactive job on a GPU node using 1 GPU for 2 hours. Note that we\u0026#39;d need more memory for LLama models. salloc --gpus=1 --partition=stamps-b,agro-b,mcordcpu-b --cpus-per-task=4 --mem=100gb --time=0-2:00 Then, we would do exactly the same sequence of commands but with Podman instead of Singularity.\n# Lets use podman module load podman # directories to bind-mount mkdir source mkdir source-translated # The example Comics image for AI to work on, from the Github pages of the manga translator. # Please make sure to follow all the applicable Copyrigth laws when using pictures from the Internet! cd source \u0026amp;\u0026amp; wget https://user-images.githubusercontent.com/31543482/232265479-a15c43b5-0f00-489c-9b04-5dfbcd48c432.png cd .. mkdir translators # running it with podman. Note the more options like --rm, and how the GPU is handled with --device. podman run --rm --device=nvidia.com/gpu=all --ipc=host -v `pwd`/translators:/app/models/translators -v `pwd`/source:/app/source -v `pwd`/source-translated:/app/translated docker://zyddnys/manga-image-translator:main --use-gpu -l ENG -i source -v --translator=sugoi -m batch --overwrite --manga2eng Note that the Podman example also bind-mounts the ./translators . Even thought it might work without it, it does make a lot of sense to bind-mount ./translators to avoid re-downloading the LLM models into Docker containers\u0026rsquo; image every time we run a new container instance using them!\nExternal links# The Alliance documentation about Python The Alliance list of Available wheels An old talk by Bart Oldeman at CC/DRAC Python at ComputeCanada Training Huggingface LLMs on Alliance systems# A new documentation item covers on how to download and use HuggingFace models on Alliance Software stack . This documentation largely applies to Grex as well, when using CCEnv software stack.\n"},{"id":"22","rootTitleIndex":"7","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-person-running fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/batch-jobs/","permalinkTitle":"Homepage / Running jobs on Grex / Running batch jobs on Grex","title":"Running batch jobs on Grex","content":"Batch jobs# HPC systems usually are clusters of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users\u0026rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called \u0026ldquo;jobs\u0026rdquo; to a \u0026ldquo;batch queue\u0026rdquo;. A subset of the available resources of the HPC machine is allocated to each of the users\u0026rsquo; batch jobs, and they run without any need for user intervention as soon as the resources become available.\nThe job placement, usage monitoring and job accounting are done via a special software, the HPC scheduler. This is an often-under-appreciated automation that makes usage efficient and saves a lot of work on part of the user. However, using HPC is hard in a sense that users have to make an effort in order to figure out what are the available resources on an HPC cluster, and what is the efficient way of requesting the resources for their jobs. Asking for too many resources might be wasteful both in preventing others from using them and in making for a longer queuing time.\nThe resources (\u0026ldquo;tractable resources\u0026rdquo; in SLURM speak) are CPU time, memory, and GPU time. Generic resources can be software licenses, \u0026hellip; etc. Requesting resources is done via command line options to job submission commands sbatch and salloc, or via special comment lines or SLURM directives (starting with #SBATCH) in job scripts. There are also options to control job placement such as partitions.\nThere are default values for the resources which are taken when you do not specify the resource limit. Note that the default values are, as a rule, quite small. On Grex, the default values are set as follow: 3 hours of wall time, 256mb of memory per CPU. In most of the cases, it is better to have an explicit request of an appropriate resource limit rather than using the default.\nWe ask our users to be fair and considerate and do not allow for deliberate waste of resources (such as running serial jobs on more than one CPU core, or running CPU-only calculations on GPU nodes).\nThere are certain scheduling policies in place to prevent the cluster from being swamped by a single user. In particular, the MAXPS / GrpRunMins limit disfavors asking for many CPU cores for long wall time, a MaxCPU limits restricts number of CPU cores used, and there are limits on number of user\u0026rsquo;s jobs in the system and number of array job elements, as described below.\nScheduling policies# The following policies are implemented on Grex:\nThe default wall time is 3 hours (equivalent to: --time=3:00:00 or --time=0-3:00:00). The default amount of memory per processor (--mem-per-cpu=) is 2500M. Memory limits are enforced, so an accurate estimate of memory resource (either in the form of --mem= or --mem-per-cpu=) should be provided. The maximum wall time is 21 days on genoa, skylake partitions, on largemem and genlm partition. The maximum wall time is 7 days on the gpu partition. The maximum wall time is 7 days on the preempted partitions: stamps-b, livi-b and agro-b. The maximum number of processor-minutes for all currently running jobs of a group without a RAC is 4 M. The maximum number of jobs that a user may have queued to run is 4000. The maximum size of an array job is 2000. Users without a RAC award are allowed to simultaneously use up to 400 CPU cores per accounting group. There are limits on the number of GPUs that can be used on contributed hardware (1 GPU per job). Note that you can see some information about the partitions by running the custom script partition-list from your terminal:\npartition-list Typical batch job cases# Any batch job is submitted with sbatch command. Batch jobs are usually shell (BASH, etc.) scripts wrapping around the invocation of a code. The comments on top of the script that start with #SBATCH are interpreted by the SLURM scheduler as options for resource requests:\nDirective Example Description --ntasks= --ntasks=4 Number of tasks (MPI processes) per job. --nodes= --nodes=2 Number of nodes (servers) per job. --ntasks-per-node= --ntasks-per-node=4 Number of tasks (MPI processes) per node. --cpus-per-task= --cpus-per-task=8 Number of threads per task (should not exceed the number of physical cores. --mem-per-cpu= --mem-per-cpu=1500M Memory per task (or thread). --mem= --mem=16000M Memory per node. --gpus= --gpus=1 Number of GPUs per job. --time- --time=0-8:00:00 wall time in format DD-HH:MM:SS --qos= * QOS by name (Not to be used on Grex!). --partition= --partition=skylake Partition name: skylake, \u0026hellip; etc (very much used on Grex!). Assuming the name of myfile.slurm (the name or the extension does not matter, it can be called afile.job, otherjob.sh, \u0026hellip; etc.), a job is submitted with the command:\nsbatch myfile.slurm or\nsbatch [+some options] myfile.slurm Some options like --partition=skylake could be invoked at submission time.\nRefer to the official SLURM documentation and/or man sbatch for the available options. Below we provide examples for typical cases of SLURM jobs.\nSerial jobs# The simplest kind of job is a serial job when one compute process runs in a sequential fashion. Naturally, such job can utilize only a single CPU core: even large parallel supercomputers as a rule do not parallelize binary codes automatically. So, the CPU request for a serial job is always 1, which is the default; the other resources can be wall time and memory. SLURM has two ways of specifying the memory: memory per core (--mem-per-cpu=) and total memory per node (--mem=). It is more logical to use per-core memory always; except in case of the whole-node jobs when special value --mem=0 gives all the available memory for the allocated node. An example script (for 1 CPU, wall time of 30 minutes and a memory of 2500M) is provided below.\nScript template for serial job run-serial-job-template.sh#!/bin/bash #SBATCH --time=0-0:30:00 #SBATCH --mem=2500M #SBATCH --job-name=\u0026#34;Serial-Job-Test\u0026#34; # Script for running serial program: your_program echo \u0026#34;Current working directory is `pwd`\u0026#34; # Load modules if needed: echo \u0026#34;Starting run at: `date`\u0026#34; ./your_program \u0026lt;+options or arguments if any\u0026gt; echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; An important special case of serial jobs is high-throughput computing: jobs are serial because they are too short to parallelize them, however there are very many such jobs per research project. The case of embarrassingly parallel computations like some of the Monte Carlo simulations are often High Throughput Computing (HTC).\nSerial jobs that have regularly named inputs and run more than a few minutes each best be specified as a Job array (see below). Serial jobs that are great in numbers, and run less than a few minutes each, better be joined into a task farm running within a single larger job using tools like GLOST, GNU Parallel or a workflow engine like QDO. An example of GLOST job is under the MPI jobs section (see below).\nSMP / threaded / single node jobs# The next kind of job is multi-threaded, shared memory or single-node parallel jobs. Often these jobs are for Symmetric Multiprocessing (SMP) codes that can use more than one CPU on a given node to speed up the calculations. However, SMP/multithreaded jobs rely on some form of inter-process communication (shared memory, \u0026hellip; etc.) that limits them to the CPU cores within just a single server. They cannot scale across multiple compute nodes. Examples are OpenMP, pthreads, Java codes, etc. Gaussian and PSI4 are SMP codes; threaded BLAS/LAPACK routines from MKL (inside NumPY) can utilize multiple threads, \u0026hellip; etc. Note that this kind of programs do not scale very well when increasing the number of threads. We recommend to our users to run a benchmark to see how their programs scale with the number of threads to define the combination or a set of threads for better performance.\nThus, from the point of view of the SMP/threaded jobs resources request, the following considerations are important:\nasking always only a single compute node and one task (--nodes=1 --ntasks=1) job. asking for several CPU cores on it per job, up to the maximum number of CPU cores per node (--cpus-per-task=N) where N should not exceed the total physical cores available on the node. Depending on the partition, you may choose N up to 52 on the skylake partition, up to 40 on the largemem partition, \u0026hellip; etc. making sure that the total memory asked for does not exceed the memory available on the node (refer to the section about node characteristics, hardware for more information). making sure that the code would use exactly the number of CPU cores allocated to the job, to prevent waste or congestion of the resources. In SLURM, it makes a difference whether you ask for parallel tasks (--ntasks) or threads (--cpus-per-task) ; the threads should not be isolated from each other (because they might need to use shared memory!) but the tasks are isolated to each own \u0026ldquo;cgroup\u0026rdquo;.\nAn environment variable ${SLURM_CPUS_PER_TASK} is set in the job, so you can set an appropriate parameter of your code to the same value.\nFor OpenMP, it would be done like:\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} For MKL it is MKL_NUM_THREADS, for Julia --JULIA_NUM_THREADS, for Java -Xfixme parameter.\nScript template for running a job on **skylake** partition: using full node run-smp-job-node-template.sh#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --nodes=1 #SBATCH --ntask-per-node=1 #SBATCH --cpus-per-task=52 #SBATCH --mem=0 #SBATCH --partition=skylake #SBATCH --job-name=\u0026#34;OMP-Job-Test\u0026#34; # An example of an OpenMP threaded job that # takes a whole \u0026#34;skylake\u0026#34; node for 8 hours. export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; ./your-openmp.x input.dat \u0026gt; output.log echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; Note that the above example requests the whole node\u0026rsquo;s memory with --mem=0 because the node is allocated to the job fully due to all the CPUs anyways. It is easier to use the --mem syntax for SMP jobs because typically the memory is shared between threads (i.e., the amount of memory used does not change with the number of SMP threads). Note, however, that the memory request should be reasonably \u0026ldquo;efficient\u0026rdquo; if possible.\nIt is also possible to use a fraction of the node for running OpenMP jobs. Here is an example asking for 1 task with 4 threads on compute partition:\nScript template for running a job on **skylake** partition: using a fraction of the node run-smp-job-partial-node-template.sh#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --nodes=1 #SBATCH --ntask-per-node=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=8000M #SBATCH --partition=skylake #SBATCH --job-name=\u0026#34;OMP-Job-Test\u0026#34; # An example of an OpenMP threaded job that # takes 4 threads for 8 hours. export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; ./your-openmp.x input.dat \u0026gt; output.log echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; GPU jobs# The GPU jobs would usually be similar to SMP/threaded jobs, with the following differences:\nThe GPU jobs should run on the nodes that have GPU hardware, which means you\u0026rsquo;d want always to specify one of the following options: Directive Description --partition=gpu to use the gpu partition. --partition=stamps-b to use the stamps-b partition. --partition=livi-b to use the livi-b partition. --partition=agro-b to use the agro-b partition. SLURM on Grex uses the so-called \u0026ldquo;GTRES\u0026rdquo; plugin for scheduling GPU jobs, which means that the request syntax in the form --gpus=N or --gpus-per-node=N or --gpus-per-task=N is used. How many GPUs to ask for?# Grex, at the moment, does not have GPU-direct MPI enabled, which means that most of the jobs would be single-node. The GPU nodes in either gpu (two nodes, 32GB V100s) or stamps-b (three nodes, 16GB V100s) partition have 4 V100 GPUs, 32 Intel 52xx CPUs and 192GB of CPU memory. There is also the livi-b partition with a large single 16x v100 GPU server. So, asking 1 to 4 GPUs, one node, and 6-8 CPUs per GPU with an appropriate amount of RAM (4-8 Gb) per job would be a good starting point.\nNote that V100 is a fairly large GPU for most of the jobs, and for good utilization of the GPU resources available on Grex, it is a good idea to start with a single GPU, and then try if the code actually is able to saturate it with load. Many codes cannot scale to utilize more than one GPU, and few codes can utilize more than two of them.\nScript template for running ont-guppy on GPU run-guppy-gpu.sh#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=stamps-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=6 #SBATCH --mem-per-cpu=6000M #SBATCH --time=0-12:00:00 #SBATCH --job-name=genomics-test # Adjust the resource requests above to your needs. # Example of loading modules, CUDA: module load cuda/12.4.1 export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} echo \u0026#34;Starting run at: `date`\u0026#34; nvidia-smi guppy_basecaller -x auto --gpu_runners_per_device 6 -i Fast5 -s GuppyFast5 -c dna_r9.4.1_450bps_hac.cfg echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; The above script (if called say, gpu.job) can be submitted with the usual command:\nsbatch gpu.job Distributed, massively parallel jobs# Parallel jobs that can spawn multiple servers are the most scalable ones, because they are not limited by the number of CPUs or memory per node. Running many parallel tasks across more than one node requires some inter-node communication (which as a rule is slower than shared memory within one server). In HPC, high speed interconnect and specialized RDMA-aware communication libraries make distributed parallel computation very scalable. Grex uses InfiniBand interconnect (IB).\nMost often (but not always), parallel programs are built upon a low-level message passing library called MPI. Refer to the Software section for more information about parallel libraries on Grex. Examples of distributed parallel codes are GAMESS-US, ORCA , LAMMPS , VASP , \u0026hellip; etc.\nThe distributed parallel jobs can be placed across their compute nodes in several ways (i.e., how many parallel tasks per compute node?). Thus, SLURM resource request syntax allows to specify the required layout of nodes/tasks (or nodes/tasks/threads, or even nodes/tasks/GPUs since hybrid MPI+OpenMP and MPI+GPU programs exist). A consideration about the layout is a tradeoff between making the program work faster (and sometimes to work correctly at all) and making the scheduler\u0026rsquo;s work easier.\nA well written MPI software theoretically should not care how the tasks are distributed across how many physical compute nodes. Thus, SLURM\u0026rsquo;s --ntasks= request (similar to the old Torque procs=) specified without --nodes would work and make the scheduling easier.\nA note on process starting:\nSince MPI jobs are distributed, there should be a mechanism to start the compute processes across all of the nodes (or CPUs) allocated for it. The mechanism should know which nodes to use, and how many. Most modern MPI implementations \u0026ldquo;tightly integrate\u0026rdquo; with SLURM, so they will get this information automatically via a Process Management Interface (PMI). SLURM provides its own job starting command called srun . Most MPI implementations also provide their own job spawned commands, usually called mpiexec or mpirun. These are specific to each MPI vendor/kind and not well standardized, and differ in the support of SLURM.\nFor example, OpenMPI (the default, supported MPI implementation) on Grex is compiled against PMIx (3.x, 4.x) or PMI1 (1.6.5). So, it is preferable to use srun instead of mpiexec to kick start the MPI processes, because srun would use PMI.\nFor Intel MPI (another MPI, also available on Grex and required by some of the binary codes, like ANSYS or ADF), srun sometimes may not work, but PMI1 can be used with mpiexec.hydra by setting the following environment variable:\nexport I_PMI_LIBRARY=/opt/slurm/lib/libpmi.so Some examples# Here is an example for running MPI job (in this case, Quantum ESPRESSO) using 32 cores:\nScript template for running QE on 32 CPUs run-qe-mpi-template.sh#!/bin/bash #SBATCH --time=0-8:00:00 #SBATCH --mem-per-cpu=1500M #SBATCH --ntasks=32 #SBATCH --job-name=\u0026#34;QE-Job\u0026#34; # A example of an MPI parallel that # takes 32 cores on Grex for 8 hours. # Load the modules: module load arch/avx512 intel/2023.2 openmpi/4.1.6 espresso/7.3.1 export OMP_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; srun pw.x -in MyFile.scf.in \u0026gt; Myfile.scf.log echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; However, in practice there are cases when layout should be more restrictive. If the software code assumes equal distribution of processes per node, the request should be --nodes=N --ntasks-per-node=M. A similar case is MPMD codes (Like NWCHem or GAMESS-US or OpenMolcas) that have some of the processes doing computation and some communication functions, and therefore requires at least two tasks running per each node.\nFor some codes, especially for large parallel jobs with intensive communication between tasks there can be performance differences due to memory and interconnect bandwidths, depending on whether the same number of parallel tasks is compacted on few nodes or spread across many of them. Find an example of the job below.\nScript template for running NWChem on 32 cores distributed on 4 nodes run-nwchem-template.sh#!/bin/bash #SBATCH --nodes=4 #SBATCH --ntasks-per-node=8 #SBATCH --mem-per-cpu=4000M #SBATCH --job-name=\u0026#34;NWchem-Job\u0026#34; #SBATCH --job-name=NWchem-dft-test # Adjust the number of tasks, time and memory required. # the above spec is for 32 compute tasks over 4 nodes. # Load the modules: module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1 echo \u0026#34;Starting run at: `date`\u0026#34; which nwchem # Uncomment/Change these in case you want to use custom basis sets NWCHEMROOT=/global/software/cent7/nwchem/6.8.1-intel15-ompi314 export NWCHEM_NWPW_LIBRARY=${NWCHEMROOT}/data/libraryps export NWCHEM_BASIS_LIBRARY=${NWCHEMROOT}/data/libraries # In most cases SCRATCH_DIR would be on local nodes scratch # While results are in the same directory export NWCHEM_SCRATCH_DIR=$TMPDIR export NWCHEM_PERMANENT_DIR=`pwd` # Optional memory setting; note that this one or the one in your code # must match the #SBATCH --mem-per-cpu times compute tasks ! export NWCHEM_MEMORY_TOTAL=2000000000 # 24000 MB, double precision words only export MKL_NUM_THREADS=1 srun nwchem dft_feco5.nw \u0026gt; dft_feco5.$SLURM_JOBID.log echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; OpenMPI# OpenMPI is the default MPI implementation for Grex (and Compute Canada, now the Alliance). The modules for it on Grex are called ompi . The MPI example scripts above are all OpenMPI based. The old version 1.6.5 is there for compatibility reasons with older software; most users should use 3.1.x or 4.x.x versions. Using srun is recommended in all cases.\nIntel MPI# For applications using IntelMPI (impi modules on Grex, or Intel-MPI based software from Compute Canada CVMFS software stack), a few environment variables have to be set. The following link explains it: Using SLURM with PMI .\nThe JLab documentation example shows an example of SLURM script with IntelMPI .\nOther MPIs# Finally, some canned codes like ANSYS or StatCCM+ would use a vendor-specific MPI implementation that would not tightly integrate with our scheduler\u0026rsquo;s process to CPU core placement. In that case, several whole nodes (that is, with the number of tasks equal to the node\u0026rsquo;s number of CPU cores) should be requested to prevent the impact on other jobs with resource congestion.\nSuch codes will also require a nodelist (machinefile) file obtained from SLURM and provided to them in their own format.\nA custom script slurm_hl2hl.py makes this easier (see CC StarCCM+ or CC ANSYS documentation ). The script slurm_hl2hl.py is already available on Grex.\nslurm_hl2hl.py --format STAR-CCM+ \u0026gt; machinefile Job arrays# Job arrays allow for submitting many similar jobs \u0026ldquo;in one blow\u0026rdquo;. It saves users work on job submission, and also makes SLURM scheduler more efficient in scheduling the array jobs because it would know they are the same with respect to size, expected wall time etc.\nArray jobs work most naturally when a single code has to be applied for parameter sweep and/or to a large number of input files that are regularly named, for example as: test1.in, test2.in, \u0026hellip; test99.in\nThen, a single job script with #SBATCH --array=1,99 can be used to submit the 99 jobs.\nIn order to distinguish between the input files, within each of the jobs at run time, you would have to obtain a value for the array index. Which is set by SLURM as ${SLURM_ARRAY_TASK_ID} environment variable. The call to the code on a particular input will then be like:\n./my_code test${SLURM_ARRAY_TASK_ID}.in This way each of the array element jobs can distinguish their own portion of the work to do. A real life example is below; it attempts to run all of the Gaussian standard tests which have names of the format test0001.com, test0002.com, .. test1204.com, etc. Note the printf trick to deal with trailing zeroes in the input names.\nScript template for running job array: case of Gaussian standard tests run-array-job-gauss-tests.sh#!/bin/bash #SBATCH --cpus-per-task=1 #SBATCH --mem=1000MB #SBATCH --job-name=\u0026#34;G16-tests\u0026#34; #SBATCH --array=1-1204 echo \u0026#34;Current working directory is `pwd`\u0026#34; echo \u0026#34;Running on `hostname`\u0026#34; echo \u0026#34;Starting run at: `date`\u0026#34; # Set up the Gaussian environment using the module command: module load gaussian/g16.c01 # Run g16 on an array job element id=`printf \u0026#34;%04d\u0026#34; $SLURM_ARRAY_TASK_ID` v=test${id}.com w=`basename $v .com` g16 \u0026lt; $v \u0026gt; ${w}.${SLURM_JOBID}.out echo \u0026#34;Job finished with exit code $? at: `date`\u0026#34; There are limits on how large array jobs can be (see our scheduling policies): the maximal number of elements in job array, as well as the maximal number of jobs that can be submitted by a user.\nUsing CC CVMFS software# As explained in more detail in the software/Modules documentation, we provide Compute Canada\u0026rsquo;s software environment. Most of it can run out of the box by just specifying the corresponding module.\nThere are some caveats:\nSome of the Compute Canada software might have hardcoded environment variables that exist only on these systems. An example is SLURM_TMPDIR. On Grex, add export SLURM_TMPDIR=$TMPDIR to your job scripts.\nIn general, it is hard to containerize HPC. So the software that requires low-level hardware/device drivers access (OpenMPI, CUDA) may have problems when running on non-CC systems. Newer version of OpenMPI (3.1.x) seems to be more portable for using the PMIx job starting mechanism.\n\u0026ldquo;Restricted\u0026rdquo; (commercial) software\u0026rsquo;s binaries are not distributed by Compute Canada CVMFS due to the obvious licensing issues. It has to be installed locally on Grex.\nHaving said that, module load CCEnv gives the right software environment to be run on Grex for a vast majority of threaded and serial software items from CC software stack. See discussion about MPI-parallel jobs below.\nBecause of the distributed nature of CVMFS, it might take time to download a program or library or data file. It would probably make sense to first access it interactively or from an interactive job to warm the CVMFS local cache, to avoid job failures due to the delay.\nBelow is an example of an R serial job that uses quite a few packages from Compute Canada software stack.\nScript template for running R using a module from Compute Canada software stack. run-r-cc-cvmfs-template.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-72:00:00 #SBATCH --job-name=\u0026#34;R-gdal-jags-bench\u0026#34; cd ${SLURM_SUBMIT_DIR} # Load the modules: module load CCEnv module load arch/avx512 module load StdEnv/2023 module load gcc/12.3 r/4.4.0 jags/4.3.2 geos/3.12.0 gdal/3.9.1 export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; R --vanilla \u0026lt; Benchmark.R \u0026amp;\u0026gt; benchmark.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Users of contributed systems which are newer than the original Grex nodes might want to switch to arch/avx2 or arch/avx512 from the default arch/sse3.\nUsing CC CVMFS software that is MPI-based.# We have found that the recent Compute Canada toolchains that use OpenMPI 3.1.x work on Grex without any changes (that is, with srun). Therefore, for OpenMPI based applications, we recommend to load Compute Canada\u0026rsquo;s software that depends on the recent toolchains, 2018.3 or later (Intel 2018 compilers, GCC 7.3 compilers and openmpi/3.1.2).\nFor example, the module commands below would load the Intel/OpenMPI 3.1.2 toolchain-based environment:\nmodule load CCEnv module load StdEnv/2018.3 Below is an arbitrarily chosen IMB benchmark result for MPI1 on Grex, the sendrecv tests using two processes on two nodes with several MPI implementations (CC means MPI coming from the Compute Canada (now, the Alliance) stack, Grex means compiled locally on Grex).\nYou can see that differences in performance between OpenMPI 3.1.x from CC stack and Grex are minor for this benchmark, even without attempting any local tuning for the CC OpenMPI.\nUsing New 2020 CC CVMFS Stack# Since Spring 2021, Compute Canada has updated the default software stack on their CVMFS distribution to StdEnv/2020 and gentoo. This version will not run on legacy Grex partitions (compute) at all, because it requires AVX2 CPU architecture. It will work as expected on all new GPU and CPU nodes (skylake, largemem, gpu and contributed systems).\nRelated links# SLURM "},{"id":"23","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/code-development/","permalinkTitle":"Homepage / Software and Applications / Code Development on Grex","title":"Code Development on Grex","content":"Introduction# Grex comes with a sizable software stack that contains most of the software development environment for typical HPC applications. This section of the documentation covers best practices for compiling and building your own software on Grex.\nOn Grex, login nodes can be used to compile software and to run short interactive and/or test runs. All other jobs must be submitted to the batch system. User sessions on the login nodes are limited by cgroups to prevent resource congestion. Thus, it sometimes makes sense to perform some of the code development in interactive jobs, in cases such as (but not limited to):\n(a) the build process and/or tests requires heavy, many-core computations, \u0026hellip; (b) you need access to specific hardware that is not present on the login nodes, such as GPUs and newer/different CPUs. Most of the software on Grex is available through environmental modules. It is almost always necessary to use modules to load current C, C++, Fortran compilers and Python interpreter. To find a software development tool or a library to build your code against, the module spider command is a good start. The applications software is usually installed by us from sources, into subdirectories under /global/software .\nIt is almost always better to use communication libraries (MPI) provided on Grex rather than building your own, because ensuring tight integration of these libraries with our SLURM scheduler and low-level, interconnect-specific libraries might be tricky.\nGeneral Linux Base OS notes# The base operating system on Grex is a RedHat type of Linux. For many years it used to be a CentOS Linux. Since 2024, we have switched to Alma Linux which is a community owned and governed, RedHat-style distribution. The current OS is Alma Linux 8.\nAlma Linux OS comes with its set of development tools, and RedHat environment does provide various developer toolsets and software channels. However, due to the philosophy of RedHat being stable, server-focused distribution, the tools are usually rather old. For example, cmake and git and gcc and python are always a couple of years behind the current versions. Therefore, even for these basic tools you more likely would want to load a module with newest versions of these tools:\nmodule load git module load cmake Alma Linux also has its system versions of Python, Perl, and GCC compilers. When no modules are loaded, the binaries of these will be available in the PATH. The purpose of these is to make some systems scripts possible, to compile OS packages, drivers and so on. We suggest using these tools using Modules and one of our Software Stacks instead.\nWe do not install many packages for the dynamic languages (such as python-something) in the base OS level, because it makes maintaining different versions of them complicated. Use the module spider command to find a version of Perl, Python, R, etc. to suit your needs. The same applies to compiler suites like GCC and Intel.\nWe do install AlmaLinux packages with OS that are:\nbase OS packages necessary for functioning graphical libraries that have many dependencies never change versions that are not critical for performance and/or security. Here are some examples: FLTK, libjpeg, PCRE, Qt and Gtk. Login nodes of Grex have many \u0026lsquo;\u0026rsquo;-devel\u0026rsquo;\u0026rsquo; packages installed, while compute nodes do not because we want them lean and quickly re-installable. Therefore, compiling codes that requires \u0026lsquo;\u0026rsquo;-devel\u0026rsquo;\u0026rsquo; base OS packages might fail on compute nodes. Contact us if something like that happens when compiling or running your applications.\nFinally, because HPC machines are shared systems and users do not have sudo access, following some instructions from a Web page that asks for apt-get install this or yum install that will fail. Rather, module spider should be used to see if the package you want is already installed and available as a module. If not, you can always contact support and ask for help to install the program either under your account or as a module when possible.\nCompilers and Toolchains# Due to the hierarchical nature of our Lmod modules system, compilers and certain core libraries (MPI and CUDA) form toolchains. Normally, you would need to choose a compiler suite (GCC or Intel or AOCC) and, in case of parallel applications, a MPI library (OpenMPI or IntelMPI). These come in different versions. Also, you\u0026rsquo;d want to know if your program support CUDA to be able to utilize GPUs. A combination of compiler/version, MPI/version and possibly CUDA makes a toolchain. Toolchains are mutually exclusive; you cannot mix software items compiled with different toolchains!\nSee Using Modules page for more information.\nThere is no module loaded by default! There will be only the system\u0026rsquo;s GCC-8 and no MPI whatsoever. To get started, load an Architecture module, then a compiler/version. Then, if necessary, an MPI (openmpi or intelmpi). If GPUs are required, a CUDA module would be needed to load first because it forms a root of GPU-enabled toolchains.\nA typical sequence of commands to get an environment with new Intel-One compiler and OpenMPI, for AVX512 compute nodes is as follows:\nmodule load arch/avx512 module load intel-one/2024.1 module load openmpi/4.1.6 The example below is for GCC 13 and openmpi:\nmodule load arch/avx512 module load gcc/13.2.0 module load openmpi/4.1.6 Compiler modules would set standard system environment variables ($CC, $FC and $CXX) for compiler names. The MPI wrappers (mpicc, mpicxx, mpif90 or mpifort \u0026hellip; etc.) will be set correctly by MPI modules to point to the right compilers.\nIntel compilers suite# Intel had been providing an optimizing compiler suite for Intel x86_64 CPU architectures for many years. Since 2023, the venerable \u0026ldquo;classic\u0026rdquo; Intel compilers were gradually discontinued and in 2024 replaced by a new Intel-OneAPI compilers suite based on the open source LLVM/Clang codebase. The \u0026ldquo;classic\u0026rdquo; compilers (icc, icpc, ifort) are replaced in the OneAPI suite with the new icx, icpx, ifx correspondigly.\nAs a result of our CentOS to AlmaLinux upgrade, all the older Intel compiler versions were obsoleted and removed from the local Grex software stack.\nThe name for the Intel suite modules is intel; module spider intel is the command to find available Intel \u0026ldquo;classic\u0026rdquo; versions. Latest Intel Classic compilers did include both \u0026ldquo;classic\u0026rdquo; and LLVM compilers. On Grex local software stack, the name for the new Intel OneAPI compilers suite modules is intel-one; module spider intel-one is the command to find available Intel OneAPI versions. These will not contain old compilers like icc anymore.\nThe Intel compilers suite also provides tools and libraries such as MKL (Linear Algebra, FFT, etc.), Intel Performance Primitives (IPP), Intel Threads Building Blocks (TBB), and VTune . Intel MPI as well as MKL for GCC compilers are available as separate modules, should they be needed for use separately. Both classic and OneAPI compiler suites come with and can use optimized performance libraries: Intel MKL, TBB and IPP.\nGCC compilers suite# GCC stands for \u0026ldquo;GNU Compiler Collection\u0026rdquo; and includes C, C++ and Fortran languages (as well as many other optionally).\nThe module name for GCC is gcc, as in module spider gcc.\nMulti-lib GCC is not supported, thus all the GCC modules are strictly 64-bit, and thus unable to compile legacy 32-bit programs.\nRecent GCC versions have a good support for AVX512 CPU instructions on both Intel and AMD CPUs. However, care may be taken with -march=native because the subsets of AVX512 implemented on Intel Cascade Lake and AMD Genoa CPUs may be different, so it does matter on which host a given code had been compiled.\nAOCC comilers suite# AMD AOCC is an optimized compiler collection for C, C++ and Fortran. It generates code optimized for AMD CPUs, like for newest Zen4 and Zen5 architectures.\nThe module name for AOCC compiler bunlde is aocc, as in module spider aocc. The compilers are called clang , clang++ and flang .\nMPI and Interconnect libraries# The standard distribution of MPI on Grex is OpenMPI . We build most of the software with it. To keep compatibility with the old Grex software stack, we name the modules openmpi. MPI modules depend on the compiler they were built with, which means that a compiler module should be loaded first; then the dependent MPI modules will become available as well. Changing the compiler module will trigger automatic MPI module reload. This is how the Lmod hierarchy works now.\nFor a long time Grex was using the interconnect drivers with ibverbs packages from the IB hardware vendor, Mellanox. It is no longer the case: for CentOS-7, we have switched to the vanilla Linux InfiniBand drivers, the open source RDMA-core package, and OpenUCX libraries. The current version of UCX on Grex is 1.6.1. Recent versions of OpenMPI (3.1.x and 4.0.x) do support UCX. Also, our OpenMPI is built with process management interface versions PMI1, PMI2 and PMIx4, for tight integration with the SLURM scheduler.\nThe current default and recommended version of MPI is OpenMPI 4.1.\n#load a compiler module first! module load openmpi/4.1.6 All MPI modules, be that OpenMPI or Intel, will set MPI compiler wrappers such as mpicc, mpicxx, mpif90 to the compiler suite they were built with. The typical workflow for building parallel programs with MPI would be to first load a compiler module, then an MPI module, and then use the wrapper of C, C++ or Fortran in your makefile or build script.\nIn case a build or configure script does not want to use the wrapper and needs explicit compiler and link options for MPI, OpenMPI wrappers provide the --show option that lists the required command line options. Try for example:\nmpicc --show to print include and library flags to the C compiler to be linked against the currently loaded OpenMPI version.\nThere is also IntelMPI, for which the modules are named intelmpi. See the notes on running MPI applications under SLURM here .\nLinear Algebra BLAS/LAPACK# Linear Algebra packages are used in most of the STEM research software. A very popular suite of libraries are BLAS and LAPACK from NetLib , written in Fortran and C. However, modern CPU architectures with its complex instruction sets and memory hierarchies are too complex for code generation. Various optimizations allow to improve BLAS and LAPACK performance at least tenfold as compared to the reference Netlib versions. Thus, it is always a good idea to use the Linear Algebra libraries that are optimized for a given CPU architecture. Such as vendor-optimized Intel MKL and AMD AOCL, OpenBLAS, or similar. These libraries are provided as modules on HPC systems.\nIt is worth noting that the linear algebra libraries might come with two versions: one 32-bit array indexes, another full 64-bit. Users must pay attention and link against the proper version for their software (that is, a Fortran code with -i8 or -fdefault-integer-8 would link against 64-bit pointers BLAS).\nIntel MKL# The fastest BLAS/LAPACK implementation from Intel, for Intel CPUs. With Intel compilers, it can be used as a convenient compiler flag, -mkl or if threaded version is not needed, -mkl=sequential.\nWith both Intel and GCC compilers, the MKL libraries can be linked explicitly with compiler/linker options. The base path for MKL includes and libraries is defined as the MKLROOT environment variable. For GCC compilers, module load mkl is needed to add MKLROOT to the environment. There is a command line advisor Website to pick the correct order and libraries. Libraries with the _ilp64 suffix are for 64-bit indexes while _lp64 are for the default, 32-bit indexes.\nNote that when the milti-threaded MKL is used, the number of threads is controlled with the MKL_NUM_THREADS environment variable. On the Grex software stack, it is set by the MKL module to 1 to prevent accidental CPU oversubscription. Redefine it in your SLURM job scripts if you really need threaded MKL execution as follows:\nexport MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK We use the MKL\u0026rsquo;s BLAS and LAPACK for compiling R and Python\u0026rsquo;s NumPY package on Grex, and that\u0026rsquo;s one example when threaded MKL can speed up computations if the code spends significant time in linear algebra routines by using SMP.\nMKL distributions also include ScaLAPACK and FFTW libraries.\nOpenBLAS# The successor and continuation of the famous GotoBLAS2 library. It contains both BLAS and LAPACK in a single library, libopenblas.a . Only the BLAS portion of the library is CPU-optimized though, so performance of LAPACK would lag behind Intel MKL. Use \u0026lsquo;\u0026lsquo;module spider openblas\u0026rsquo;\u0026rsquo; to find available versions for a given compiler suite. We provide both 32-bit and 64-bit indexes versions (and reflect it in the version names, like openblas/0.3.7-i32). The performance of OpenBLAS is close to that of MKL.\nOpenBLAS does not contain ScaLAPACK, which would have to be loaded as a separate module.\nAMD AOCL# AMD provides its vendor-optimized version of Blis and FLAME libraries which are modern, C++ template based implementations of BLAS and LAPACK. Use \u0026lsquo;\u0026lsquo;module spider aocl\u0026rsquo;\u0026rsquo; to see how to load it. AOCL also includes ScaLAPACK for OpenMPI.\nScaLAPACK# ScaLAPACK is \u0026ldquo;a library of high-performance linear algebra routines for parallel distributed memory machines.\u0026rdquo; It is almost always used together with an optimized BLAS/LAPACK implementation. Being a parallel library, ScaLAPACK depends on BLACS, which in turn depends on an MPI library.\nIntel MKL includes ScaLAPACK with support of both IntelMPI and OpenMPI for the BLACS layer. MKL also provides ScaLAPACK in both 32bit and 64bit interfaces. It is thus necessary to pick the right library to link against. The command line advisor is helpful for that.\nAMD AOCL includes a single ScaLAPACK library compatible with OpenMPI 4.1.\nOpenBLAS does not come with ScaLAPAC and needs the separate module loaded for the later.\nFast Fourier Transform (FFTW)# FFTW3 is the standard and well performing implementation of FFT. module spider fftw should find it. There is a parallel version of the FFTW3 that depends on MPI it uses, thus to load the fftw module, compiler and MPI modules would have to be loaded first. MKL also provides FFTW bindings, which can be used as follows:\nEither Intel or GCC MKL modules would set the MKLROOT environment variable and add necessary directories to LD_LIBRARY_PATH. The MKLROOT is handy when using explicit linking against libraries. It can be useful if you want to select a particular compiler (Intel or GCC), pointer width (the corresponding libraries have suffix _lp64 for 32-bit pointers and_ilp64 for 64 bit ones; the later is needed for, for example, Fortran codes with INTEGER*8 array indexes, explicit or set by -i8 compiler option) and a kind of MPI library to be used in BLACS (OpenMPI or IntelMPI which both are available on Grex). An example of the linker options to link against sequential, 64 bit pointed version of BLAS, LAPACK for an Intel Fortran code is:\nifort -O2 -i8 main.f -L$MKLROOT/lib/intel64 -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core -lpthread -lm MKL also has FFTW bindings. They must be enabled separately from the general Intel compilers installation; and therefore, details of the usage might be different between different clusters. On Grex, these libraries are present in two versions: 32-bit pointers (libfftw3xf_intel_lp64) and 64-bit pointers (fftw3xf_intel_ilp64). To link against these FFT libraries, the following include and library options to the compilers can be used (for the _lp64 case):\n-I$MKLROOT/include/fftw -I$MKLROOT/interfaces/fftw3xf -L$MKLROOT/interfaces/fftw3xf -lfftw3xf_intel_lp64 The above line is, admittedly, rather elaborate but gives the benefit of compiling and building all the code with MKL, without the need for maintaining a separate library such as FFTW3.\nAOCL provides an optimized FFTW dynamic library included in the aocl module.\nHDF5 and NetCDF# Popular hierarchical data formats. Two versions exist on the Grex software stack, one serial and another MPI-dependent version. Which one you load depends whether MPI is loaded.\nTo see the available versions, use:\nmodule spider hdf5 and/or:\nmodule spider netcdf Python# There are modules for Python versions that we build from source using optimizations specific to our HPC hardware.\nNote that the base OS python should in most cases not be used; rather find and use a module!\nmodule spider python We do install certain most popular python modules centrally. pip list would show the installed modules.\nR# We build R from sources and link against MKL. We find that some packages would only work with GCC-compiled versions of R, so R requires using one of GCC toolchains.\nmodule spider \u0026#34;r\u0026#34; Several of the most popular R packages are installed with the R modules on Grex. Note that it is often the case that R packages are bindings for some other software (JAGS, GEOS, GSL, PROJ, etc.) and require the software or its dynamic libraries to be available at runtime. This means, the modules for the dependencies (JAGS, GEOS, GSL, PROJ) are also to be loaded when R is loaded.\n"},{"id":"24","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2025/lora-huggingface/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2025 / LoRa walkthrough","title":"LoRa walkthrough","content":"High Performance Computing Workshop - Oct 14-17, 2025# This is an example of training LoRa in a batch job. The example uses a script from Huggingface diffusers package. We assume the partiticipant already tried simple text to image generation with SD 1-5 as per 02-text-to-image-ipynb notebook.\nPull Huggingface diffusers, copy data# We will downolad Huggingface source code from their Github using Git. This is needed to use their Python example training script rather than developing ours from scrath. We will also copy the datasets for training from pythonai subdirectory of our Workshop materials.\npwd # on Grex. lets work from Project filesystem rather than Home! # # do: cd $HOME/projects/def-your-pi/your-user # # assuming the current directory is under your project as per above cp -r /global/software/ws-oct2025/pythonai . cd ./pythonai pwd \u0026amp;\u0026amp; ls # should see /home/user/pythonai ; dataset1 dataset2 notebooks Magic Castle has no GPUs as of October 2025! So please ignode the MC instructions below!\n# on MC cp -r /home/shared/pythonai ~/scratch/ cd ~/scratch/pythonai pwd \u0026amp;\u0026amp; ls # should see ; dataset1 dataset2 notebooks # clone the HF repository and scripts git clone https://github.com/huggingface/diffusers.git ls # should see disffusers directory added We will more or less follow Diffusers source installation in an interactive job.\nStart an interactive job on a GPU node# We will use workshop reservation ws_gpu and (any of) reserved GPU partitions. Will need one GPU. Please add \u0026ndash;account= if you have more than one active.\nsalloc --time=0-2:00 --partition=agro-b,mcordgpu-b --gpus=1 --cpus-per-gpu=6 --mem=50gb --reservation=ws_gpu We should see a GPU information from nvidia-smi there. Will get either a V100 or A30.\nCreate a virtualenv and instal packages# We will need to load CUDA and Python 3.12. To this end, use module spider python/3.12 and pick the version that has a CUDA dependency.\n#first, load modules #on Grex module purge module load SBEnv # loading according to spider, need the CUDA version! module load cuda/12.4.1 arch/avx2 gcc/13.2.0 python/3.12 python --version \u0026gt;\u0026gt; Magic Castle has no GPUs as of October 2025! So please ignode the MC instructions below! # on MC module load StdEnv/2023 arch/avx2 cuda python/3.12 python --version Now that modules are loaded, we can create a new virtualenv here and call it hf\nvirtualenv hf source hf/bin/activate #installing packages . This can be flakey! use line by line and see if no previous step failed pip install torch==2.8.0 arrow transformers datasets peft accelerate torchvision==0.23.0 pip install git+https://github.com/huggingface/diffusers python -c \u0026#34;import torch\u0026#34; deactivate We got our Diffusers virtual environment now, and it should be working! Hopefully. We will use it in interactive and batch jobs from now on.\nRun a test LoRa text-to-image training on dataset1# # now actiate the environment source hf/bin/activate # change directory to the Examples we cloned cd diffusers/examples/text_to_image pwd ls #must have train_text_to_image_lora.py amongst the files there # this is what we are goung to use! Note the path to dataset1 ../../../dataset1/train python ./train_text_to_image_lora.py --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 --train_data_dir ../../../dataset1/train --output_dir ../../../lora-sd-output --resolution 256 --train_batch_size 1 --max_train_steps 200 # go to the results directory and see if there are new weights cd ../../../lora-sd-output pwd ls # should show something like pytorch_lora_weights.safetensors # now exit from the salloc job! exit Note that we use resolution 256 on the dataset1.\nDebug , try to make it run and deliver the new Weight file under the output directory.\nRun production batch jobs on dataset2# Now that we are sure that a LoRa training environment is good, lets try to run it as a production batch job We will change to the larger dataset 2 and use the following script (also provided under pythonai ).\nAlways use the same modules and virtualenv! The job script below is for Grex.\nIf needed change directory to the project filesystem on Grex.\n# must be in /home/your_user/projects/def-your_project/your_user/pythonai sbatch trainingjob.sh # the above may need --account= if you have more than one PI The trainingjob.sh script looks as follows:\n#!/bin/bash #SBATCH --reservation=ws_gpu #SBATCH --partition=agro-b,mcordcpu-b #SBATCH --cpus-per-task=6 #SBATCH --gpus=1 #SBATCH --mem=40000 #SBATCH --time=0-3:0:00 # Load requested software stack module load SBEnv module load cuda/12.4.1 arch/avx2 gcc/13.2.0 module load python/3.12 # now actiate the environment source hf/bin/activate echo \u0026#34;Starting run at: `date`\u0026#34; python ./diffusers/examples/text_to_image/train_text_to_image_lora.py --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 --train_data_dir dataset2/train --output_dir lora-sd-output2 --resolution 512 --train_batch_size 1 --max_train_steps 1200 echo \u0026#34;Program finished with exit code ${?} at: `date`\u0026#34; # use SD-1.5 with added LoRa-optimized weights# Start a Jupyter job on Grex OOD or Magic Castle.\nIn the notebooks folder, open a 03-text-to-image-lora.ipynb , correct path to the updated LoRa weights and run the inference again. Try also merging the two LoRa weights trained from dataset1 and dataset2.\n"},{"id":"25","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2024/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2024","title":"Workshops - 2024","content":"Autumn workshop, October 2024# This workshop has been created for a group of users from Bannatyne campus to explain the usage of contributed nodes. However, most of the material from the following slides (slurm, software, singularity, podman, \u0026hellip;) is still valid for general use. Below are the slides from the workshop that was held on October 31 and November 1, 2024:\nStart Guide for Using Grex efficiently: Slides Containers in HPC - Singularity/Apptainer: Slides Containers in HPC – Podman: Slides Fall DATA-4010 seminars, September 2024# Below are the slides from the DATA 4010 seminars that were held on September 18 and 25, 2024:\nIntroduction to Version Control Systems 1: Slides Introduction to Version Control Systems 2: Slides "},{"id":"26","rootTitleIndex":"4","rootTitle":"Connecting to Grex","rootTitleIcon":"fa-solid fa-plug fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex","permalink":"/grex-docs/connecting/ood/","permalinkTitle":"Homepage / Connecting to Grex / Connect and Transfer data with OOD","title":"Connect and Transfer data with OOD","content":"OpenOnDemand (OOD) is a Web portal application for High-Performance Computing systems. An OOD portal is available on Grex at https://ood.hpc.umanitoba.ca ; it can be used to Connect to Grex, Transfer data and and run both interactive and batch Applications. For more information about how to connect and use OOD, please refer to the main OOD page.\nInternal links# Use OOD Files app to move data: OOD Files Run applications with OOD External links# Homepage of the OnDemand project "},{"id":"27","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/containers/","permalinkTitle":"Homepage / Software and Applications / Containers for Software","title":"Containers for Software","content":"Introduction# Linux Containers are means to isolate software dependencies from the base Linux operating system (OS). Several different Linux container engines exist, most notably Docker which was first to emerge as the most popular tool in the DevOps community.\nSince then, a lot of work had been done by major Linux players like Google, RedHat and others to develop an open standard for container runtimes, which developed based on Docker, OCI .\nThere are HPC-specific container engines/runtimes that offer similar or equivalent functionality but allow for easier integration with shared Linux HPC systems. At the time of writing, the most widely used is the Singularity container system, developed by a company called SyLabs, and its fork, a Linux Foundation project called Apptainer . They are compatible with each other. Singularity/Apptainer provides functionality for running most Docker images by converting them to the Singularity Image format (SIF). However, Singularity/Apptainer own format is not completely OCI-compatible , so there exists Docker images that would not work properly.\nFinally, recent developments in Linux Kernel namespaces allowed to happen such projects as \u0026ldquo;rootless Docker\u0026rdquo; and \u0026ldquo;rootless Podman \u0026rdquo; which are more suitable for HPC systems than the original Docker implementation which requires privileged access to the Linux system.\nOn Grex, Sylabs Singularity-CE is supported on local SBEnv software stack, while Apptainer is supported as part of the ComputeCanada/Alliance CCEnv stack. At the time of writing, these engines can be used largely interchangeably.\nNew: There is also support for rootless Podman on Grex, for the use cases that require full OCI-compatibility. Using Singularity from SBEnv on Grex# A brief introduction on getting started with Singularity can be useful to get started. You will not need to install Singularity on Grex since it is already provided as a module.\nStart with module spider singularity; it will list the current version. Due to the nature of container runtime environments, we update Singularity regularly, so the installed version is usually the latest one. Load the module (in the default Grex environment) by the following command:\nmodule load singularity With singularity command, one can list singularity commands and their options:\nsingularity help To execute an application within the container, do it in the usual way for that application, but prefix the command with singularity exec image_name.sif or, if the container has a valid entry point, execute it with singularity run image_name.sif.\nsingularity run docker://ghcr.io/apptainer/lolcow In the example above, Singularity downloads a Docker image from a registry, and runs it instantly. It is advisable, to avoid getting banned by container registries for massive downloads off a single HPC system, to \u0026ldquo;pull\u0026rdquo; or \u0026ldquo;build\u0026rdquo; containers first as images, and then \u0026ldquo;run\u0026rdquo; and \u0026ldquo;exec\u0026rdquo; them locally.\nsingularity pull lolcow_local.sif docker://ghcr.io/apptainer/lolcow # The above should create a local image lolcow_local.sif # Lets run it with singularity singularity run lolcow_local.sif For another example, to run R on an R script, using an existing container image named R-INLA.sif (INLA is a popular R library installed in the container):\nsingularity exec ./R-INLA.sif R --vanilla \u0026lt; myscript.R Quite often, it is necessary to provide the containerized application with data residing outside of the container image. For running HPC jobs, the data usually resides on a shared filesystem such as /home or /project. This is done via bind mounts . Normally, the container bind-mounts $HOME, /tmp and the current working directory. It is possible to mount a subdirectory, such as $PWD/workdir, or an entire filesystem such as /project. Example below bind-mounts a ./workdir folder relative to the current path. The folder must exist before being bind-mounted.\nsingularity exec -B `pwd`/workdir:/workdir ./R-INLA.sif R --vanilla \u0026lt; myscript.R In case you do not want to mount anything to preserve the containers\u0026rsquo; environment from any overlapping of data/code from say $HOME, use the --containall flag.\nSome attention should be paid to Singularity\u0026rsquo;s local cache and temporary directories. Singularity caches the container images it pulls and Docker layers under $HOME/.singularity. Containers can be large, in tens of gigabytes, and thus they can easily accumulate and exhaust the users\u0026rsquo; storage space quota on $HOME. Thus, users might want to set the SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR variables to some place under their /global/scratch space.\nFor example, to change the location of SINGULARITY_CACHEDIR and SINGULARITY_TMPDIR, before building the singularity image, one might run:\nmkdir -p /global/scratch/$USER/singularity/{cache,tmp} export SINGULARITY_CACHEDIR=\u0026#34;/global/scratch/$USER/singularity/cache\u0026#34; export SINGULARITY_TMPDIR=\u0026#34;/global/scratch/$USER/singularity/tmp\u0026#34; In the above example, the directory refers to /global/scratch which is not available on grex for now. Therefore, we recommend to replace the path /global/scratch/$USER/singularity by a location under your project directory /home/$USER/projects/def-professor/$USER/singularity where professor refers to the user name of your sponsor. Getting and building Singularity images# The commands singularity build and/or singularity pull would get pre-built Singularity images from DockerHub, SingularityHub, or SyLabsCloud. “pulling” images does not require elevated permissions (that is, sudo). There are several kinds of container repositories from which containers can be pulled. These repositories are distinguished by the URI string (library://, docker://, oras://, etc.). It is also possible to convert a local Podman image (using the oci-archive:// URI ) into a Singularity image. Refer to the Podman section of this document below.\nmodule load singularity # Building Ubuntu image using Sylabs Library singularity build ubuntu_latest0.sif library://ubuntu # or using DockerHub, this will create ubuntu_latest.sif singularity pull docker://ubuntu:latest # or, using a local definition and --fakeroot option singularity build --fakeroot My_HPC_container.sif Singularity.def # or, using a suitable local OCI image from Podman singularity build busybox.sif oci-archive://busybox.tar Singularity (SIF) images can also be built from other local images, local “sandbox” directories, and from recipes. A Singularity recipe or definition file is a text file that specifies the base image and post-install commands to be performed on it. However, Singularity-CE requires sudo (privileged) access to build images from recipes, which is not available for users of HPC machines. There are two solutions to this problem.\nUsing remote build on Sylabs cloud with --remote option. This requires setting up a free account on Sylabs and getting access key . Using Singularity-CE or Apptainer (see below) with --fakeroot option. Make sure you understand licensing and intellectual property implications before using remote build services!\nThe fakeroot method appears to be easier and does not require an external account. As of the time of writing, both Apptainer and Singularity-CE support the fakeroot build method. On Grex, there could be differencences between running Apptainer build in an interactive job and on a login node. Running builds in a salloc job is preferred also because login nodes do limit memory and CPU per session.\nSingularity with GPUs# Add the --nv flag to singularity run / exec / shell commands for the container to be able to access NVidia GPU hardware. Naturally, your job should be on a node that has a GPU to use GPUs . Check out our Running Jobs documentation to find out which partitions have the GPU hardware. NVIDIA provides many pre-built Docker and Singularity container images on their GPU cloud , together with instructions on how to pull them and to run them. NVidia\u0026rsquo;s NGC Docker images should, as a rule, work on HPC machines with Singularity without any changes.\nSingularity with OpenScienceGrid CVMFS# We can run Singularity containers distributed with OSG CVMFS which is currently mounted on Grex\u0026rsquo;s CVMFS. The containers are distributed via CVMFS as unpacked directory images. So, the way to access them is to find a directory of interest and point singularity runtime to it. The directories will then be mounted and fetched automatically. The repository starts with /cvmfs/singularity.opensciencegrid.org/. Then you\u0026rsquo;d need an idea from somewhere what you are looking for in the subdirectories of the above-mentioned path. An example (accessing, that is, exploring via singularity shell command, Remoll software distributed through OSG CVMFS by jeffersonlab):\nmodule load singularity singularity shell /cvmfs/singularity.opensciencegrid.org/jeffersonlab/remoll\\:develop It looks like the list of what is present on the OSG CVMFS is on GitHub: OSG GitHub docker images .\nUsing Apptainer from CCEnv on Grex# The Alliance\u0026rsquo;s (formerly ComputeCanada) software stack now provides Apptainer modules in the two latest Standard Environments, StdEnv/2020 and StdEnv/2023. Most recent Apptainer versions (1.2.4 and older) do not require \u0026ldquo;suexec\u0026rdquo; and thus can be used off the CVMFS as usual. The only caveat would be to first unload any \u0026ldquo;singularity\u0026rdquo; or \u0026ldquo;apptainer\u0026rdquo; modules from other software stacks by module purge. Apptainer on the CCEnv stack is installed in suid-less mode.\nThe following commands show how to run the image from the previous example /R-INLA.sif:\nmodule purge module load CCEnv module load arch/avx512 module load StdEnv/2023 module load apptainer # testing if apptainer command works apptainer version # running the basic example apptainer run docker://ghcr.io/apptainer/lolcow Similarly to Singularity, you will need to bind mount the required data directories for accessing data outside the container. The same best practices mentioned above for Singularity (pulling containers beforehand, controlling the cache location) equally apply for the Apptainer. The environment variables for Apptainer should be using APPTAINER_ instead of SINGULARITY_ prefixes.\nBuilding apptainer images with \u0026ldquo;fakeroot\u0026rdquo;# Apptainer supports building of SIF images from recipes without sudo access using --fakeroot option where available.\nOn Grex, it can be used as in the following example:\nmodule purge module load CCEnv module load StdEnv/2023 module load apptainer # testing if apptainer command works apptainer version # build an image, INLA, from a local Singularity.def recipe # --fakeroot makes the build possible without elevated access apptainer build --fakeroot INLA.sif Singularity.def The resulting SIF image should be compatible with either Singularity-CE or Apptainer runtimes.\nUsing Podman from SBEnv on Grex# Podman modules are now provided under the default Grex SBEnv environment. On Grex, Podman is configured as rootless. Podman is meant to be used by experienced users for jobs that cannot be executed as regular binaries, or through Singularity due to OCI requirements. Grex is an HPC systems, so it is expected that users would be using Podman to run compute jobs rather than persistent services (including and not limited to databases, and network services). Thus, Podman jobs and/or running Podman containers that deemed to be inappropriate for HPC may be terminated without notice.\nIt is not recommended to run Podman on login nodes; it should be run only on compute nodes (e.g. using sbatch or salloc).\nThe only allowed use on login nodes is to pull images before actually starting a job.\nAccess to the Podman runtime is through a module. Due to the nature of the container runtime environment, we strive to update Podman regularly, so in most cases, the latest installed version must be used:\nmodule load podman podman version podman run --rm docker.io/godlovedc/lolcow The above command would pull a container image and run it.\nGetting and Managing Podman images# When using Podman to run a job, we suggest to manually pre-download the required image to avoid wasting time during the job. Grex is hosting a Docker Registry proxy/cache locally to improve the download performance, and for avoiding rate limits that can be imposed by various container registries.\nmodule load podman podman pull \u0026lt;REQUIRED_IMAGE\u0026gt; The command podman pull image_name would get Podman images from a container registry.\nImages can also be built from other images, or from containerfiles (e.g. Dockerfiles) using the command podman build Containerfile. A containerfile is a text \u0026ldquo;recipe\u0026rdquo; that specifies the base image and commands to be run on it. Podman\u0026rsquo;s recipes are compatible with Docker\u0026rsquo;s Dockerfiles. Below is an example of building a container from a source tree of a package called Chemprop. The Dockerfile is provided by the authors.\n# Lets try to build a local container from chemprop source directory module load podman # change the directory to the source tree where Dockerfile is located cd chemprop-1.7.1 podman build -t chemprop . # this creates a local container image \u0026#34;localhost/chemprop latest\u0026#34; Podman, as configured on Grex, by default stores all pulled and locally built images inside the Slurm TMPDIR directory that is local to the node. This means that images will be deleted when the job finishes or get cancelled.\nTo list the local images, users can take advantage of the following Podman commands:\n# list images podman image ls # delete an unnecessary image podman image rm \u0026lt;IMAGE_ID\u0026gt; However, the local images as listed by image ls are local to the compute node they were pulled or created. That is, they are not visible on other nodes, or even the same node when the job ends, and would have to be pulled again or built again from the Containerfile.\nTo avoid rebuilding the same image every time the job runs, users can take advantage of podman save and podman load commands that allow for storing the local container as a single image file. Note that these files are large, and it is the user\u0026rsquo;s responsibility to manage their Podman images (delete the old/unused ones)):\n# after building an image locally, save it to a file podman save --format=oci-archive -o ${HOME}/my-local-image.tar \u0026lt;LOCALLY_BUILT_IMAGE\u0026gt; # when running a job that needs that image, load it podman load -i ${HOME}/my-local-image.tar Note that it is also possible to convert podman OCI archives into Singularity .sif images as described above, using oci-archive://my-local-image.tar syntax for the source image.\nPodman with User Namespaces# OCI containers may contain multiple users in the container. For example, a Jupyter container would have the jovian user, a Mambaforge container would have the user mambauser, etc., in addition to the root user inside container. To properly run such contains with rootless Podman on HPC, an explicit switch --userns=keepid with corresponding userID and groupID mappings has to be provided to Podman.\n# run a container with userID mapping for a user with id 1000 # while binding the home directory of the \u0026#34;outside\u0026#34; user podman run -it --userns=keep-id:uid=1000,gid=1000 -v ${HOME}:${HOME} \u0026lt;REQUIRED_IMAGE\u0026gt; Since the user and group ID in arbitrary containers are not known a priori, it is sometimes needed to use the id command when running the container to get the user and group IDs first.\nPlease refer to the Podman documentation on User namespace options for more information.\nNamespaces may cause issues when bind-mounting folders not owned by the user\u0026rsquo;s primary group. Unfortunately, such folders are all the folders of the sponsored users on the /project filesystems, that require access to supplemental groups to be accessible. If you encounter such issues when using containers with --keep-id, please add --annotation run.oci.keep_original_groups=1 to the podman command line options.\nPodman with GPUs# Use the --device=nvidia.com/gpu=all flag to the podman command when running a podman container that needs GPU hardware. Naturally, your job should be on a node that has a GPU to use GPUs . Check out our Running Jobs documentation to find out which partitions have the GPU hardware. NVIDIA provides many pre-built Docker container images on their NGC Cloud , together with instructions on how to pull and run them. Many software developers also put images to various container registries, such as DockerHub or Quay.io, or distribute the sources of containers along with Dockerfiles; all these can be used, but \u0026ldquo;your mileage may vary\u0026rdquo;. Podman would usually run Docker containers without changes to the command line parameters. It is a good idea to run a test job for a given container and use nvidia-smi command on the node the job runs on to check whether the image is able to utilize the GPU(s).\nExternal links# Singularity/Sylabs homepage Apptainer homepage Podman homepage Apptainer documentation on the Alliance Wiki Docker Hub RedHat Quay.io Hub Sylabs Cloud NVIDIA NGC Cloud "},{"id":"28","rootTitleIndex":"10","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-cloud fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/desktops/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal / Desktops","title":"Desktops","content":"Introduction# From the menu Interactive Apps, it is possible to start a given application and use it for interactive work. Some applications have already predefined settings, like the slurm partition and allow only to adjust the wall time. As an example, the Grex Desktop Simplified that uses by default the test partition.\nTo start an application, click on the name of the application, then fill the required parameters for slurm (accounting group, partition name, number of cpus, wall time, email notifications, \u0026hellip;etc).\nMost of the applications require to set up the following parameters:\nSlurm accounting group: if you have access to one accounting group (def-professor), the scheduler will pick it by default. If you have more than one accounting group, you need to specify the accounting group to use. Wall time: crrently, there is a limit of 6 hours for most of the applications. E-mail notification: this is an optional feature to get email notfications about your job. You will need to provide a working email. Other parameters will be introduced in the next sections about custom interactive applications like for MATLAB, OVITO, \u0026hellip; etc. Here, we will discuss the usage of the Desktops: Grex Desktop Simplified and Grex Desktop.\nGrex Desktop Simplified# This is a very simplified instance of a Desktop. It requires only to setup the accounting group (def-professor), the wall time and if needed add the email notifications. The following snapshot shows an example of settings:\nGrex Desktop Simplified setup Once all the fields are filled, you can lauch the application using the Launch button. This will submit a job to compute node (in this case, it will run on the test partition). First, it goes to the queue and start when the resources are available:\nGrex Desktop Simplified Queued The Desktop starts as a job. Therefore, it may stay on the queue depending on the available resources. Once the resources are granted by the scheduler, the interactive app will show up as running:\nGrex Desktop Simplified Running At this point, you may want to decrease the compression and image quality for not having a slow application.\nThen, click on the buttom Launch Grex Desktop Simplified to start the application.\nAfter starting the Desktop, you will have access to your files and a terminal. In the following snapshot, it shows a terminal where we loaded matlab module and started matlab interface. Note that there is a dedicated application for matlab that you can use. We are loading matlab here to show how the desktop looks like. The Desktop can be used to start any other program that uses GUI.\nGrex Desktop Simplified Demonstration Grex Desktop# As discussed in the previous section, Grex Desktop Simplified does not require many parameters beside the wall time that can be set in the form. It only runs on one node or a predefined test partition. For more flexibility, there is another Desktop called Grex Desktop. This later offers more choices for the parameters to set before launching the application.\nFor Desktops, the following parameters can be set from the form before submitting the interactive app:\nLinux Desktop Environmemnt: From the drop down menu, a user can choose one of the following Desktop Environmemnts: IceWM, OpenBox or FluffBox.\nSlurm accounting group: if you have access to one accounting group (def-professor), the scheduler will pick it by default. If you have more than one accounting group, you need to specify which one to use.\nSLURM partition: The Grex Desktop Simplified uses only a predeined partition test. Using Grex Desktop application, there is a possibility to choose the partition. In the same way as running jobs via salloc or sbatch, a user can specify which CPU or GPU partition to use for interactive Desktop. Below the drop down menu, there is a brief reference to the available partitions and the characteristics of the nodes on each partition. Use the menu to select the appropriate partition for your interactive session. Please make sure to pick the right partition for your application. For example, do not select GPU partition if your program is not optimized for GPU usage.\nNumber of cores: The number of CPUs can be set from the form. Note that we have set a maximum limit of the number of CPUs to use with OOD to limit the waste of resources. This limit is 64 for the partitions that have more than 64 cores per node, like genoa partition. For other partitions, it is limitted by the available physical core on the partition {52 for skylake partition, 40 for largemem partition}. The total memory for the job is set automatically when selecting the number of CPUs and the partition. For each partition, there is a base memory per CPU which is shown just below the field SLURM partition. For example, if you select _genoa partition and asked for 8 cores, the total memory will be 8 x 4000M. If needed for more memory, increase the number of CPUs.\nWall time: crrently, there is a limit of 6 hours for most of the applications.\nE-mail notification: this is an optional feature to get email notfications about your job. You will need to provide a working email.\nThe following screenshot shows an example of Grex Desktop Settings:\nGrex Desktop Settings After launching the Desktop, the job will be submitted to the queue and wait for resources and once available, the job will start. At this time you may also change the compression and image quality to something low. Otherwise, the interaction with the interface will be slower. Then click on the button with the title Launch Grex Desktop to connect to the interface.\nSimular to Grex Desktop Simplified, you will have access to a terminal and some other tools like a text editor.\nHere is an example of Grex Desktop view where two terminals were opened. One is used to test the GUI by running xeyes and another to load and start matlab:\nGrex Desktop View "},{"id":"29","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2023/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2023","title":"Workshops - 2023","content":"Autumn workshop, November 2023# Below are the slides from the Grex workshop that was held on November 6-8, 2023:\nUpdates on Research Computing Resources: Slides Getting accounts, connecting, using Duo MFA: Slides Basics of Linux Shell: Slides Beginner running HPC jobs with SLURM: Slides Using HPC with OpenOnDemand Web portal: Slides Beginner HPC software overview: Slides Using containers in HPC (Apptainer, SingularityCE): Slides Basics of using OpenStack cloud: Slides Getting most of HPC system: Slides Fall DATA-4010 seminars, September 2023# Below are the slides from the DATA 4010 seminars that were held on September 18 and 25, 2023:\nIntroduction to Version Control Systems 1: Slides Introduction to Version Control Systems 2: Slides Spring workshop, May 2023# Below are the slides from the Grex workshop that was held on May 17-19, 2023:\nUpdates on Research Computing Resources: Slides Basics of Linux Shell: Slides Beginner\u0026rsquo;s Introduction to Use HPC Machines: Slides Beginner\u0026rsquo;s HPC Software - Overview: Slides Using OpenOnDemand Web Portal on Grex - Live Demonstration: Slides Using HPC Clusters Efficiently (SLURM, Kinds of jobs): Slides Advanced HPC Software (Containers, CVMFS): Slides Using GPUs on HPC Systems - Live Demonstration: Slides Beginner’s Introduction for Using Cloud Computing (OpenStack): Slides Overview of Further Training and Materials Available: Slides "},{"id":"30","rootTitleIndex":"7","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-person-running fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/contributed-systems/","permalinkTitle":"Homepage / Running jobs on Grex / Scheduling policies and running jobs on Contributed nodes","title":"Scheduling policies and running jobs on Contributed nodes","content":"Scheduling policies for contributed systems# Grex has a few user-contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the \u0026ldquo;preferred access\u0026rdquo; is preemption.\nOn the definition of preferential access to HPC systems# Preferential access is when you have non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have. HPC makes access to CPU cores / GPUs / Memory exclusive per job, for the duration of the job (as opposed to time-sharing). Priority is a factor that decides which job gets to start (and thus exclude other jobs) first if there is a competitive situation (more jobs than free cores).\nThe owner is the owner of the contributed hardware. Others are other users. A partition is a subset of the HPC system’s compute nodes.\nPreemption by partition: the contributed nodes have a SLURM partition on them, allowing the owner to use them, normally, for batch or interactive jobs. The partition is a “preemptor”. There is an overlapping partition, on the same set of the nodes but for the others to use, which is “preemptible”. Jobs in the preemptible partition can be killed after a set “grace period” (1 hour) as the owner\u0026rsquo;s job enters the \u0026ldquo;preemptor\u0026rdquo; partition. If works, preempted jobs might be check-pointed rather than killed, but that’s harder to set up. Currently, it is not generally supported. If you have a code that supports checkpoint/restart at the application level, you can get most of the contributed nodes.\nOn Grex, the \u0026ldquo;preemptor\u0026rdquo; partition is named after the name of the owner PI, and the preemptible partitions named similarly but with added -b suffix. Use the --partition= option to submit the jobs with sbatch and salloc commands to select the desired partition.\nContributed Nodes# As of now, the preemptible partitions are:\nPartition Nodes GPUs/Node CPUs/Node Mem/Node Notes stamps-b 1 3 4 32 187 Gb AVX512 livi-b 2 1 16 48 1500 Gb NVSwitch server agro-b 3 2 2 24 250 Gb AMD mcordcpu-b 4 5 - 168 1500 Gb ** AMD EPYC 9634 84-Core** mcordgpu-b 5 5 - 168 1500 Gb AMD EPYC 9634 Partition \u0026ldquo;stamps-b\u0026rdquo;# To submit a GPU job to stamps-b partition, please include the directive:\n#SBATCH --partition=stamps-b in your job script or submit your jobs using:\nsbatch --partition=stamps-b run-gpu-job.sh assuming that run-gpu-job.sh is the name of your job script.\nHere is an example of script for running LAMMPS job on this partition:\nScript example for running LAMMPS on **stamps-b** partition run-lmp-gpu.sh#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=stamps-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Partition \u0026ldquo;livi-b\u0026rdquo;# To submit a GPU job to livi-b partition, please include the directive:\n#SBATCH --partition=livi-b in your job script or submit your jobs using:\nsbatch --partition=livi-b run-gpu-job.sh assuming that run-gpu-job.sh is the name of your job script.\nHere is an example of script for running LAMMPS job on this partition:\nScript example for running LAMMPS on **livi-b** partition run-lmp-gpu.sh#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=livi-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Partition \u0026ldquo;agro-b\u0026rdquo;# To submit a GPU job to agro-b partition, please include the directive:\n#SBATCH --partition=agro-b in your job script or submit your jobs using:\nsbatch --partition=agro-b run-gpu-job.sh assuming that run-gpu-job.sh is the name of your job script.\nHere is an example of script for running LAMMPS job on this partition:\nScript example for running LAMMPS on **agro-b** partition run-lmp-gpu.sh#!/bin/bash #SBATCH --gpus=1 #SBATCH --partition=agro-b #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-3:00:00 # Load the modules: module load intel/2020.4 ompi/4.1.2 lammps-gpu/24Mar22 echo \u0026#34;Starting run at: `date`\u0026#34; ngpus=1 ncpus=1 lmp_exec=lmp_gpu lmp_input=\u0026#34;in.metal\u0026#34; lmp_output=\u0026#34;log-${ngpus}-gpus-${ncpus}-cpus.txt\u0026#34; mpirun -np ${ncpus} lmp_gpu -sf gpu -pk gpu ${ngpus} -log ${lmp_output} -in ${lmp_input} echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; stamps-b: GPU [4 - V100/16GB] nodes contributed by Prof. R. Stamps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nlivi-b: GPU [HGX-2 16xGPU V100/32GB] node contributed by Prof. L. Livi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nagro-b: GPU [AMD Zen] node contributed by Faculty of Agriculture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmcordcpu-b GPU nodes contributed by Prof. Marcos Cordeiro (Department of Agriculture).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nmcordgpu-b CPU nodes contributed by Prof. Marcos Cordeiro (Department of Agriculture).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"31","rootTitleIndex":"10","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-cloud fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/interactive-apps/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal / Custom Interactive Apps","title":"Custom Interactive Apps","content":"Introduction# For the common applications used on Grex, we have addedd separate interactive applications to make it easier for users. Here is a list of the applications, but nit limited to: Gaussview, Matlab, Mathematica, Relio, Paraview, OVITO, \u0026hellip; etc. Some of the applications may not show up in your dashboard as they are protected with a POSIX group. For example, if you do not have access to Gaussian , Gaussview will not show up in your dashboard. Other applications like stata, metashape are commercial. Therefore, they are set only for the groups who purchased the license for these programs. Access to these applications is restricted via a POSIX group.\nWhen launching these applications, they start automatically the corresponding program (application), like for matlab interface, Gaussview interface, OVITO, Mathematica, \u0026hellip;. They are all configured to work like Grex Desktop with two exceptions:\nThey start automatically the interface without having to start a terminal, then load a module and start the interface. Everything is pre-configured to start automaticaly top open the interface. For some of the applications, there is also a possibility to choose which module to use. It is another step in the form where other resources are set. Here are some snapshots for Gaussview, Matlab, Paraview:\nA snapshot for Gaussview application: Gaussview Application: snapshot A snapshot for Gnuplot application: Gnuplot Application: snapshot A snapshot for Grace (XmGrace) application: Grace Application: snapshot A snapshot for Mathematica application: Grace Application: snapshot A snapshot for Matlab application: Matlab Application: snapshot A snapshot for OVITO application: OVITO Application: snapshot A snapshot for Paraview application: Paraview Application: snapshot "},{"id":"32","rootTitleIndex":"7","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-person-running fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/using-localdisks/","permalinkTitle":"Homepage / Running jobs on Grex / Using local disks: $TMPDIR","title":"Using local disks: $TMPDIR","content":"Introduction# High-Performance Computing (HPC) systems, such as Grex, typically provide a shared, scalable, POSIX-compliant filesystem that is accessible by all compute nodes. For Grex and the current generation of Alliance HPC machines, this shared filesystem is powered by Lustre FS , which enables data sharing for compute jobs running on the cluster\u0026rsquo;s nodes. Due to its scalable design, Lustre FS can offer significant bandwidth for large parallel jobs through its parallelized Object Storage Targets (OSTs).\nHowever, there are situations where parallel filesystems like Lustre can experience performance issues. For instance, when handling a very large number (tens of thousands) of small files, such workloads can be taxing on \u0026ldquo;metadata operations\u0026rdquo; (e.g., opening and closing files), even though they don’t require high data bandwidth.\nIn some extreme cases, it may be beneficial to avoid using the shared parallel filesystem and instead utilize local disk storage. Local disks, particularly SSDs, are directly attached to the node and do not strain the Metadata Servers of the shared filesystem. This approach may improve the performance of jobs that involve large numbers of small files.\nHowever, there are some limitations:\nThe node-local storage is temporary and will be deleted at the end of the SLURM job. The node-local storage is limited by what is available at the node (usually about 100-200GB). Large datasets would still have to use the /project filesystem. The node-local storage is local to the node, and is not available for multi-node parallel computing jobs. Generally, it is not worth the trouble to try using local disks manually, across several nodes. Using temporarily directory for SLURM jobs# SLURM automatically creates a local temporary directory for each job. On Grex, the path to this directory is accessible to the job via the TMPDIR environment variable.\nOn Alliance/Compute Canada systems, another environment variable, SLURM_TMPDIR, is set. Certain scripts within the CCEnv environment may expect this variable to be defined. If needed, it can be set as follows:\nexport SLURM_TMPDIR=$TMPDIR To use local storage for job\u0026rsquo;s data, you should stage the data into the local storage at the start of the SLURM job and stage it out at the end. Assuming the data is stored in a subdirectory relative to the submission path, here’s an example workflow:\n# Copy data to the local temporary directory: cp -r ./my-data-directory $TMPDIR pushd $TMPDIR # Run software that uses the data: # ... popd # Copy the data back from the local temporary directory cp -rf $TMPDIR/my-data-directory . Note: If possible, for a given software, it is advisable to direct output and checkpoint files to the parallel filesystem (e.g., /project). In case of job interruption (due to hardware failure or walltime expiration), data on the local disk will be lost. Using local scratch for particular software# Many codes provide a configuration option or an environment variable that determines, where to direct the temporary files. In such cases, when the application software handles temporary files, it is sufficient to set the location to $TMPDIR.\nExamples are Gaussian (export GAU_SCRDIR=$TMPDIR) , Quantum Espresso (export ESPRESSO_TMPDIR=$TMPDIR).\nExternal links# Alliance user documentation on Using node local storage "},{"id":"33","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2022/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2022","title":"Workshops - 2022","content":"Autumn workshop, October 2022# Below are the slides from the Grex workshop that was held on October 26-27, 2022:\nProgram and updates: Slides High Performance Computing: Start Guide: Slides High Performance Computing: software environments: Slides OSC OpenOnDemand portal on Grex: Slides Spring workshop, May 2022# Below are the slides from the Grex workshop that was held on May 4-5, 2022:\nIntroduction to local and National HPC at UManitoba: How to use available software and run jobs efficiently: Slides Introduction to High Performance Computing step by step: From getting an account to running jobs: Slides Using GP GPU compute on Grex: Slides High Performance Computing and software environments: Slides OSC OpenOnDemand portal on Grex: Slides "},{"id":"34","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/cern-vmfs/","permalinkTitle":"Homepage / Software and Applications / CVMFS and the Alliance software stack","title":"CVMFS and the Alliance software stack","content":"CC CernVMFS on Grex# CVMFS or CernVM FS stands for CernVM File System. It provides a scalable, reliable and low-maintenance software distribution service. CVMFS was originally developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications. Since then it has been used as a a generic way of distributing software. Presently, we use CernVMFS (CVMFS) to provide the Alliance\u0026rsquo;s (or Compute Canada\u0026rsquo;s) software stack. Through the Alliance CVMVS servers, several other publically available CVMFS software repositories are available as well. The examples are a Singularity/Apptainer repository from OpenScienceGrid , Extreme-Scale Scientific Software Stack E4S , and a Genomics software colection (GenPipes/MUGQIC) from C3G . Note that we can only \u0026ldquo;pull\u0026rdquo; the software from these repositories. To actually add or change software, datasets, etc., or receive support, the respective organizations controlling these CVMFS repositories should be contacted directly.\nAccess to the software and data distributed via CVMFS should be transparent to the Grex users: no action is needed other than loading a software module or setting a path. However, for accessing the Compute Canada software stack, a module should always be loaded to switch between software environments.\nGrex does not have a local CVMFS \u0026ldquo;stratum\u0026rdquo; (that is, a replica server). All we do is to cache the software items as they get requested. Thus, there can be a delay associated with pulling a software item for the first time, from the Alliance\u0026rsquo;s Stratum 1 (Replica Servers) located at the National HPC sites. It usually does not matter for serial programs but parallel codes, that rely on simultaneous process spawning across many nodes, might cause timeout errors. Thus, it could be useful to first access the codes in a small interactive job, or on a login node, to warm up Grex\u0026rsquo;s local CVMFS cache.\nThe Alliance\u0026rsquo;s software stack# The main reason for having CVMFS supported on Grex is to provide Grex users with the software environment as similar as possible with the environment existing on National Alliance\u0026rsquo;s HPC machines. On Grex, the module tree from Compute Canada software stack is not set as default, but has to be loaded with the following commands:\nmodule purge module load CCEnv module load arch/avx512 module load StdEnv/2023 After the above commands, use module spider to search for any software that might be available in the CC software stack.\nNote that \u0026ldquo;default\u0026rdquo; environments (the StdEnv and arch modules of the CC stack) are not loaded automatically, unlike on CC / Alliance general purpose (GP) HPC machines. Therefore, it is a good practice to load these modules right away after the CCEnv module.\nThere is more than one StdEnv version to choose from. The example above is for the current StdEnv/2023 . Each \u0026ldquo;Standard Environment\u0026rdquo; of the ComputeCanada software stack provides an \u0026ldquo;OS Compatibility Layer\u0026rdquo; in form of gentoo or nixpkgs base OS packages, and a set version of Core GCC compilers and GCC and Intel toolchains.\nThere is more than one CPU architecture (as set by the arch modules) in the CC software stack. They differ in the CPU instruction set used by the compilers, to generate the binary code. Most of Grex would now use arch/avx512 . There are some of the GPU nodes on Grex that are of the AMD Rome and Milan architecture, and require arch/avx2 . AVX2 and AVX512 are also used on the majority of the Alliance HPC machines.\nNote that AMD Genoa CPUs on Grex provide a larger subset of the AVX512 instructions than Intel Cascade Lake CPUs. Using -xHost or -march=native may generate a different code when ran on AMD or Intel compute nodes!\nSome of software items on CC software stack might assume certain environment variables set that are not present on Grex; one example is SLURM_TMPDIR. In case your script fails for this reason, the following line could be added to the job script:\nexport SLURM_TMPDIR=$TMPDIR While a majority of CC software stack is built using OpenMPI, some items might be based on IntelMPI. These will require following additional environment variables to be able to integrate with SLURM on Grex:\nexport I_MPI_PMI_LIBRARY=/opt/slurm/lib/libpmi.so export I_MPI_FABRICS_LIST=shm:dapl If a script or a pre-built software package assumes, or relies on, using the mpiexec.hydra launcher, the later might have to be provided with -bootstrap slurm option.\nHow to find software on CC CVMFS# Compute Canada\u0026rsquo;s software building system automatically generates documentation for each item, which is available at the Available Software page. So, the first destination to look for a software item is probably to browse this page. Note that this page covers the default CPU architectures (AVX2, AVX512) of the National systems. Legacy architectures (SSE3, AVX) that were present on earlier versions of StdEnv, are no longer supported.\nThe module spider command can be used on Grex to search for modules that are actually available. Note that the CCEnv software stack is not loaded by default; you would have to load it first to enable the spider command to search through the CC software stack. The the example below is for the Amber MM software:\nmodule purge module load CCEnv module load arch/avx512 module load StdEnv/2023 module spider amber One of the available versions of Amber as returned by the commands above, would be amber/22.5-23.5 . A subsequent command module spider amber/22.5-23.5 would then provide dependencies. Then, when finding available software versions and their dependencies, module load commands can be used, as described here or here How to request software added to CC CVMFS# The Alliance (formerly Compute Canada) maintains and distributes the software stack as part of its mandate to maintain the National HPC systems. To request a software item installed, the requestor should have an account in CCDB , which is also a prerequisite to have access to Grex. Any CCDB user can submit such a request to support@tech.alliancecan.ca .\nAn example, R code with dependencies from CC CVMFS stack# A real-world example of using R on Grex, with several dependencies required for the R packages.\nFor dynamic languages like R and Python, the Alliance (formerly known as Compute Canada) does not, in general, provide or manage pre-installed packages. Rather, users are expected to load the base R (Python, Perl, Julia) module and then proceed for the local installation of the required R (or Python, Perl, Julia etc.) packages in their home directories. Check the R documentation and Python documentation .\nScript example for running R using the Alliance\u0026#39;s software stack (CC cvmfs) run-r-cc-cvmfs.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-72:00:00 #SBATCH --job-name=\u0026#34;R-gdal-jags-bench\u0026#34; #SBATCH --partition=genoa # Load the modules: module load CCEnv module load arch/avx512 module load StdEnv/2023 module load gcc/12.3 r/4.5.0 jags/4.3.2 geos/3.12.0 gdal/3.9.1 export MKL_NUM_THREADS=1 echo \u0026#34;Starting run at: `date`\u0026#34; R --vanilla \u0026lt; Benchmark.R \u0026amp;\u0026gt; benchmark.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; Notes on MPI based software from CC Stack# We recommend using a recent environment/toolchain that provides OpenMPI 4.1.x or later, which has a recent PMIx process management interface and supports UCX interconnect libraries that are used on Grex. Earlier versions of OpenMPI might or might not work. With OpenMPI 3.1.x or 4.1.x, srun command should be used in SLURM job scripts on Grex.\nBelow is an example of an MPI job (Intel benchmark) using the StdEnv/2023 toolchain (GCC 12.3 and OpenMPI 4.1).\nScript example for running MPI program using the Alliance\u0026#39;s software stack run-mpi-cc-cvmfs.sh#!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=2 #SBATCH --mem-per-cpu=4000M #SBATCH --time=0-1:00:00 #SBATCH --job-name=\u0026#34;IMB-MPI1-4\u0026#34; # Load the modules: # first the CCEnv stack, then the dependencies, then the Intel IMB benchmark module load CCEnv module load StdEnv/2023 gcc/12.3 openmpi/4.1.5 module load imb/2021.3 module list echo \u0026#34;Starting run at: `date`\u0026#34; srun IMB-MPI1 \u0026gt; imb-ompi41-2x2.${SLURM_JOBID}.txt echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; If the script above is saved into imb.slurm, it can be submitted as follows:\nsbatch imb.slurm Notes on Restricted/Commercial software on CC Stack# The Alliance (formerly Compute Canada) software stack has two options for distribution: open source software stack to all non-CC systems, or the full software stack to systems that obey CCDB groups and ACL permissions that control access to licensed, commercial software. Grex is presntly a CCDB-based system and has full access to the CC software stack.\nHowever, each item of the proprietary code on the CC software stack comes with its own license and/or its own access conditions that we abide by. Thus, to request access to each item of commercial software the procedure must be found on the Alliance documentation site, and followed up via support@tech.alliancecan.ca .\nMany commercial items there also are BYOL (bring-your-own license). An example would be Matlab, where our users would want to provide UManitoba\u0026rsquo;s Matlab license even when using the code from CC CVMFS.\nAs of now, older Intel compiler modules on the CC CVMFS software stack do not match license available on Grex. Thus, while all GCC compilers and GCC-based toolchains from CC Stack are useful for the local code development on Grex, for Intel it might depend on a version. Newest Intel OneAPI compilers (past 2023.x) are free to use and will work without a license check-out.\nOther software repositories available through CC CVMFS# OpenScienceGrid repository for Singularity/Apptainer OSG software# On Grex, we mount OSG repositories, mainly for Singularity/Apptainer containers provided through OSG . Pointing the singularity to the desired path under /cvmfs/singularity.opensciencegrid.org/ will automatically mount and fetch the required software items.\nDiscovering what software is where on the OSG stack, seems to be up to the users. One of the ways would be simply exploring the directories under the path /cvmfs/singularity.opensciencegrid.org/ using ls and cd commands.\nSee more about using Singularity in our Containers documentation page.\nE4S containers in the OSG repository of Singularity/Apptainer software# In particular, the path /cvmfs/singularity.opensciencegrid.org/ecpe4s provides access to the containerized E4S software stack for HPC and AI applications .\nC3G repository for GenPipes/MUGQIC genomes and modules# On Grex, GenPipes/MUGQIC repositories should be also available through CC CVMFS. Please refer to the GenPipes/MUGQIC Documentation provided by C3G on how to use them.\nAlphaFold data repository from ComputeCanada CVMFS# On Grex, several Genomics data repositories are available thanks to the effort of the Alliance\u0026rsquo;s Biomolecular National Teams. One of them is Alpha Fold. As of the time of writing this page, the current version of it can be seen as follows:\nls /cvmfs/bio.data.computecanada.ca/content/databases/Core/alphafold2_dbs/2024_01/ Thus, Alphafold can be used on Grex using CC software stack as described here .\nA few other databases seems to be also available under:\n/cvmfs/bio.data.computecanada.ca/content/databases/Core/ "},{"id":"35","rootTitleIndex":"10","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-cloud fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/servers/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal / Servers","title":"Servers","content":"OpenOnDemand Servers# Using OOD, users can use RStudio, Jupyter, Code server \u0026hellip; etc.\nAll these applications work in the same way as for Desktops in the previous section. In other terms, before launching these application, a user has to set some parameters or resources to use (like the accounting group, slurm partition, number of cpus, \u0026hellip;etc.) before launching the application. Once the job is granted, click on the link with name _Connect to \u0026hellip; to start the server. At the time of writing this documentation, Code server, Matlab Server, RStudio Server and JupeyterLab server are available.\nCode server# For development purposes, users can run code server using OOD. It runs as a job on compute node. After setting all the resources, use the link Connect to Code Server to connect the server:\nCode Server: Connect Here is an example of snapshot that shows the xode server interface:\nCode Server: View Matlab Server# With OOD, one can run MATLAB GUI to debug the code or for visualization. In the form, there is a possibility to choose the version of MATLAB to use:\nMatlab Server: Connect Once the job stats, click on the button Connect to MATLAB Server to access the interface:\nMatlab Server: View RStudio Server# This application can be used to run R programs or for development and visualization. After initiating RStudio server, one can choose the version of R and some additional modules from the submission form. These modules are required to install R packages.\nRStudio Server: select modules Once all mandatory fields in the form are set, launch the application and connect to RStudio server:\nRStudio Server: Connect Here is a snapshot of RStudio Server:\nRStudio Server: View JupeyterLab server# JupyterLab Server: Connect JupyterLab Server: View "},{"id":"36","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/workshops-2021/","permalinkTitle":"Homepage / Workshops and Training Material / Workshops - 2021","title":"Workshops - 2021","content":"Autumn workshop, November 2021# Below are the slides from the Grex workshop that was held on November 01-02, 2021:\nIntroduction to HPC: Basics: Steps from getting an account to submitting jobs: Slides Linux (Unix) shell basics: Slides Software on HPC clusters: Slides Scheduling jobs on HPC clusters: How to optimize your jobs and get more from the resources available?: Slides Spring workshop, April 2021# Below are the slides from the Grex workshop that was held on April 21-22, 2021:\nUpdates about local HPC resources\u0026quot;: Slides Introduction to HPC and available resources: UofM and Compute Canada: Slides Introduction to HPC: SLURM, best practices for Grex: Slides Introduction to Software on HPC clusters: Slides "},{"id":"37","rootTitleIndex":"10","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-cloud fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/job-composer/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal / Job Composer","title":"Job Composer","content":"Job Composer# The Job Composer is an interface to create and submit jobs via OOD. It is accessible from the menu Jobs ==\u0026gt; Grex JobComposer.\nOnce launched, it shows the applications that are already configured:\nGrex JobComposer At the time of writing this page, there are two categories:\nGeneric applications to create slurm scripts.\nSpecific job composer for some applications like Gaussian, GROMACS, ORCA, MATLAB and R.\nMore application could be configured and added to the JobComposer interface.\nThe following will explain the steps to use the generic template called SLURM. This template can be adapted for any application:\nFirst, Open the interface from the JobComposer by clicking on the icon with the title SLURM.\nHere is a snapshot of the intercace:\nGrex JobComposer This interface allows to set the parameters required for job script:\nScript location: using the button Select Path, one can navigate through the directories {home or project} to specify the location where the job script will be saved. By default, it saves the script under home directory.\nScript name: it is used to give a name for your script, for example run-slurm-job.sh_. By default, the name is job.sh\nJob name: this is optional but you can use it to give a name to your job. The job name will only appear in the script once the job is submitted.\nAdd modules: this field is used to add the modules needed to run your job, for example:\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 lammps/2024-08-29p1 Project/AccountingGroup: for users who have one slurm accounting group, this field is populated automaticaly. It works exactly like sbatch or salloc where the option \u0026ndash;account=def-professor is set to the default slurm accounting group. However, if you have access to more than one accounting group, you should speficify which one to use, like \u0026ndash;account=def-professor1 or \u0026ndash;account=def-professor2__ Grex JobComposer Parameters SLURM partition: pick the appropriate partition where to run your job. If your application does not support GPU, please select a CPU partition. The drop down menu shows all the partitions (CPU and GPU) and their caharacteristis (name of the partition, total number of nodes, number of CPUs per node, total memory per node, base memory per core). It may help to onspect first the partitions status to see where the resources are available to minimize the waiting time on the queue.\nNodes and CPUs:\nNumber of nodes, Minimum number of nodes, Maximum number of nodes Number of tasks per node Total Number of tasks Number of CPUs per task Memory request type: this field is used to set the memory for the job either total memory per node or memory per core. It set one of the options --mem=Y or --mem=per-cpu=Z.\nSoftware stack: it is used to choose the software stack to load, either the local SBEnv or the CCEnv by adding module load SBEnv or module load CCEnv. Note that you have to add manually the rest of the modules.\nModules: use this field to add the modules required to run your application.\nWall time: set the appropriate time for your job.\nE-mail options: if you need e-mail notifications, you could set what notifications to receive (All, Beginning of job execution, End of job execution, Fail of the job, When the job is requeued). It is mandatory to add a valid email.\nOnce all the fields are set, you have two options:\nsubmit the job using the button Submit at the buttom of the JobComposer interface. save the job using the button Save at the buttom of the JobComposer interface. This can be handy for adapting the script manually where needed. Note: this template is more generic and one should know exactly what otions to use when filling the form, like the list of modules, \u0026hellip; etc. In addtion to the generic template, other templates for very specific applications are available from the job composer, like for R, Gaussian, ORCA, GROMACS, \u0026hellip; etc. These templates have some restrictions and additional features to adapt the script for a particular program. For example, it uses by default \u0026ndash;cpus-per-task for any threaded applications and sets \u0026ndash;ntasks=1. Here are some features of the specific templates:\nset the appropriate directives to select the resources (nodes, cores, \u0026hellip; etc.) Where available, modules are predefined, like for ORCA, Gaussian, \u0026hellip; allows to set the path to an input file, like for R Where the possible, the command line is set, like for ORCA, Gaussian, MATLAB and R. Similar to the generic templates, they also allow to save the scrip or submit the job from the JobComposer interface. "},{"id":"38","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/software-list/","permalinkTitle":"Homepage / Software and Applications / List of Software in SBEnv environment","title":"List of Software in SBEnv environment","content":"Software available on the CCEnv software stack on Grex.# Grex receives the ComputeCanada/Alliance software environment via CVMFS. Full autogenerated list of software is available at the Available Software page on the Allince Documentation Wiki.\nNote that Grex does not install local, commercial software licensed only to the National/Alliance systems. The software listed under \u0026ldquo;List of globally-installed modules\u0026rdquo; is available.\nSoftware available on the SBEnv software stack on Grex.# Below is the auto-generated list of software modules available on Grex\u0026rsquo;s local SBEnv software stack.\nNote that many items on this list, most notably for commercial software, are available only to the licensed software owners/research groups of that software! Please use module spider , module help and module load commands to check if a particular software module can be loaded by your group.\nThe list is updated periodically. The most authoritative method to check for available software is still module spider as described on the Using Modules page.\nSoftware Versions Description/Homepage Module dependencies CCEnv Expand CCEnv description Description: Compute Canada software modules. Must be loaded to see Compute Canada modules in \u0026lsquo;module spider\u0026rsquo; https://docs.computecanada.ca/wiki/Available_software SBEnv Expand SBEnv description Alma8 software environment for Grex usin Niagara SB adf 2019.305-impi, 2021.106-impi, 2021.107-impi, 2023.104-impi, 2024.105-impi, 2024.105-impi-aocl, 2025.104-impi-aocl Expand adf description AMS, Amsterdam Density Functional suite of codes Georg Schrecnekbach group at UManitoba, IntelMPI version admixture 1.3.0 Expand admixture description ADMIXTURE: Fast ancestry estimation amber 24 Expand amber description Description: Amber (originally Assisted Model Building with Energy Refinement) is software for performing molecular dynamics and structure prediction https://ambermd.org/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6) ansys 2023R2, 21.1 Expand ansys description ANSYS suite of engineering simulation software ant 1.10.15 Expand ant description Description: Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications https://ant.apache.org/ aocc 4.2.0 Expand aocc description Description: The AOCC compiler system is a high performance, production quality code generation tool https://developer.amd.com/amd-aocc/ (arch/avx2), (arch/avx512) aocl 4.2.0, 4.2.0-64 Expand aocl description Description: AMD Optimizing CPU Libraries (AOCL) are a set of numerical libraries optimized for AMD EPYC processor family https://developer.amd.com/amd-aocl/ (arch/avx2 gcc/13.2.0), (arch/avx512 aocc/4.2.0), (arch/avx512 gcc/13.2.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) arch avx2, avx512 Expand arch description Description: a module to pick the CPU architecture (cuda/11.8.0), (cuda/12.2.2), (cuda/12.4.1) armadillo 11.4.3, 14.2.2 Expand armadillo description ; Description: Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions http://arma.sourceforge.net (arch/avx512 gcc/13.2.0) arpack-ng 3.9.1+mkl-2019.5, 3.9.1+mkl-2024.1 Expand arpack-ng description Description: ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems https://github.com/opencollab/arpack-ng (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 gcc/13.2.0) arrow 18.1.0 Expand arrow description Description: Apache Arrow is a cross-language development platform for in-memory data https://arrow.apache.org (arch/avx512 gcc/13.2.0) asynch 1.4.3, git-a5d1d77 Expand asynch description Asynch is numerical library for solving differential equations with a tree structure (arch/avx512 gcc/13.2.0 openmpi/5.0.6) autodock 4.2.6 Expand autodock description Description: AutoDock is a suite of automated docking tools. It is designed to predict how small molecules, such as substrates or drug candidates, bind to a receptor of known 3D structure http://autodock.scripps.edu/ (arch/avx512 gcc/13.2.0), (arch/avx512 intel/2023.2) autodock-vina 1.2.7 Expand autodock-vina description Description: AutoDock Vina is an open-source program for doing molecular docking http://vina.scripps.edu/index.html (arch/avx512 gcc/13.2.0) autotools 2022a Expand autotools description Description: GNU autoconf, automake, gettext, and libtool https://www.gnu.org/software/{autoconf,automake,gettext,libtool}/ (arch/avx2), (arch/avx512) bamtools 2.5.2 Expand bamtools description Description: BamTools provides both a programmer\u0026rsquo;s API and an end-user\u0026rsquo;s toolkit for handling BAM files https://github.com/pezmaster31/bamtools beagle 5.4-20241029 Expand beagle description Description: Beagle is a software package for phasing genotypes and for imputing ungenotyped markers https://faculty.washington.edu/browning/beagle/beagle.html bedtools 2.31.1 Expand bedtools description Description: a swiss-army knife of tools for a wide-range of genomics analysis tasks https://github.com/arq5x/bedtools2 (arch/avx512) binutils 2.42 Expand binutils description (arch/avx512) birch 3.90, 4.0 Expand birch description BIRCH is a powerful framework for organizing and using bioinformatics. ; BIRCH comes with hundreds of commonly-used tools, and allows you to customize your BIRCH site by adding new ones https://home.cc.umanitoba.ca/~psgendb/index.html blastplus 2.16.0, 2.17.0 Expand blastplus description Description: Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences https://blast.ncbi.nlm.nih.gov/ (arch/avx512 gcc/13.2.0) blat 3.7 Expand blat description Description: BLAT on DNA is designed to quickly find sequences of 95% and greater similarity of length 25 bases or more https://genome.ucsc.edu/goldenPath/help/blatSpec.html (arch/avx512 gcc/13.2.0) blis 0.9.0 Expand blis description Description: BLIS is a portable software framework for instantiating high-performance BLAS-like dense linear algebra libraries https://github.com/flame/blis (arch/avx2 gcc/13.2.0), (arch/avx512 gcc/13.2.0) boost 1.78.0, 1.85.0 Expand boost description Description: Boost provides free peer-reviewed portable C++ source libraries, emphasizing libraries that work well with the C++ Standard Library http://www.boost.org (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel/2023.2), (cuda/12.4.1 arch/avx2 gcc/13.2.0) bowtie2 2.5.4 Expand bowtie2 description Description: Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences http://bowtie-bio.sourceforge.net/bowtie2/ (arch/avx512) buildah 1.41.0 Expand buildah description Buildah is a tool that facilitates building Open Container Initiative (OCI) container images https://buildah.io/ busco 5.8.3 Expand busco description Description: BUSCO provides Assessment of assembly completeness using Universal Single Copy Orthologs https://busco.ezlab.org bwa 0.7.18 Expand bwa description Description: BWA - Burrows-Wheeler Aligner is an efficient program that aligns relatively short nucleotide sequences http://bio-bwa.sourceforge.net/ (arch/avx512 gcc/13.2.0) cdo 2.5.0 Expand cdo description Description: CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data https://code.mpimet.mpg.de/projects/cdo (arch/avx512 gcc/13.2.0 openmpi/4.1.6) cfitsio 4.4.1, 4.5.0 Expand cfitsio description Description: CFITSIO is a library of C and Fortran subroutines for reading and writing data files in FITS (Flexible Image Transport System) data format http://heasarc.gsfc.nasa.gov/fitsio/ cgal 5.5, 6.0.1 Expand cgal description Description: The Computational Geometry Algorithms Library (CGAL) is a C++ library ; that aims to provide easy access to efficient and reliable algorithms in ; computational geometry https://www.cgal.org/ (arch/avx512 gcc/13.2.0) circos 0.69-9 Expand circos description Description: Circos is a software package for visualizing data and information. It visualizes data in a circular layout - this makes Circos ideal for exploring relationships between objects or positions http://www.circos.ca/ (arch/avx512) cistem 1.0.0 Expand cistem description Description: cisTEM is user-friendly software to process cryo-EM images of macromolecular complexes and obtain high-resolution 3D reconstructions from them https://cistem.org/ (arch/avx512 gcc/13.2.0) clhep 2.4.7.1 Expand clhep description Description: The CLHEP project is intended to be a set of HEP-specific foundation and utility classes such as random generators, physics vectors, geometry and linear algebra. CLHEP is structured in a set of packages independent of any external package https://proj-clhep.web.cern.ch/proj-clhep/ (arch/avx512 gcc/13.2.0) cmake 3.28.4, 3.31.1, 4.1.1 Expand cmake description Description: A cross-platform, open-source build system https://www.cmake.org code-server 4.103.1 Expand code-server description Run VS Code on any machine anywhere and access it in the browser compleasm 0.2.6 Expand compleasm description Description: Compleasm: a faster and more accurate reimplementation of BUSCO, a genomics tool https://github.com/huangnengCSU/compleasm cp2k 2024.3 Expand cp2k description Description: CP2K is a freely available (GPL) program, written in Fortran 95, to perform atomistic and molecular simulations of solid state, liquid, molecular and biological systems. It provides a general framework for different methods such as e.g. density functional theory (DFT) using a mixed Gaussian and plane waves approach (GPW), and classical pair and many-body potentials https://www.cp2k.org/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6) cppzmq 4.10.0 Expand cppzmq description Description: cppzmq is a C++ binding for libzmq https://github.com/zeromq/cppzmq crest 2.12 Expand crest description Description: CREST is an utility/driver program for the xtb program. Originally it was designed as conformer sampling program, hence the abbreviation Conformer-Rotamer Ensemble Sampling Tool, but now offers also some utility functions for calculations with the GFNn-xTB methods. Generally the program functions as an IO based OMP scheduler (i.e., calculations are performed by the xtb program) and tool for the creation and analysation of structure ensembles https://xtb-docs.readthedocs.io/en/latest/crest.html (arch/avx512 intel/2023.2) ctffind 4.1.14 Expand ctffind description CTFFIND4 is a fast and accurate defocus estimation from electron micrographs https://grigoriefflab.umassmed.edu/ctffind4 (arch/avx512 intel-one/2024.1), (cuda/12.4.1 arch/avx2 gcc/13.2.0) cuda 11.8.0, 12.2.2, 12.4.1 Expand cuda description Description: CUDA is a parallel computing platform and programming model invented by NVIDIA https://developer.nvidia.com/cuda-zone (arch/avx2), (arch/avx512) cudnn 8.8.1.3+cuda-11.8.0 Expand cudnn description Description: NVIDIA cuDNN is a GPU-accelerated library of primitives for deep neural networks https://developer.nvidia.com/cudnn deepvariant 1.8.0, 1.8.0-gpu Expand deepvariant description DeepVariant is a deep learning-based variant caller that takes aligned reads (in BAM or CRAM format), produces pileup image tensors from them, classifies each tensor using a convolutional neural network, and finally reports the results in a standard VCF or gVCF file dejagnu 1.6.3 Expand dejagnu description Description: DejaGnu is a framework for testing other programs https://www.gnu.org/software/dejagnu/ diamond 2.1.10 Expand diamond description Description: Accelerated BLAST compatible local sequence aligner https://github.com/bbuchfink/diamond dlb 3.5.0 Expand dlb description Description: DLB is a dynamic library designed to speed up HPC hybrid applications by improving the load balance of the outer level of parallelism by dynamically redistributing the computational resources at the inner level of parallelism at run time https://pm.bsc.es/dlb/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) eccodes 2.31.0, 2.40.0 Expand eccodes description Description: ecCodes is a package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats: WMO FM-92 GRIB edition 1 and edition 2, WMO FM-94 BUFR edition 3 and edition 4, WMO GTS abbreviated header (only decoding) https://software.ecmwf.int/wiki/display/ECC/ecCodes+Home (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0) eigen 3.4.0 Expand eigen description ; Description: Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms https://eigen.tuxfamily.org/index.php?title=Main_Page (arch/avx512), (cuda/12.4.1 arch/avx2 gcc/13.2.0) esmf 8.7.0 Expand esmf description Description: The Earth System Modeling Framework (ESMF) is software for building and coupling weather, climate, and related models http://earthsystemmodeling.org (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) espresso 7.3.1, 7.3.1+aocl-4.2.0, 7.4.1, 7.4.1+aocl-4.2.0, 7.5, 7.5+aocl-4.2.0 Expand espresso description Description: Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft), Description: Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. ; Description: Quantum ESPRESSO is an integrated suite of computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials (both norm-conserving and ultrasoft https://www.quantum-espresso.org, (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) expect 5.45.4 Expand expect description Description: Expect is a tool for automating interactive applications such as telnet, ftp, passwd, fsck, rlogin, tip, etc http://expect.sourceforge.net/ fastqc 0.12.1 Expand fastqc description Description: FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and can either provide an interactive application to review the results of several different QC checks, or create an HTML based report which can be integrated into a pipeline https://www.bioinformatics.babraham.ac.uk/projects/fastqc/ fasttree 2.1.11 Expand fasttree description Description: FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences http://www.microbesonline.org/fasttree/ (arch/avx512 gcc/13.2.0), (arch/avx512 intel-one/2024.1) feko 2021.2 Expand feko description Altair Feko high-frequency electromagnetic simulation software for Okhmatovsky group ffmpeg 7.0.2 Expand ffmpeg description A statically build binary for ffmpeg Linux https://johnvansickle.com/ffmpeg/ fftw 3.3.10 Expand fftw description Description: FFTW is a C subroutine library for computing the discrete Fourier transform (DFT) http://www.fftw.org (arch/avx512 aocc/4.2.0 openmpi/4.1.6), (arch/avx512 aocc/4.2.0), (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/14.3.0), (arch/avx512 gcc/9.5.0 openmpi/4.1.6), (arch/avx512 intel-one/2024.1 openmpi/4.1.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2 openmpi/4.1.6), (arch/avx512 intel/2023.2), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/5.0.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0) flex 2.6.4 Expand flex description Description: Flex is a tool for generating scanners https://github.com/westes/flex flexpart 11 Expand flexpart description Description: FLEXPART (FLEXible PARTicle dispersion model) is a Lagrangian transport and dispersion model suitable for the simulation of a large range of atmospheric transport processes https://www.flexpart.eu (arch/avx512 gcc/13.2.0) freeglut 3.4.0 Expand freeglut description Description: freeglut is a free-software/open-source alternative to the OpenGL Utility Toolkit (GLUT) library https://freeglut.sourceforge.net/ gate 9.4 Expand gate description Description: GATE is an advanced opensource software developed by the international OpenGATE collaboration and dedicated to the numerical simulations in medical imaging. It currently supports simulations of Emission Tomography (Positron Emission Tomography - PET and Single Photon Emission Computed Tomography - SPECT), and Computed Tomography http://www.opengatecollaboration.org/ (arch/avx512 gcc/13.2.0) gatk 4.6.1.0 Expand gatk description Description: The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyse next-generation resequencing data. The toolkit offers a wide variety of tools, with a primary focus on variant discovery and genotyping as well as strong emphasis on data quality assurance. Its robust architecture, powerful processing engine and high-performance computing features make it capable of taking on projects of any size http://www.broadinstitute.org/gatk/ (arch/avx512 gcc/13.2.0) gaussian g16.b01, g16.c01 Expand gaussian description Gaussian 16.c01 - A quantum chemistry package gcc 11.5.0, 12.3.0, 13.2.0, 14.3.0, 8.5.0, 9.5.0 Expand gcc description Description: The GNU Compiler Collection for C, C++, and Fortran https://gcc.gnu.org (arch/avx2), (arch/avx512), (cuda/11.8.0 arch/avx2), (cuda/12.2.2 arch/avx2), (cuda/12.4.1 arch/avx2) gdal 3.10.0, 3.9.1 Expand gdal description Description: GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing https://www.gdal.org/ (arch/avx512 gcc/13.2.0) geant4 11.2.2, 11.3.0 Expand geant4 description Description: Geant4 is a toolkit for the simulation of the passage of particles through matter. Its areas of application include high energy, nuclear and accelerator physics, as well as studies in medical and space science http://geant4.cern.ch/ (arch/avx512 gcc/13.2.0) geos 3.13.0 Expand geos description Description: GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS) https://trac.osgeo.org/geos (arch/avx512 gcc/13.2.0) gibbs2 1.0 Expand gibbs2 description Description: Gibbs2 is a program for the calculation of thermodynamic properties in periodic solids under arbitrary conditions of temperature and pressure https://aoterodelaroza.github.io/gibbs2 (arch/avx512 gcc/13.2.0) git 2.49.0, 2.51.0 Expand git description Description: Git is a free and open source distributed version control system http://git-scm.com git-annex 10.20250929 Expand git-annex description Git-annex allows managing large files with git, without storing the file contents in git https://git-annex.branchable.com/ git-lfs 3.7.0 Expand git-lfs description Git Large File Storage (git-LFS) is a command line extension and specification for managing large files with Git https://git-lfs.com/ globus 3.36.0, 3.38.0 Expand globus description Tools to interact with Globus using the command line interface: ; - Globus CLI (https://github.com/globus/globus-cli ) ; - Globus Connect Personal (https://www.globus.org/globus-connect-personal ) https://www.globus.org/ glost 0.3.1 Expand glost description Description: This is GLOST, the Greedy Launcher Of Small Tasks https://github.com/cea-hpc/glost (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) glpk 5.0 Expand glpk description Description: The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library https://www.gnu.org/software/glpk/ (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel/2023.2) gmp 6.2.1, 6.3.0 Expand gmp description Description: GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers https://gmplib.org/ (arch/avx512 gcc/13.2.0), (arch/avx512), (cuda/12.4.1 arch/avx2 gcc/13.2.0) gnina 1.1, 2024 Expand gnina description gnina (pronounced NEE-na) is a molecular docking program with integrated support for scoring and optimizing ligands using convolutional neural networks https://github.com/gnina/gnina gnuplot 5.4.2, 6.0.2 Expand gnuplot description Description: Gnuplot is a portable command-line driven graphing utility for Linux, OS/2, MS Windows, OSX, VMS, and many other platforms http://www.gnuplot.info golang 1.24.4, 1.25.2 Expand golang description Go is an opensource programming language supported by Google grace 5.99.0 Expand grace description Description: Grace is a WYSIWYG 2D plotting tool for X Windows System and Motif http://freecode.com/projects/grace (arch/avx512 gcc/13.2.0) gromacs 2021.6, 2022, 2023.3, 2024.1, 2025.2, 2025.3 Expand gromacs description Description: GROMACS (GROningen MAchine for Chemical Simulations) is a molecular dynamics package primarily designed for simulations of proteins, lipids and nucleic acids http://www.gromacs.org (arch/avx512 aocc/4.2.0 openmpi/4.1.6), (arch/avx512 gcc/11.5.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0) gsl 2.7 Expand gsl description Description: The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers http://www.gnu.org/software/gsl (arch/avx2 gcc/13.2.0), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2), (cuda/12.4.1 arch/avx2 gcc/13.2.0) haploview 4.2 Expand haploview description Description: Haploview is designed to simplify and expedite the process of haplotype analysis by providing a common interface to several tasks relating to such analyses https://www.broadinstitute.org/haploview/haploview hdf5 1.10.9, 1.12.3, 1.14.2, 1.14.6 Expand hdf5 description Description: HDF5 is a data model, library, and file format for storing and managing data https://portal.hdfgroup.org (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/14.3.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel-one/2024.1 openmpi/4.1.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2 intelmpi/2021.10), (arch/avx512 intel/2023.2 openmpi/4.1.6), (arch/avx512 intel/2023.2), (cuda/12.4.1 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/5.0.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0) highfive 2.10.1 Expand highfive description Description: HighFive is a modern header-only C++11 friendly interface for libhdf5 https://github.com/BlueBrain/HighFive (arch/avx512 gcc/13.2.0 openmpi/4.1.6) hisat2 2.2.1 Expand hisat2 description Description: HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) against the general human population (as well as against a single reference genome) https://daehwankimlab.github.io/hisat2/ (arch/avx512 gcc/13.2.0) hmmer 3.4 Expand hmmer description Description: HMMER is used for searching sequence databases for homologs of protein sequences, and for making protein sequence alignments. It implements methods using probabilistic models called profile hidden Markov models (profile HMMs) http://hmmer.org/ homer 5.1 Expand homer description Description: HOMER (Hypergeometric Optimization of Motif EnRichment) is a suite of tools for Motif Discovery and ; next-gen sequencing analysis. It is a collection of command line programs for unix-style operating systems written ; in Perl and C++. HOMER was primarily written as a de novo motif discovery algorithm and is well suited for finding ; 8-20 bp motifs in large scale genomics data. HOMER contains many useful tools for analyzing ChIP-Seq, GRO-Seq, ; RNA-Seq, DNase-Seq, Hi-C and numerous other types of functional genomics sequencing data sets http://homer.ucsd.edu/homer/download.html (arch/avx512 gcc/13.2.0) htgettoken 2.0-2 Expand htgettoken description htgettoken gets OIDC bearer tokens by interacting with Hashicorp vault https://github.com/fermitools/htgettoken imb 2021.7 Expand imb description ; IMB is Intel suite for MPI benchmarks. ; This module only includes classic C benchmarks that can be built by most of compilers. ; ; ; IMB is Intel suite for MPI benchmarks. ; This module only includes classic C benchmarks that can be built by most of compilers https://github.com/intel/mpi-benchmarks, (arch/avx2 intel/2023.2 intelmpi/2021.10), (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel/2023.2 intelmpi/2021.10) intel 2019.5, 2023.2 Expand intel description ; Description: Intel compilers for C, C++, and Fortran ; ; Description: Intel compilers for C, C++, and Fortran https://software.intel.com/content/www/us/en/develop/tools/oneapi.html (arch/avx2), (arch/avx512) intel-one 2023.2, 2024.1 Expand intel-one description ; ; Description: Intel compilers for C, C++, and Fortran from New OneAPI suite ; ;, ; Description: Intel compilers for C, C++, and Fortran from New OneAPI suite https://software.intel.com/content/www/us/en/develop/tools/oneapi.html (arch/avx512) intelmpi 2019.8, 2021.10 Expand intelmpi description Description: Intel MPI library with compiler wrappers for C, C++, and Fortran https://software.intel.com/en-us/intel-mpi-library (arch/avx2 gcc/13.2.0), (arch/avx2 intel/2023.2), (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2) jags 4.3.2+mkl-2019.5, 4.3.2+mkl-2024.1 Expand jags description Description: JAGS is Just Another Gibbs Sampler. It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation http://mcmc-jags.sourceforge.net/ (arch/avx512 gcc/13.2.0) jasper 4.0.0 Expand jasper description Description: The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard https://www.ece.uvic.ca/~frodo/jasper/ (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0) jellyfish 2.3.1 Expand jellyfish description Description: Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA http://www.genome.umd.edu/jellyfish.html jemalloc 5.3.0 Expand jemalloc description Description: jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support http://jemalloc.net jq 1.7 Expand jq description Description: jq is a lightweight and flexible command-line JSON processor https://stedolan.github.io/jq/ julia 1.10.3, 1.11.3 Expand julia description Description: The Julia Language: A fresh approach to technical computing http://julialang.org kim 2.3.0 Expand kim description Open Knowledgebase of Interatomic Models (KIM) (arch/avx2 gcc/13.2.0), (arch/avx512 gcc/13.2.0), (arch/avx512 intel-one/2024.1), (cuda/12.4.1 arch/avx2 gcc/13.2.0) kitops 1.6.0 Expand kitops description Packaging for AI/ML projects kokkos 4.4.01 Expand kokkos description KOKKOS is a portable, hardware agnostic layer for writing C++ HPC applications (cuda/12.4.1 arch/avx2 gcc/13.2.0) lammps 2021-09-29, 2024-08-29p1, 2024-08-29p1-nep Expand lammps description Description: LAMMPS stands for Large-scale Atomic/Molecular Massively Parallel Simulator http://www.lammps.org/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel-one/2024.1 openmpi/4.1.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6) libaec 1.0.6 Expand libaec description Description: Libaec provides fast lossless compression of 1 up to 32 bit wide signed or unsigned integers (samples). The library achieves best results for low entropy data as often encountered in space imaging instrument data or numerical model output from weather or climate simulations. While floating point representations are not directly supported, they can also be efficiently coded by grouping exponents and mantissa https://gitlab.dkrz.de/k202009/libaec libint-cp2k 2.6 Expand libint-cp2k description Description: Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory https://github.com/evaleev/libint (arch/avx512 gcc/13.2.0) libvori 220621 Expand libvori description libvori is a Library for Voronoi Integration written in C++ libxc 5.1.5, 6.2.2 Expand libxc description Description: Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals https://www.tddft.org/programs/octopus/wiki/index.php/Libxc (arch/avx512 aocc/4.2.0), (arch/avx512 gcc/13.2.0), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2) libxml2 2.11.9 Expand libxml2 description Description: libxml2 is an XML toolkit implemented in C, originally developed for the GNOME Project https://gitlab.gnome.org/GNOME/libxml2 mathematica 14.2 Expand mathematica description Wolfram Mathematica symbolic algebra package (U. of Manitoba users ONLY) https://www.wolfram.com/mathematica/ matlab R2020B2, R2022A, R2023B, R2024A Expand matlab description MATLAB, a computing environment from Mathworks matlab-proxy 0.26.0, 0.27.1 Expand matlab-proxy description Description: matlab-proxy is a Python® package which enables you to launch MATLAB® and access it from a web browser https://github.com/mathworks/matlab-proxy mcr R2020b, R2022a, R2023b, R2024a Expand mcr description Description: Run MATLAB applications on computers that do not have MATLAB installed https://www.mathworks.com/help/compiler/matlab-runtime.html megahit 1.2.9 Expand megahit description Description: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph https://github.com/voutcn/megahit metashape-pro 2.1.4, 2.2.1, 2.2.2 Expand metashape-pro description Software to perform photogrammetric processing of digital images, generating 3D spatial data, Agisoft Metashape is a stand-alone software product that performs photogrammetric processing of digital images ; and generates 3D spatial data to be used in GIS applications, cultural heritage documentation, ; and visual effects production as well as for indirect measurements of objects of various scales https://www.agisoft.com/ metis 5.1.0 Expand metis description Description: METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices http://glaros.dtc.umn.edu/gkhome/metis/metis/overview (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel/2023.2) metis32 5.1.0 Expand metis32 description Description: METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices http://glaros.dtc.umn.edu/gkhome/metis/metis/overview (arch/avx512 gcc/13.2.0) micro 2.0.14 Expand micro description micro is a simple, modern and intuitive text editor mii 1.1.1 Expand mii description Description: A smart search engine for module environments https://github.com/codeandkey/mii minimap2 2.28 Expand minimap2 description Minimap2 is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database mkl 2019.5, 2024.1 Expand mkl description Description: Intel Math Kernel Library https://software.intel.com/en-us/mkl (arch/avx2 gcc/13.2.0), (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0), (cuda/11.8.0 arch/avx2 gcc/11.5.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) molden 7.3 Expand molden description Description: Molden is a package for displaying Molecular Density from the Ab Initio packages GAMESS-UK, GAMESS-US and GAUSSIAN and the Semi-Empirical packages Mopac/Ampac http://www.cmbi.ru.nl/molden/ mpfr 4.2.1 Expand mpfr description Description: The MPFR library is a C library for multiple-precision ; floating-point computations with correct rounding https://www.mpfr.org/ (arch/avx512 gcc/13.2.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) mumax3 3.10 Expand mumax3 description Mumax3 GPU-accelerated micromagnetic simulation program for CUDA mummer 4.0.0rc1 Expand mummer description Description: MUMmer is a system for rapidly aligning entire genomes, whether in complete or draft form. AMOS makes use of it http://mummer.sourceforge.net/ mustang 3.2.4 Expand mustang description Description: MUSTANG (MUltiple STructural AligNment AlGorithm), for the alignment of multiple protein structures http://www.csse.monash.edu.au/~karun/Site/mustang.html (arch/avx512 gcc/13.2.0) nbo nbo7-2021 Expand nbo description The Natural Bond Orbital (NBO) program NBO 7 nco 5.3.1 Expand nco description Description: The NCO toolkit manipulates and analyzes data stored in netCDF-accessible formats, including DAP, HDF4, HDF5, and, most recently, Zarr https://nco.sourceforge.net (arch/avx512 gcc/13.2.0) ncview 2.1.11 Expand ncview description Description: Ncview is a visual browser for netCDF format files. Typically you would use ncview to get a quick and easy, push-button ; look at your netCDF files. You can view simple movies of the data, view along various dimensions, take a look at the actual data values, change color maps, invert the data, etc http://meteora.ucsd.edu/~pierce/ncview_home_page.html (arch/avx512 gcc/13.2.0) netcdf 4.9.2+hdf5-1.14.2 Expand netcdf description Description: NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data http://www.unidata.ucar.edu/software/netcdf (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2), (cuda/12.4.1 arch/avx2 gcc/13.2.0) nextflow 24.10.0 Expand nextflow description Nextflow is a reactive workflow framework and a programming DSL that eases writing computational pipelines with complex data https://www.nextflow.io/ ninja 1.10.2 Expand ninja description Description: Ninja is a small build system with a focus on speed https://ninja-build.org nodejs 18.20.2, 18.20.4, 18.20.5, 20.12.2, 20.15.1, 20.16.0, 20.18.1, 22.11.0, 22.4.1, 22.5.1 Expand nodejs description Node.JS - the asynchronous server-side JavaScript language. https://nodejs.org nvtop 3.1.0, 3.2.0 Expand nvtop description Description: NVTOP stands for Neat Videocard TOP, a (h)top like task monitor for GPUs and accelerators. It can handle multiple GPUs and print information about them in a htop-familiar way https://github.com/Syllo/nvtop nwchem 7.2.2, 7.2.2+aocl-4.2.0-64 Expand nwchem description Description: NWChem is an open source high performance computing package for computational chemistry (arch/avx512 aocc/4.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel-one/2024.1 openmpi/4.1.6) omlmd 0.1.6 Expand omlmd description OCI Artifact for ML model and metadata openbabel 3.1.1 Expand openbabel description Description: Open Babel is a chemical toolbox designed to speak the many languages of chemical data. It\u0026rsquo;s an open, collaborative project allowing anyone to search, convert, analyze, or store data from molecular modeling, chemistry, solid-state materials, biochemistry, or related areas https://openbabel.org/ (arch/avx512 gcc/13.2.0) openblas 0.3.26, 0.3.28 Expand openblas description Description: OpenBLAS: An optimized BLAS library https://www.openblas.net (arch/avx512 aocc/4.2.0), (arch/avx512 gcc/13.2.0), (arch/avx512 intel/2023.2), (arch/avx512), (cuda/11.8.0 arch/avx2 gcc/11.5.0), (cuda/12.4.1 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) openeye 2022.2.1 Expand openeye description OpenEye: module for R. Davis Group only openfoam 9 Expand openfoam description Description: OpenFOAM is a GPL-opensource C++ CFD-toolbox https://www.openfoam.com/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6) openjdk 11.0.22, 17.0.11_9, 17.0.12_7, 17.0.13_11, 21.0.2, 21.0.3_9, 21.0.4_7, 21.0.5_11, 21.0.6_7 Expand openjdk description Description: The free and opensource java implementation https://jdk.java.net openmm 8.2.0, 8.3.1 Expand openmm description Description: OpenMM is a toolkit for molecular simulation http://openmm.org (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/4.1.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/5.0.6) openmpi 4.1.6, 5.0.6 Expand openmpi description Description: An open source Message Passing Interface implementation http://www.open-mpi.org (arch/avx2 aocc/4.2.0), (arch/avx2 intel/2023.2), (arch/avx512 aocc/4.2.0), (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/14.3.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel-one/2024.1), (arch/avx512 intel/2023.2), (cuda/12.2.2 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) opennurbs 8.12 Expand opennurbs description Description: The openNURBS Initiative provides CAD, CAM, CAE, and computer graphics software developers the tools to accurately transfer 3D geometry between applications https://github.com/mcneel/opennurbs (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0) openssl 3.4.0 Expand openssl description Description: OpenSSL is an open source project that provides a robust, commercial-grade, and full-featured toolkit for the Transport Layer Security (TLS) and Secure Sockets Layer (SSL) protocols http://www.openssl.org openstack-client 8.2.0 Expand openstack-client description OpenStackClient is a command-line client for OpenStack orca 5.0.4, 6.0.1 Expand orca description Description ; =========== ; ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum ; chemistry with specific emphasis on spectroscopic properties of open-shell molecules. ; It features a wide variety of standard quantum chemical methods ranging from ; semiempirical methods to DFT to single- and multireference correlated ab initio methods. ; It can also treat environmental and relativistic effects http://cec.mpg.de/forum/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) osu-micro-benchmarks 5.7.1 Expand osu-micro-benchmarks description Description: The Ohio MicroBenchmark suite is a collection of independent MPI message passing performance microbenchmarks developed and written at The Ohio State University http://mvapich.cse.ohio-state.edu/benchmarks/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6) ovito 3.12.3, 3.13.1, 3.14.0 Expand ovito description OVITO (Open Visualization Tool) is a scientific data visualization and analysis software designed for molecular and particle-based simulation models., OVITO (Open Visualization Tool) is a scientific data visualization and analysis software designed for molecular and particle-based simulation models https://www.ovito.org/ pandaseq 2.11 Expand pandaseq description Description: PANDASEQ is a program to align Illumina reads and reconstruct an overlapping sequence https://github.com/neufeld/pandaseq (arch/avx512 gcc/13.2.0) pandoc 3.6.1 Expand pandoc description Description: If you need to convert files from one markup format into another, pandoc is your swiss-army knife https://pandoc.org paraview 5.13.3 Expand paraview description Description: ParaView is a scientific parallel visualizer https://www.paraview.org (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6) paraview-offscreen 5.10.1 Expand paraview-offscreen description Description: Off-screen rendering with ParaView, a scientific parallel visualizer http://www.paraview.org parmetis 4.0.3-shared Expand parmetis description Description: ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) perl 5.38.2, 5.40.1 Expand perl description Description: Perl 5 is a highly capable, feature-rich programming language with over 27 years of development http://www.perl.org (arch/avx2), (arch/avx512) petsc-complex 3.13.3+mkl-2019.5, 3.22.2+mkl-2019.5 Expand petsc-complex description Description: PETSc is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations http://www.mcs.anl.gov/petsc/index.html (arch/avx512 gcc/13.2.0 openmpi/4.1.6) petsc-real 3.13.3+mkl-2019.5, 3.22.2+mkl-2019.5 Expand petsc-real description Description: PETSc is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations http://www.mcs.anl.gov/petsc/index.html (arch/avx512 gcc/13.2.0 openmpi/4.1.6) picard 3.3.0 Expand picard description Picard: A set of tools (in Java) for working with next generation sequencing data in the BAM format https://broadinstitute.github.io/picard/ pnetcdf 1.14.0 Expand pnetcdf description Description: Parallel netCDF: A Parallel I/O Library for NetCDF File Access https://trac.mcs.anl.gov/projects/parallel-netcdf (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) podman 5.6.1, 5.6.2 Expand podman description Podman (the POD MANager) is a tool for managing containers and images, volumes mounted into those containers, and pods made from groups of containers https://podman.io/ podman-compose 1.5.0 Expand podman-compose description An implementation of Compose Spec with Podman backend https://github.com/containers/podman-compose podman-tui 1.9.0 Expand podman-tui description Podman-tui is a terminal user interface for podman environment. ; It uses podman go bindings to communicate with local or remote podman machine (through SSH) https://github.com/containers/podman-tui/ potreeconv 2.1.1 Expand potreeconv description Description: PotreeConverter generates an octree LOD structure for streaming and real-time rendering of massive point clouds https://github.com/potree/PotreeConverter (arch/avx512 gcc/13.2.0) process-compose 1.75.1 Expand process-compose description Process Compose is a simple and flexible scheduler and orchestrator to manage non-containerized applications prodigal 2.6.3 Expand prodigal description Description: Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm) is a microbial (bacterial and archaeal) gene finding program developed at Oak Ridge National Laboratory and the University of Tennessee https://github.com/hyattpd/Prodigal/ proj 9.2.0, 9.5.0 Expand proj description Description: Program proj is a standard Unix filter function which converts geographic longitude and latitude coordinates into cartesian coordinates https://proj.org (arch/avx512 gcc/13.2.0) prsice 2.3.5 Expand prsice description Description: PRSice (pronounced \u0026lsquo;precise\u0026rsquo;) is a software package for calculating, applying, evaluating and plotting the results of polygenic risk scores (PRS). PRSice can run at high-resolution to provide the best-fit PRS as well as provide results calculated at broad P-value thresholds, illustrating results corresponding to either, can thin SNPs according to linkage disequilibrium and P-value ( clumping ), and can be applied across multiple traits in a single run https://github.com/choishingwan/PRSice/tree/master (arch/avx512) python 3.10.14, 3.10.16, 3.11.11, 3.11.8, 3.12.9 Expand python description Description: The Python programming language https://www.python.org/ (arch/avx2 gcc/13.2.0), (arch/avx512 aocc/4.2.0), (arch/avx512 gcc/11.5.0), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/14.3.0), (arch/avx512 gcc/9.5.0), (arch/avx512 intel-one/2024.1), (cuda/11.8.0 arch/avx2 gcc/11.5.0), (cuda/12.2.2 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) qhull 2020.2 Expand qhull description Description: Qhull computes the convex hull, Delaunay triangulation, Voronoi diagram, halfspace intersection about a point, furthest-site Delaunay triangulation, and furthest-site Voronoi diagram http://www.qhull.org (arch/avx512 gcc/13.2.0) qiime2 2024.10 Expand qiime2 description Description: QIIME is an open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data http://qiime2.org/ qrupdate 1.1.2 Expand qrupdate description Description: Qrupdate is a Fortran library for fast updates of QR and Cholesky decompositions https://sourceforge.net/projects/qrupdate/ (arch/avx512 gcc/13.2.0) qt 6.7.1, 6.8.1, 6.8.3, 6.9.1 Expand qt description Description: Qt is a comprehensive cross-platform C++ application framework https://qt.io/ (arch/avx512 gcc/13.2.0) quast 5.3.0 Expand quast description Description: QUAST is an open-source bioinformatics Quality Assessment Tool for Genome Assemblies r 4.3.3, 4.4.1+aocl-4.2.0, 4.4.1+mkl-2019.5, 4.4.1+mkl-2024.1, 4.5.0+mkl-2024.1, 4.5.0+openblas-0.3.28 Expand r description Description: R is a language and environment for statistical computing and graphics https://www.r-project.org (arch/avx2 gcc/13.2.0), (arch/avx2), (arch/avx512 gcc/13.2.0), (arch/avx512), (cuda/12.4.1 arch/avx2 gcc/13.2.0) ramalama 0.12.0 Expand ramalama description RamaLama tool facilitates local management and serving of AI Models https://github.com/containers/ramalama ratarmount 1.2.0 Expand ratarmount description Random Access To Archived Resources (Ratarmount) ; Ratarmount collects all file positions inside a TAR so that it can easily jump to and read from any file without extracting it. ; It, then, mounts the TAR using fusepy for read access just like archivemount. ; In contrast to libarchive, on which archivemount is based, random access and true seeking is supported. ; And in contrast to tarindexer, which also collects file positions for random access, ratarmount offers easy access via FUSE and support for compressed TARs https://github.com/mxmlnkn/ratarmount raxml-ng 1.2.1 Expand raxml-ng description Description: RAxML-NG is a phylogenetic tree inference tool which uses maximum-likelihood (ML) optimality criterion https://github.com/amkozlov/raxml-ng (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6) rclone 1.70.3, 1.71.0 Expand rclone description Rclone: sync files/directories to and from a variety of online storage services. ; Rclone: sync files/directories to and from a variety of online storage services https://rclone.org/, relion 4.0.2, 5.0.0 Expand relion description Description: RELION (for REgularised LIkelihood OptimisatioN) is a stand-alone computer program for Maximum. A Posteriori refinement of (multiple) 3D reconstructions or 2D class averages in cryo-electron microscopy (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/5.0.6) root 6.32.08, 6.34.02 Expand root description Description: The ROOT system provides a set of OO frameworks with all the functionality needed to handle and analyze large amounts of data in a very efficient way https://root.cern.ch/drupal/ (arch/avx512 gcc/13.2.0) rs-server 2025.09.0-387, 2025.09.1-401 Expand rs-server description RStudio server for use with Ondemand or Jupyter rust 1.89.0, 1.90.0 Expand rust description Description: Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety https://www.rust-lang.org s3cmd 2.4.0 Expand s3cmd description Free command line tool and client for uploading, retrieving and managing data ; in Amazon S3 and other cloud storage service providers that use the S3 protocol, ; such as Google Cloud Storage https://github.com/s3tools/s3cmd samtools 1.20 Expand samtools description Description: SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging, indexing and generating alignments in a per-position format http://www.htslib.org (arch/avx512 gcc/13.2.0) scalapack 2.2.2 Expand scalapack description Description: The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers. This module uses OpenBLAS. Please use AOCL or MKL modules for more efficient implementations of ScaLAPACK! https://www.netlib.org/scalapack/ (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (cuda/12.4.1 arch/avx2 gcc/13.2.0 openmpi/5.0.6) scipy-bundle 2023+python-3.10.14, 2023+python-3.10.16, 2023+python-3.11.8 Expand scipy-bundle description A bundle of common Python packages: Numpy, Scipy, Pandas, Matplotlib etc (arch/avx512 gcc/13.2.0), (cuda/12.4.1 arch/avx2 gcc/12.3.0), (cuda/12.4.1 arch/avx2 gcc/13.2.0) scons 3.1.2 Expand scons description Description: SCons is a software construction tool http://scons.org scotch 6.0.9, 7.0.5 Expand scotch description Description: Software package and libraries for sequential and parallel graph partitioning, static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning https://www.labri.fr/perso/pelegrin/scotch/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) singularity 4.1.2, 4.2.2 Expand singularity description Singularity CE - Containers for HPC from sylabs.io skopeo 1.20.0 Expand skopeo description Skopeo is a command line utility that performs various operations on container images and image repositories https://github.com/containers/skopeo/ snp-sites 2.5.1 Expand snp-sites description Description: Extracting single nucleotide polymorphisms (SNPs) from a large whole genome alignment is now a routine task, but existing tools have failed to scale efficiently with the increased size of studies https://github.com/sanger-pathogens/snp-sites snpeff 5.2f Expand snpeff description Description: SnpEff is a variant annotation and effect prediction tool. It annotates and predicts the effects of genetic variants (such as amino acid changes) https://pcingola.github.io/SnpEff/ sparsehash 2.0.4 Expand sparsehash description Description: An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed https://github.com/sparsehash/sparsehash sqlite 3.35.5 Expand sqlite description Description: SQLite3 is an SQL database engine in a C library https://www.sqlite.org stata 15.0-fagfs, 18.0-ffin Expand stata description Description: Stata 18.0 - Statistics / Data Analysis http://www.stata.com stringtie 3.0.0 Expand stringtie description Description: StringTie is a fast and highly efficient assembler of RNA-Seq alignments into potential transcripts https://ccb.jhu.edu/software/stringtie/ (arch/avx512 gcc/13.2.0) structure 2.3.4 Expand structure description Description: The program structure is a free software package for using multi-locus genotype data to investigate population structure. Its uses include inferring the presence of distinct populations, assigning individuals to populations, studying hybrid zones, identifying migrants and admixed individuals, and estimating population allele frequencies in situations where many individuals are migrants or admixed http://web.stanford.edu/group/pritchardlab/software/ subread 2.0.8 Expand subread description Description: High performance read alignment, quantification and mutation discovery http://subread.sourceforge.net/ subversion 1.14.3 Expand subversion description Description: Apache Subversion - an open source version control system https://subversion.apache.org/ sundials 7.4.0+mkl-2024.1 Expand sundials description Description: SUNDIALS: SUite of Nonlinear and DIfferential/ALgebraic Equation Solvers https://computing.llnl.gov/projects/sundials (arch/avx512 gcc/13.2.0 openmpi/5.0.6) superlu 5.3.0, 5.3.0+mkl-2019.5, 7.0.0, 7.0.0+mkl-2019.5 Expand superlu description Description: SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations on high performance machines http://crd-legacy.lbl.gov/~xiaoye/SuperLU/ (arch/avx512 gcc/13.2.0), (arch/avx512 intel/2023.2) swig 4.2.0 Expand swig description Description: SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages http://www.swig.org taudem 5.3.8 Expand taudem description Description: TauDEM (Terrain Analysis Using Digital Elevation Models) is a suite of Digital Elevation Model (DEM) https://github.com/dtarb/TauDEM/wiki (arch/avx512 gcc/13.2.0 openmpi/5.0.6) tbb 2021.13.0 Expand tbb description Description: Threading Building Blocks (TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability https://github.com/oneapi-src/oneTBB (arch/avx2 gcc/13.2.0), (arch/avx512 gcc/13.2.0), (arch/avx512 gcc/9.5.0) tblite 0.4.0 Expand tblite description Description: Light-weight tight-binding framework https://github.com/tblite/tblite (arch/avx512 intel/2023.2) trimmomatic 0.39 Expand trimmomatic description Description: Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data. The selection of trimming steps and their associated parameters are supplied on the command line http://www.usadellab.org/cms/?page=trimmomatic udunits 2.2.28 Expand udunits description Description: A C library for units of physical quantities and a unit-definition and value-conversion utility http://www.unidata.ucar.edu/software/udunits (arch/avx2 gcc/13.2.0), (arch/avx512 gcc/13.2.0) unrar 7.10.2 Expand unrar description Description: Utility used to extract RAR file archives https://www.rarlab.com valgrind 3.24.0 Expand valgrind description Description: An instrumentation framework for building dynamic analysis http://valgrind.org/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 gcc/9.5.0 openmpi/4.1.6) vasp 6.1.2-sol, 6.3.2-vtst, 6.3.2-vtst-sol, 6.5.0-iyer, 6.5.1-deng Expand vasp description The Vienna Ab initio Simulation Package (VASP) (arch/avx512 gcc/11.5.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 intel/2023.2 intelmpi/2021.10) vaspkit 1.5.1 Expand vaspkit description Description: VASPKIT aims at providing a powerful and user-friendly interface to perform high throughput analysis of various material properties from the raw calculated data using the widely-used VASP code https://vaspkit.com/index.html vcftools 0.1.16 Expand vcftools description Description: A set of tools written in Perl and C++ for working with VCF files, such as those generated by the 1000 Genomes Project https://vcftools.github.io (arch/avx512) velvet 1.2.10 Expand velvet description Description: Sequence assembler for very short reads http://www.ebi.ac.uk/~zerbino/velvet/ vep 113.4 Expand vep description Description: Variant Effect Predictor (VEP) determines the effect of your variants (SNPs, insertions, deletions, CNVs or structural variants) on genes, transcripts, and protein sequence, as well as regulatory regions. Includes EnsEMBL-XS, which provides pre-compiled replacements for frequently used routines in VEP https://www.ensembl.org/info/docs/tools/vep vim 9.1 Expand vim description Description: Vim is a highly configurable text editor built to make creating and changing any kind of text very efficient. It is included as vi with most UNIX systems and with Apple OS X https://www.vim.org visit 3.4.2 Expand visit description Description: Open Source, interactive, scalable, visualization, animation and analysis tool https://wci.llnl.gov/simulation/computer-codes/visit voroplusplus 0.4.6 Expand voroplusplus description Description: Voro++ is a software library for carrying out three-dimensional computations of the Voronoi tessellation http://math.lbl.gov/voro++/ vtk 9.4.0 Expand vtk description Description: The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation http://www.vtk.org (arch/avx512 gcc/13.2.0) wine 10.0 Expand wine description Description: Wine (originally an acronym for \u0026lsquo;Wine Is Not an Emulator\u0026rsquo;) is a compatibility layer capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, macOS, \u0026amp; BSD https://www.winehq.org/ wxwidgets 3.0.2 Expand wxwidgets description Description: wxWidgets is a C++ library that lets developers create applications for Windows, Mac OS X, Linux and other platforms with a ; single code base. It has popular language bindings for Python, Perl, Ruby and many other languages, and unlike other cross-platform toolkits, wxWidgets gives applications a truly native look and feel because it uses the platform\u0026rsquo;s native API rather than emulating the GUI https://www.wxwidgets.org (arch/avx512 gcc/13.2.0) xfemm 4.0 Expand xfemm description Description: FEMM: Magnetics, Electrostatics, Heat Flow, and Current Flow https://www.femm.info/wiki/HomePage (arch/avx512 gcc/13.2.0) xgboost 3.0.2 Expand xgboost description Description: XGBoost is an open-source software library that implements gradient boosting algorithms https://xgboost.readthedocs.io/ (cuda/12.4.1 arch/avx2 gcc/13.2.0) xtb 6.7.1, 6.7.1+mkl-2019.5 Expand xtb description Description: xtb - An extended tight-binding semi-empirical program package https://xtb-docs.readthedocs.io (arch/avx512 gcc/13.2.0), (arch/avx512 intel/2023.2) xz 5.6.4 Expand xz description XZ Utils provide a general-purpose data-compression library and command-line tools for xz and lzma formats yaml-cpp 0.8.0 Expand yaml-cpp description Description: A YAML parser and emitter in C++ matching the YAML 1.2 spec https://github.com/jbeder/yaml-cpp yaxt 0.11.3 Expand yaxt description Description: Yet Another eXchange Tool https://www.dkrz.de/redmine/projects/yaxt (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) z3 4.13.4 Expand z3 description Description: Z3 is a theorem prover from Microsoft Research with support for bitvectors, booleans, arrays, floating point numbers, strings, and other data types. This module includes z3-solver, the Python interface of Z3 https://github.com/Z3Prover/z3 zoltan 3.83, 3.901 Expand zoltan description Description: Zoltan Dynamic Load Balancing and Graph Algorithm Toolkit https://sandialabs.github.io/Zoltan/ (arch/avx512 gcc/13.2.0 openmpi/4.1.6), (arch/avx512 gcc/13.2.0 openmpi/5.0.6), (arch/avx512 intel-one/2024.1 openmpi/5.0.6), (arch/avx512 intel/2023.2 openmpi/4.1.6) zstd 1.5.6 Expand zstd description Description: Zstandard, or zstd as short version, is a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios https://facebook.github.io/zstd/ "},{"id":"39","rootTitleIndex":"6","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-database fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/data-sizes-and-quota/","permalinkTitle":"Homepage / Storage and Data / Data sizes and quotas","title":"Data sizes and quotas","content":"Data size and quotas# This section explains how to find the actual space and inode usage of /home/ and /project allocations on Grex. We limit the size of the data and the number of files that can be stored on these filesystems. The table provides a \u0026ldquo;default\u0026rdquo; storage quota on Grex. Larger quota can be obtained on /project via UM local RAC process.\nFile system Type Total space Bulk Quota Files Quota /home NFSv4/RDMA 15 TB 100 GB / user 0.5M per user /project Lustre 2 PB 5-20 TB / group 1M / user, 2M / group To figure out where your current usage stands with the limit, POSIX quota or Lustre\u0026rsquo;s analog, lfs quota, commands can be used. A convenient command, diskusage_report summarizes usage and quota across all the available filesystems.\nNFS quota# The /home/ filesystem is served by NFSv4 and thus supports the standard POSIX quota command. For the current user, it is just:\nquota or\nquota -s The command will result in something like this (note the -s flag added to make units human readable):\n[someuser@yak ~]$ quota -s Disk quotas for user someuser (uid 12345): Filesystem space quota limit grace files quota limit grace 192.168.x.y:/ 249M 100G 105G 4953 500k 1000k The output is a self-explanatory table. There are two values: soft \u0026ldquo;quota\u0026rdquo; and hard \u0026ldquo;limit\u0026rdquo; per each of (space, files) quotas. If you are over soft quota, the value of used resource (space or files) will have a star * to it, and a grace countdown will be shown. Getting over grace period, or over the hard limit prevents you from writing new data or creating new files on the filesystem. If you are over quota on /home, it is time to do some cleaning up there, or migrate the data-heavy items to /global/scratch where they belong.\nCAVEAT: The Alliance (Compute Canada) breaks the POSIX standard by redefining the quota command in their software stack. So, after loading the CCEnv module on Grex, the quota command may return garbage. For accurate output about your quota, load the local software environment SBEnv first before running the command quota or use the command diskusage_report (see below). Lustre quota# The main Lustre storage appliances on Grex, is the /project filesystem. The previous filesystem was called /global/scratch/ and , as of now, is disabled and not available for users.\nOn a Lustre filesystem, three types of quota are possible: per user, across all the filesystem; per group, across all the filesystem; and a directory quota that is per directory, per group.\n/project/ (current)# This filesystem has a similar hierarchical directory structure to the Alliance/ComputeCanada HPC systems. On Grex, it is like follows:\n/project/Project-GID/{user1, user2, user3} Where the Project-GID is a number (or identifier) of the PI\u0026rsquo;s default RAPI group in CCDB, and user1..3 are the users on Grex, including the PI.\nNote that the directories get created, and the quota is set, on a first login of a user to the Grex system. Before the first login of any user, no /home and /project directories exist and no quota is set for them on Grex. To have your directories created, please log in to Grex either via a normal SSH through a regular login node or via OOD . It is inconvenient to go by using numerical values of the Project-GID in the paths, so there are symbolic links present in each user\u0026rsquo;s /home/$USER/projects directory that point to his /project directories. A user can belong to more than one research group and thus can have more than one project link. Also, on the filesystem there is a system of symbolic links in the form of /project/Faculty/def-PIname/ .\n[someuser@yak ~]$ lfs quota -h -p 123456 /project/123456 Disk quotas for prj 123456 (pid 123456): Filesystem used quota limit grace files quota limit grace /project/123456 150.6G 4.883T 5.371T - 208654 2000000 2200000 - In addition to the directory quota, each user has her own quota, presently for inodes (the number of files and directories on the entire filesystem. To see the space and inode usage per user, run the following command:\n[someuser@yak ~]$ lfs quota -h -u $USER /project Disk quotas for usr kerrache (uid 123456): Filesystem used quota limit grace files quota limit grace /project 150.6G 0k 0k - 175394 1000000 1100000 - A wrapper for quota# To make it easier, we have set a custom script with the same name as for the Alliance (Compute Canada) clusters, diskusage_report, that gives both /home and /project quotas (space and number of files: usage/quota or limits), as in the following example: [someuser@bison ~]$ diskusage_report ------------------------------------------------------------------------ Description (FS) Space (U/Q) # of files (U/Q) ------------------------------------------------------------------------ /home (someuser) 254M/104G 4953/500k /project (def-professor) 131G/2147G 992k/1000k ------------------------------------------------------------------------ /project/6543210 = /home/someuser/projects/def-professor ------------------------------------------------------------------------\nThe command diskusage_report can also be invoked with the argument \u0026ndash;home or \u0026ndash;project to get the quota for the corresponding file system.\n"},{"id":"40","rootTitleIndex":"1","rootTitle":"Grex: High Performance Computing Cluster at University of Manitoba","rootTitleIcon":"fa-solid fa-microchip fa-lg","rootTitlePath":"/grex-docs/grex/","rootTitleTitle":"Homepage / Grex: High Performance Computing Cluster at University of Manitoba","permalink":"/grex-docs/grex/","permalinkTitle":"Homepage / Grex: High Performance Computing Cluster at University of Manitoba","title":"Grex: High Performance Computing Cluster at University of Manitoba","content":" Introduction# Grex is a UManitoba High Performance Computing (HPC) system, first put in production in early 2011 as part of WestGrid consortium. \u0026ldquo;Grex\u0026rdquo; is a Latin name for \u0026ldquo;herd\u0026rdquo; (or maybe \u0026ldquo;flock\u0026rdquo;?). The names of the Grex login nodes (bison , yak ) also refer to various kinds of bovine animals.\nPlease note that older login nodes tatanka and zebu are decommissioned during and after the outage of August - September 2024. These login nodes are no longer available. Since being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators.\nIf you are a new Grex user, proceed to the quick start guide and documentation right away.\nHardware# The original Grex was an SGI Altix machine, with 312 compute nodes (Xeon 5560, 12 CPU cores and 48 GB of RAM per node) and QDR 40 Gb/s InfiniBand network. The SGI Altix machines were decommissioned in Sep 2024. In 2017, a new Seagate Storage Building Blocks (SBB) based Lustre filesystem of 418 TB of useful space was added to Grex. The SBB that serves as /scratch is not available. In 2020 and 2021, the University added 57 Intel CascadeLake CPU nodes, a few GPU nodes, a new NVME storage for home directories, and EDR InfiniBand interconnect.\nOn March 2023, a new storage of 1 PB was added to Grex. It is called /project filesystem.\nOn January 2024, the /project was extended by another 1 PB.\nOn Sep 2024, new AMD Genoa nodes have been added (30 nodes with a total of 5760 cores).\nOn April 2025, a new GPU node L40S with 2 GPUs was added to Grex.\nThe current computing hardware available for general use is as follow:\nLogin nodes# As of Sep 14, 2022, Grex is using UManitoba network. We have decommissioned the old WG and BCNET network that was used for about 11 years. Now, the DNS names use hpc.umanitoba.ca instead of the previous name westgrid.ca.\nOn Grex, there are multiple login nodes:\nYak: yak.hpc.umanitoba.ca (please note that the architecture for this node is avx512) Bison: bison.hpc.umanitoba.ca (a second login nodes similar to Yak) Grex: grex.hpc.umanitoba.ca is a DNS alias to the above Yak and Bison login nodes OOD: ood.hpc.umanitoba.ca (only used for OpenOnDemand Web interface and requires VPN if used outside campus network) To login to Grex in the text (bash) mode, connect to grex.hpc.umanitoba.ca using a secure shell client, SSH .\nCompute nodes# There are several researcher-contributed nodes (CPU and GPU) to Grex which make it a \u0026ldquo;community cluster\u0026rdquo;. The researcher-contributed nodes are available for others on opportunistic basis; the owner groups will preempt the others\u0026rsquo; workloads.\nThe current compute nodes available on Grex are listed in the following table:\nCPU Nodes CPUs/Node Mem/Node GPU GPUs/Node VMem/GPU Network(InfiniBand) IntelXeon 6248 12 40 384 GB N/A N/A N/A EDR 100GB/s IntelXeon 6230R 43 52 188 GB N/A N/A N/A EDR 100GB/s AMDEPYC 96541 31 192 750 GB N/A N/A N/A HDR 200GB/s AMDEPYC 96541 4 192 1500 GB N/A N/A N/A HDR 200GB/s AMDEPYC 96342 5 168 1500 GB N/A N/A N/A HDR 100GB/s IntelXeon 52183 2 32 180 GB nVidia Tesla V100 4 32 GB FDR 56GB/s IntelXeon 52184 3 32 180 GB nVidia Tesla V100 4 16 GB FDR 56GB/s IntelXeon 6248R5 1 48 1500 GB nVidia Tesla V100 16 32 GB EDR 100GB/s AMDEPYC 7402P6 2 24 240 GB nVidia A30 2 24 GB EDR 100GB/s AMDEPYC 7543P7 2 32 480 GB nVidia A30 2 24 GB EDR 100GB/s AMDEPYC 93343 1 64 370 GB nVidia L40S 2 48 GB HDR 200GB/s Storage# Grex\u0026rsquo;s compute nodes have access to three filesystems:\nFile system Type Total space Quota per user Quota per group /home NFSv4/RDMA 15 TB 100 GB N/A /project Lustre 2 PB N/A 5 TB In addition to the shared file system, the compute nodes have their own local disks that can be used as temporary storage when running jobs .\nSoftware# Grex is a traditional HPC machine, running Linux and SLURM resource management systems. On Grex, we use different software stacks .\nWeb portals and GUI# In addition to the traditional bash mode (connecting via SSH), users have access to:\nOpenOnDemand : on Grex, it is possible to use OpenOnDemand (OOD for short) to login to Grex and run batch or GUI applications (VNC Desktops, Matlab, Gaussview, Jupyter, \u0026hellip;) Useful links# Digital Research Alliance of Canada (Alliance), formerly known as Compute Canada Alliance documentation Local Resources at UManitoba Grex status page WestGrid ceased operations on April 1st, 2022. The former WestGrid institutions are now re-organized into two consortia: BC DRI group and Prairies DRI group. CPU nodes available for all users (of these, five are contributed by a group of CHRIM researchers).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCPU nodes contributed by Prof. M. Cordeiro (Department of Agriculture).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes available for all users.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes contributed by Prof. R. Stamps (Department of Physics and Astronomy).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes contributed by Prof. L. Livi (Department of Computer Science).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes contributed by Faculty of Agriculture.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPU nodes contributed by Prof. M. Cordeiro (Department of Agriculture).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":"41","rootTitleIndex":"10","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-cloud fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/guide-lines/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal / Guide Lines","title":"Guide Lines","content":"OpenOnDemand Guide# How to connect to OOD?# URL: https://ood.hpc.umanitoba.ca/ It requires a Grex account and MFA. VPN required if used outside UManitoba network. What is OOD used for?# Via OOD interface, one can use and have access to the following:\nInteractive Apps: GUI applications, Desktops and servers (MATLAB, Gaussview, RStudion, Jupyter, OVITO, ParaView, \u0026hellip;).\nFile browser: navigate through the directories and files under home and project directories. Many operations related to files and directories are accessible from the file browser (copy, delete, edit, move, upload, download, \u0026hellip;). It gives also access to GLOBUS link.\nJobs: Status of queues, Rinning Jobs and a JobComposer interface to submit batch scripts.\nClusters: Status of Grex system and its SLURM partitions.\nHow to start applications?# Here is a summary on how to start any OOD GUI application:\nFrom the menu, Interactive Apps, select the application to use.\nA form will show up. This form is used to set some parameters (like accounting group, number of CPUs, \u0026hellip;) that are required to run the application. For some applications, there are already predefined parameters and for others you may need to set additional parameters.\nOnce all the parameters are set, use the button Launch to start the application by submitting a job. This later will start when the resources are available.\nOnce the job starts, use the link Connect to \u0026hellip; to use the corresponding application.\nWhat are the parameters to set in the form?# Here is a list of parameters that may appear on the form before launching any interactive application:\nAccounting group:# If the user has only one accounting group, it will be picked automatically by slum. If the user has more than one accounting group, setting the accounting group is mandatiory, like when using sbatch or salloc.\nWall time:# Use this field to set the wall time you need to use OOD application. As of now, the maximum wall time is 6 hours.\nE-mail notifications# Similar to sbatch, one can add E-mail notifications to receive emails from the scheduler for your OOD session. It requires to provide a valid email in the field Email that will show up when you select at least one of the following options:\nI would like to receive an email when the session starts I would like to receive an email when the session terminates Slurm partitions# We have interactive applications where the partition is predeined, like Grex Desktop Simplified, Gnuplot and Grace. However, for other application, a field with the name SLURM partition will show in the form as in the following screenshot:\nSlurm Partitions You can use this field to select the partition where you want to run your OOD application:\nSelect Slurm Partitions There is a summary of all the partitions and their characteristics {name of the partition, number of CPUs per node, memory per node, memory per core}.\nPartition type: please make sure to select the right partition for your application (cpu or gpu). If your application does not use or support GPU, selct the CPU partition to run it. Number of Cores# Memory# Memory multiplier# Modules and dependencies# "},{"id":"42","rootTitleIndex":"2","rootTitle":"Quick Start Guide","rootTitleIcon":"fa-solid fa-bolt-lightning fa-lg","rootTitlePath":"/grex-docs/start-guide/","rootTitleTitle":"Homepage / Quick Start Guide","permalink":"/grex-docs/start-guide/","permalinkTitle":"Homepage / Quick Start Guide","title":"Quick Start Guide","content":"Grex# Grex is an UManitoba High Performance Computing (HPC) system, first put in production in early 2011 as part of WestGrid consortium. Now it is owned and operated by the University of Manitoba. Grex is accessible only for UManitoba users and their collaborators.\nA Very Quick Start guide# 1. Create an account on CCDB . You will need an institutional Email address. If you are a sponsored user, you\u0026rsquo;d want to ask your PI for their CCRI code {Compute Canada Role Identifier}. For a detailed procedure, visit the page Apply for an account .\n2. If you did not set yet MFA, please enrol using the instructions from this page .\n3. Wait for half a day. While waiting, install an SSH client, and SFTP client for your operating system.\n4. Connect to grex.hpc.umanitoba.ca (or yak.hpc.umanitoba.ca or bison.hpc.umanitoba.ca) with SSH, using your username/password from step 1.\n5. Make a sample job script, call it, for example, sleep-job.sh . The job script is a text file that has a special syntax to be recognized by SLURM. You can use the editor nano , or any other right on the Grex SSH prompt (vim, emacs, pico, \u0026hellip; etc); you can also create the script file on your machine and upload to Grex using your SFTP client or scp.\nsleep-job.sh#!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=500M #SBATCH --time=00:01 echo \u0026#34;Starting run at: `date`\u0026#34; echo \u0026#34;Hello world! will sleep for 10 seconds\u0026#34; time sleep 10 echo \u0026#34;Program finished with exit code $? at: `date`\u0026#34; 6. Submit the script using sbatch command, to the skylake partition using:\nsbatch --partition=skylake sleep-job.sh 7. Wait until the job finishes; you can monitor the queues state with the \u0026lsquo;sq\u0026rsquo; command. When the job finishes, a file slurm-NNNN.out should be created in the same directory.\n8. Download the output slurm-NNNN.out from yak.hpc.umanitoba.ca to your local machine using your SFTP client.\n9. Congratulations, you have just run your first HPC-style batch job. This is the general workflow, more or less; you\u0026rsquo;d just want to substitute the sleep command to something useful, like ./your-code.x your-input.dat .\n"},{"id":"43","rootTitleIndex":"3","rootTitle":"Access and Usage Conditions","rootTitleIcon":"fa-solid fa-id-card fa-lg","rootTitlePath":"/grex-docs/access/","rootTitleTitle":"Homepage / Access and Usage Conditions","permalink":"/grex-docs/access/","permalinkTitle":"Homepage / Access and Usage Conditions","title":"Access and Usage Conditions","content":"Access Conditions# Grex is open to all researchers at University of Manitoba and their collaborators. The main purpose of the Grex system is Research; it might be used for grad studies courses with a strong research component, for their course-based research.\nAccess to the system, and resource allocations are by research group; that is, the Principal Investigator (PI) receives resources for his group, and approves access for his group members.\nGrex\u0026rsquo;s resources (CPU and GPU time, disk space, software licenses) are automatically managed by a batch scheduler, SLURM , according to the University\u0026rsquo;s priorities. There is a process of resource allocation competition (RAC) to get an increased share of Grex resources; however, a \u0026ldquo;Default\u0026rdquo; share of these resources is available immediately and free of charge by getting an account.\nIt is expected that Grex accounts and resource allocations are used for the research projects they are requested for.\nOwners of the user-contributed hardware on Grex have preferential access to their hardware, which can only be used by the general community of UM researchers when idle and not reserved.\nGetting an account on Grex# Grex is using the Compute Canada (now the Alliance) account management database (CCDB ).\nThere are two technical conditions for getting access:\nAn active CCDB account. An active CCDB \u0026ldquo;role\u0026rdquo; affiliated with UManitoba (University of Manitoba). Any eligible Canadian Faculty member can get an Alliance account in the CCDB system, in their own right. Researchers and tecnhical staff members in other roles (undergraduate students, doctoral student, postdoctoral fellow, research assistant, or a non-research staff member, external collaborator) will need to be sponsored in CCDB by a Faculty member. The PI registers in the CCDB first, and then he/she can sponsor his/her group members under his/her account.\nOnce an Alliance account for a user has been approved and made active in CCDB, the user can start using Grex\u0026rsquo;s computing and data storage facilities.\nGuidelines of the Acceptable Use# Grex adheres to the Alliance\u0026rsquo;s Privacy and Security policy , and to University of Manitoba IT Security and Privacy policies Users that have Grex account have accepted both.\nIn particular, user\u0026rsquo;s accounts cannot and should not be shared with anyone for whatever reason. Our support staff will never ask for your password. Sharing any account information (login/password or SSH private keys) leads to immediate blocking of the account. UNIX groups and shared project spaces can be used for data sharing . Usage is monitored and statistics are collected automatically, to estimate the researcher\u0026rsquo;s needs and future planning, and to troubleshoot day to day operational issues. Users of Grex should be \u0026ldquo;fair and considerate\u0026rdquo; in their usage of the systems, trying not to allow for unnecessary and/or inefficient use of the resources or interfering with other users\u0026rsquo; work. We reserve to ourselves the right to monitor for inefficient use and ask users to stop their activities if they threaten the general stability of the Grex system. Getting a Resource Allocation on Grex# Similarly, to the Alliance (formerly known as Compute Canada), there is a two-tier system for resource allocation on Grex. Every group gets a \u0026ldquo;Default\u0026rdquo; allocation of Grex resources (computing time and storage space). Groups that need a larger fraction of resources and use Grex intensively, might want to apply for a local Resource Allocation Competition (Grex-RAC).\nGrex\u0026rsquo;s local RAC calls are issued once a year. They are reviewed by a local Advanced Research Computing Committee and may be scaled according to the availability of resources. The instructions and conditions of this year\u0026rsquo;s RAC are provided on the RAC template document (sent via email when the RAC is announced).\n"},{"id":"44","rootTitleIndex":"6","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-database fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/data-backup/","permalinkTitle":"Homepage / Storage and Data / Data Backup","title":"Data Backup","content":"Backup policies# Since late 2023, there has been a tape backup for user data stored on main Grex filesystems, /home and /project. Our limited resources and large amounts of data do put some limitations on what and how fast can be backed up and restored. All backup is done to tape in the same HPCC data centre.\nThe /home filesystem is backed up daily using incremental backup. A new full backup is done quarterly.\nWe aim for the /project filesystem to have a monthly backup, with a full backup done quarterly. However, due to the large amounts of data, the backup for particularly active projects having larger file counts and/or large amounts of data can be delayed past one month.\nRetention of the data on tape is determined by our available tape space. Generally, there is a month\u0026rsquo;s worth of retained data on /project and three months on /home.\nRestoring your data from backup# Restoring data from tape backup requires a Sysadmin intervention.\nOn /project it may take more time if the request happens during the time of the full backup happening (in January, April, July, October).\nPlease send us a support request to restore your data from the tape backup if needed.\nDisclaimer of responsibility despite Backup# Any electronic or mechanical system can fail. The tape system is located in the same data centre as the storage is backed up. So, while backup we have added a level of data protection against system failures and user errors, it is by no means absolute.\nUsers are encouraged to search and adopt data preservation strategies for their research data that is particularly important to them (for example, a set of data that is unique and cannot be easily re-generated). One of the popular strategies is the 3+2+1: It usually is understood as keeping at least 3 copies of your data, on at least 2 storage locations, and at least 1 copy off-site.\n"},{"id":"45","rootTitleIndex":"4","rootTitle":"Connecting to Grex","rootTitleIcon":"fa-solid fa-plug fa-lg","rootTitlePath":"/grex-docs/connecting/","rootTitleTitle":"Homepage / Connecting to Grex","permalink":"/grex-docs/connecting/","permalinkTitle":"Homepage / Connecting to Grex","title":"Connecting to Grex","content":"Connecting to Grex# In order to use almost any HPC system, you would need to be able to somehow connect and log in to it. Also, it would be necessary to be able to transfer data to and from the system. The standard means for these tasks are provided by the SSH protocol . The following hosts (login nodes) are available:\ngrex.hpc.umanitoba.ca yak.hpc.umanitoba.ca bison.hpc.umanitoba.ca To log in to Grex in the text (or bash) mode, connect to one of the above hosts using an SSH (Secure SHELL) client.\nThe DNS name grex.hpc.umanitoba.ca serves as an alias for yak.hpc.umanitoba.ca and bison.hpc.umanitoba.ca. Before trying to connect to Grex, please make sure that you have already set MFA .\nTo connect to Grex, use:\nssh someuser@grex.hpc.umanitoba.ca or\nssh someuser@yak.hpc.umanitoba.ca Please remember to replace someuser with your Alliance user name. Transferring Data# Uploading and downloading your data can be done using an SCP/SFTP capable file transfer client. For more details and options, please refer to the dedicated page about Transferring Data .\nOpenOnDemand (OOD) Web Interface# Since October 2021, there is an OpenOnDemand (OOD) Web interface to Grex, currently available at https://ood.hpc.umanitoba.ca from UManitoba IP addresses. OOD provides a way to connect both in text and graphical mode right in the browser, to transfer data between Grex local machines, and to run jobs.\nSee the documentation for more details on how to connect from various clients of operating systems: SSH , OOD .\nInternal links# Multi-Factor Authentication Connecting with SSH Connect with OOD "},{"id":"46","rootTitleIndex":"6","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-database fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/data-sharing/","permalinkTitle":"Homepage / Storage and Data / Data sharing","title":"Data sharing","content":"Data Sharing# By default, data on Grex are owned by the user and are not accessible to other Grex users or to external parties.\nIn research, it’s often necessary to share datasets or code within a research group or with collaborating groups. This documentation explains how to share data stored on a specific HPC system, in this case, Grex. Sharing data outside the HPC system can be done through other methods, such as Globus or MS OneDrive.\nHow to not share data: Sharing account credentials is strictly forbidden. Similarly, making data \u0026ldquo;world-accessible\u0026rdquo; (open to all users) is discouraged. Sharing account login information (like passwords or SSH keys) is strictly prohibited on Grex and most other HPC systems. Making a directory universally accessible—especially with write permissions (e.g., using the Linux/Unix +rwx or 777 file mode)—is risky, as it allows anyone on the system to not only read but potentially delete the entire directory. There is a secure mechanism for data sharing that doesn’t involve sharing account credentials. To access each other’s data on Grex, you should use UNIX groups and permissions, or ideally, Access Control Lists (ACLs) for more granular control over data access permissions.\nUNIX Groups and File Ownership# First, let’s review how permissions work on a Linux system.\nEach UNIX (or Linux) filesystem object (such as a file or directory) is owned by an individual user (the owner) and a group (which may include multiple users). Access permissions can be set for the owner, the group, and \u0026ldquo;others\u0026rdquo; (all other users on the system). You can view ownership and permissions for objects in the current directory with the command ls -la.\nDepending on the filesystem (e.g., /project, /home, or, on Alliance systems, /scratch), group ownership is assigned either to the user’s personal group or to the Principal Investigator’s (PI\u0026rsquo;s) group. For example, on /home, where users own their data, the listing might look like this:\n[someuser@yak Data]$ ls -la total 328408 drwxrwxr-x 2 someuser someuser 75 Aug 9 2023 . drwxrwxr-x 25 someuser someuser 4096 Oct 18 10:46 .. -rw-r--r-- 1 someuser someuser 64479232 Aug 9 2023 file.txt -rw-r--r-- 1 someuser someuser 271805449 Aug 9 2023 Calgary_Adult_Skull_Atlas.mnc On /project, where a group-based hierarchy exists, ls will show group ownership assigned to the PI’s default allocation group, as follows:\nsomeuser@yak Data]$ ls -la total 328420 drwxrwsr-x 3 someuser def-somePI 4096 Oct 21 14:43 . drwxrwsr-x 3 someuser def-somePI 4096 Oct 21 14:35 .. -rw-r--r-- 1 someuser def-somePI 64479232 Oct 21 14:35 file.txt -rw-r--r-- 1 someuser def-somePI 271805449 Oct 21 14:35 Calgary_Adult_Skull_Atlas.mnc drwxrwsr-x 2 someuser def-somePI 4096 Oct 21 14:43 Docker In UNIX/Linux, permissions are associated with group ownership. To adjust access, you need to change both ownership and permissions. You can override defaults using the chmod command to modify file permissions, and chown or chgrp to change owner and group, respectively. This assumes the required group (or user) already exists on the system.\nSharing within the PI’s group is straightforward using chgrp and chmod with the def-somePI group. These def-somePI groups are automatically created based on CCDB group memberships and are available for each group on Grex, forming the directory structure on the /project filesystem. To make a directory accessible to a group, a user can modify ownership masks as follows:\n# Allow read, write, and search access for all members of the def-somePI group. chmod g+rwX Calgary_Adult_Skull_Atlas.mnc Docker In some cases, you may need more precise control over access—perhaps only a subset of the PI’s group should access a dataset, or you need to grant access to users from multiple research groups (more than one PI). In such cases, a new UNIX group is required. Grex and Alliance systems receive group and user information from the CCDB. Thus, the group must be requested by the PI.\nRequesting a Data-sharing Group# A Principal Investigator (PI) can request the creation of a special UNIX group to facilitate data sharing with other researchers. To set up a group, the PI should email support at support@tech.alliancecan.ca , including a list of users to be added to the group. Please designate one user as the authority/manager for the group. If someone requests to join the group later, we will ask the designated authority for approval before adding the new member.\nThe group name should follow the format wg-xxxxx, where xxxxx represents up to five characters. Indicate your preferred group name in the email.\nThe group will be set up on the Grex system, which may take a day or two. You will receive an email notification whenever you are added to or removed from a UNIX group. Once your wg-xxxxx UNIX group is created, data sharing permissions can be configured using either chown / chmod commands or Linux Access Control Lists (ACLs), as described below.\nTo share a directory with the group, set group ownership and permissions on the directory. For example:\nchgrp -R wg-group ./dirname chmod g+s ./dirname Ensure that the parent directories also have the minimally necessary permissions (the search permission X) to allow access.\nTo grant read and access permissions to all files in a directory, use the following:\nchmod -R g+rX ./dirname The uppercase X in this command sets execute permissions on subdirectories (allowing group members to list contents) and on executable files.\nIf group members need both read and write access in the shared directory (to create and modify files), each member should add the following line to their ~/.bashrc or ~/.cshrc file:\numask 007\nto the ~/.bashrc or ~/.cshrc file in their respective home directories. Furthermore, you must add write permission to the shared directory itself:\nchmod -R g+rwX dirname which would allow read and write access to the directory dir and all its files and subdirectories.\nLinux ACLs# On most modern Linux filesystems, such as Lustre FS used for Grex’s /project, Linux Access Control Lists (ACLs) provide more fine-grained access control than traditional UNIX groups. ACLs allow flexible permissions by decoupling file ownership from access rights, enabling access control for multiple users or groups without changing file ownership. This flexibility makes ACLs the preferred method for organizing data sharing.\nThe Alliance’s Sharing Data a documentation describes using ACLs, and Grex’s /project filesystem has a similar hierarchical structure. Grex’s /home uses a NFSv4 FS, which has its own ACL syntax, not compatible with Linux ACLs. Generally, data sharing is recommended under /project, while $HOME remains private to each user.\nTwo main commands control ACLs on files and directories:\ngetfacl: displays the current ACL settings. setfacl: modifies ACL settings. The access modes are similar to chmod\u0026rsquo;s symbolic codes:\nr for read w for write X for execute/search permissions on directories Since ACLs are independent of file ownership, they allow setting permissions for individual users, multiple users, group or multiple groups. For example, to share a file or directory under /project:\n# share a single file for full access setfacl -m u:otherusers:rwX Calgary_Adult_Skull_Atlas.mnc #share a directory for reading and search, but first make it the Default setting, i.e. for current and future files there setfacl -d -m u:otheruser:rX /home/someuser/projects/def-somePI/Shared_data setfacl -R -m u:otheruser:rX /home/someuser/projects/def-somePI/Shared_data # share a directory for full access based on a Unix group wg-abcde setfacl -d -m g:wg-abcde:rwX /home/someuser/projects/def-somePI/Shared_data setfacl -R -m g:wg-abcde:rwX /home/someuser/projects/def-somePI/Shared_data Note that the above example refers to /home and uses the symlink to project. This will not work for data sharing between different PIs because $HOME is private. Thus, to refer to the shared data, \u0026ldquo;real\u0026rdquo; paths on project of the form /project/PI-GIDnumber/Shared_data must be used.\nData sharing across Research Groups# Linux ACLs described above are the way to share data across research groups.\nWhen sharing data between different PIs, residing under separate /project directories, \u0026ldquo;search\u0026rdquo; access must be granted at the top project directory level. For example, to allow search access for group wg-abcde on a PI\u0026rsquo;s project directory:\nsetfacl -m g:wg-abcde:X /project/123456/ Note that the real, absolute path to the project must be used rather than a $HOME-based symbolic link. To get the absolute path to the PI\u0026rsquo;s project, use diskusage_report tool.\nManaging ACLs: viewing and removing# Use getfacl to display current ACL settings. Use setfacl -b to remove ACLs when they are no longer needed:\n# Display current ACLs for a directory getfacl /home/someuser/projects/def-somePI/Shared_data # Remove all ACLs from a directory setfacl -bR /home/someuser/projects/def-somePI/Shared_data External links# Linux permissions ACLs Alliance documentation on the same topic "},{"id":"47","rootTitleIndex":"5","rootTitle":"Transferring data","rootTitleIcon":"fa-solid fa-arrow-right-arrow-left fa-lg","rootTitlePath":"/grex-docs/data-transfer/","rootTitleTitle":"Homepage / Transferring data","permalink":"/grex-docs/data-transfer/","permalinkTitle":"Homepage / Transferring data","title":"Transferring data","content":"Introduction# The recommended clients are OpenSSH (providing ssh and scp, sftp command line tools on Linux and Mac OS X) and PuTTY/WinSCP/X-Ming or MobaXterm under Windows. For large amount of data, Globus is a better option.\nOpenSSH tools: SCP, SFTP# The OpenSSH package, available on Linux, MacOS and recent versions of Windows, provides not only the command line SSH client, but two command line file transfer tools: SFTP and SCP. Thus, on almost any system that allows for SSH connection, data transfers can be performed using these tools. OpenSSH encrypts all the traffic and provides several authentication options. A useful option for SCP and SFTP is to have a key pair, with the public key deposited in CCDB.\nSCP# SCP behaves like cp. It needs a \u0026ldquo;source\u0026rdquo; and a \u0026ldquo;destination\u0026rdquo; specified. Either of these can be local or remote. The remote destination has the format of user@host:/path .\nTo copy a file myfile.fchk to Grex, from the current directory into his home directory, a user would run the following command:\nscp ./myfile.fchk someuser@grex.hpc.umanitoba.ca:/home/someuser The same example but using a key pair, assuming the corresponding public key is deposited in CCDB:\nscp -i a-private-key.key ./myfile.fchk someuser@grex.hpc.umanitoba.ca:/home/someuser The Home filesystem is limited in space and performance. For larger files, it might make sense to use SCP for the Project filesystem instead. A convenience symbolic link under /home/someuser/projects points to the Project filesystem.\nscp ./myfile_bigdata.csv someuser@grex.hpc.umanitoba.ca:/home/someuser/projects/def-somegroup/someuser/ More information about OpenSSH file transfer tools exist on OpenSSH SSP manpage . The Alliance has a detailed Wiki entry on SCP.\nSFTP# On Mac OS and Linux, where OpenSSH client packages are always available, the following command line tools are present: scp, sftp. They work like UNIX cp and ftp commands, except that there is a remote target or source.\nSFTP opens a session and then drops the user to a command line, which provides commands like ls, lls, get, put, cd, lcd to navigate the local and remote directories, upload and download files etc.\nsftp someuser@grex.hpc.umanitoba.ca sftp\u0026gt; lls sftp\u0026gt; put myfile.fchk Please replace someuser with your username.\nFile transfer SCP/SFTP clients with GUI# There are many file transfer clients that provide a convenient graphical user interface (GUI) while using an implementation of SCP or SFTP under the hood.\nSome examples of the popular file transfer clients are:\nWinSCP for Windows. CyberDuck for MacOS X and Windows, has a good MFA support. MobaXterm for Windows has an SFTP app. Cross platform FileZilla Client Other GUI clients will work with Grex too if they provide SFTP protocol support.\nTo use such clients, one would need to tell them that SFTP is needed, use the address grex.hpc.umanitoba.ca (or the name of a Grex login node like yak.hpc.umanitoba.ca or bison.hpc.umanitoba.ca) and your Grex/Alliance username.\nNote that we advise against saving your password in the clients: first, it is less secure, and second, it is easy to store a wrong password. File transfer clients would try to auto-connect automatically, and having a wrong password stored with them will create many failed connection attempts from your client machine, which in turn might temporarily block your IP address from accessing Grex.\nNote that for GUI clients, care should be taken for support of MultiFactor Authentication . MFA is enforced on both Grex and The Alliance HPC systems.\nRSYNC over SSH# rsync is a versatile local and remote copying tool. Because rsync would \u0026ldquo;synchronize\u0026rdquo; the source and destination, it allows for resuming interrupted data transfers without excessive data retransmissions. rsync would equally well synchronize single files and entire directory trees.\nFor uploading and downloading files from HPC machines that allow only SSH access, rsync does support encapsulation of the data stream in an SSH channel.\nAn example of rsync over SSH is provided below:\nrsync -aAHSv -x --delete -e \u0026#34;ssh -i a-private-key.key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \u0026#34; /home/$LOCAL_USER/somedir/ $REMOTE_USER@grex.hpc.umanitoba.ca:/home/$REMOTE_USER/destination/ In the example above,\n-e is the option governing SSH use and behavior for rsync . SSH tries to use a key pair (replace a-private-key.key with the name and location of your actual private key; for Grex, the corresponding public key can be uploaded to CCDB. If the key is not provided or not found, SSH will default to password authentication. /home/$LOCAL_USER/somedir/ is a path on a local machine. An actual source directory must be supplied instead. /home/$REMOTE_USER is a home directory on the Grex system, and $REMOTE_USER is the user name on Grex. The local and remote user names may or may not be the same. note that the trailing slash / matters for rsync! There is a lot of useful documentation pages for rsync ; just one example .\nGlobus Online file transfer# GlobusOnline, or just Globus is a specialized Data Transfer and Data Sharing tool for large transfers over WAN, possibly across different organizations. Globus transfers data, in an efficient and convenient way, between any two so called \u0026ldquo;Globus endpoints\u0026rdquo; or \u0026ldquo;data collections\u0026rdquo;.\nTo use Globus, a user would need at least two endpoints, and an identity (account) for each endpoint is needed. The identities would have to be to be \u0026ldquo;linked\u0026rdquo; using Globus Online portal. There are \u0026ldquo;server\u0026rdquo; endpoints and \u0026ldquo;personal endpoints\u0026rdquo; in Globus.\nGrex users have a choice of using either The Alliance identity that comes with a CCDB account, or UManitoba identity using UMNetID. The preliminary step of linking identities is done by logging in to www.globus.org by finding your organization in the drop-down menu there. This can be Digital Alliance or University of Manitoba. Likely both identities would have to be linked to the Globus online account to be able to transfer data between the Alliance and Grex.\nPlease visit our Globus Server or Globus Personal Endpoint Documentation for instructions on how to use Globus on Grex.\nUsing MS OneDrive, NextCloud and similar cloud storage# It is possible to move data between Grex\u0026rsquo;s storage and several Cloud Storage providers, including MicroSoft OneDrive, NextCloud, etc. by using the powerful Rclone tool. Please visit this Rclone and OneDrive documentation page for instructions.\nFile transfers with OOD browser GUI# It is now possible to use OpenOnDemand on Grex Web interface to download and upload data to and from Grex. Use the Files OOD dashboard menu to select a filesystem (currently /home/$USER and /project filesystems are listed there), and then Upload and Download buttons. There is a limit of about 10GB to the file transfer sizes with OOD. The OOD interface is, as of now, open for UManitoba IP addresses only (i.e., machines from campus and on UM VPN will work).\nThe OOD File app allows also for transferring data to/from MS OneDrive and NextCloud with Rclone tool The OOD File app now \u0026ldquo;integrates\u0026rdquo; with Globus Server endpoint on Grex as well, providing a \u0026ldquo;globus\u0026rdquo; button that redirects to the Globus WebApp pointing to the \u0026ldquo;limited\u0026rdquo; Grex server endpoint.\nMore information is available on our OOD pages Internal links# Globus on Grex Rclone and OneDrive Object Storage on OpenStack "},{"id":"48","rootTitleIndex":"5","rootTitle":"Transferring data","rootTitleIcon":"fa-solid fa-arrow-right-arrow-left fa-lg","rootTitlePath":"/grex-docs/data-transfer/","rootTitleTitle":"Homepage / Transferring data","permalink":"/grex-docs/data-transfer/globus/","permalinkTitle":"Homepage / Transferring data / Using Globus on Grex","title":"Using Globus on Grex","content":"Introduction# Globus is a service for fast, reliable, secure file transfer. Designed specifically for researchers, Globus has an easy-to-use interface with background monitoring features that automate the management of file transfers between any two resources, whether they are on a supercomputing facility, a campus cluster, lab server, desktop or laptop. Globus improves transfer performance over other file transfer tools, like rsync, scp, and sftp, by automatically tuning transfer settings, restarting interrupted transfers, and checking file integrity.\nGlobus transfers data between any two so-called “Globus endpoints” or “Data Collections”. Since the data can reside across different organizations, Globus provides a way to manage and link “identities” between these organizations to facilitate data transfers and data sharing.\nUniversity of Manitoba has a Globus subscription, so its users can participate in the Globus identity federation using their UMNetIDs and University’s authentication system, protected with UM multi-factor authentication. Globus can be accessed via the main Globus website or UManitoba or via the Alliance Globus portals.\nThere are two methods of using Globus that are available for Grex users: using a Grex Server endpoint, or using their own Personal Endpoints on Grex login nodes. Both methods are described below.\nHow to use Grex Server endpoint# To use the UManitoba Grex HPC endpoint, a user will need to link its Globus \u0026ldquo;identity\u0026rdquo; to his master identity in the GlobusOnline portal. For the purpose of these instructions, we will use the UManitoba Globus portal. You could also use any other Globus identity you may have access to (for example, the Alliance / CCDB identity).\nTo access UManitoba Grex HPC endpoint, please follow these instructions:\n1. First, open UManitoba Globus portal in your browser. Use the menu under Use your existing organizational login and select University of Manitoba and click on Continue.\nUManitoba globus login interface: Globus Web App 2. The previous step will redirect you to the UM Microsoft login webpage and ask for UM Multifactor authentication. After a successful authentication, you should be able to access Globus file manager as shown in the picture below:\nGlobus file manager 3. In the field Collection, search for UManitoba Grex HPC. For the first time, it should show a message asking to link a new identity:\nNone of your authenticated identities are from domains allowed by resource policies\nSession reauthentication required (Globus Transfer)\na rprox.hpc.umanitoba.ca identity\nas shown in the following screenshot:\nNone of your authenticated identities are from domains allowed by resource policies 4. Then, click on the link Continue\n5. After clicking on the menu Continue, you will see this message:\nIdentity Required\nAn identity from one of the following identity providers is required to continue.\nReason: Session reauthentication required (Globus Transfer)\nPlease select the identity or identity provider to continue:\nLink an identity from Grex HPC Login (rprox.hpc.umanitoba.ca)\nHere is a screenshot of the message:\nAn identity from one of the following identity providers is required to continue 6. Then, click on the link ( rprox.hpc.umanitoba.ca ) from the previous step and redirect you the authentification page where you should use your Grex user name and password and your second factor authentication method you usually use to connect to Grex:\nUse MFA to login This will display a message to log into your primary identity:\nLog into your primary identity.\nIn order to link username@rprox.hpc.umanitoba.ca to your Globus account, please log into your primary identity (username @ domainname ).\nBy selecting Continue, you agree to Globus terms of service and privacy policy.\n7. Then, click on Continue to proceed. This will dispal a messsage:\nIdentity Required\nAn identity from one of the following identity providers is required to continue.\nReason: Session reauthentication required (Globus Transfer)\nPlease select the identity or identity provider to continue:\nusername@rprox.hpc.umanitoba.ca Link an identity from Grex HPC Login (rprox.hpc.umanitoba.ca)\n8. Then select username@rprox.hpc.umanitoba.ca. This will redirect you to a file manager where your data under your home directory on Grex is shown:\nUManitoba Grex HPC. Once your identity is linked, you can use Globus to initiate data transfer between Grex and any other Globus connection, like cedar for example.\nLimitations of the Grex server endpoint : At the moment of writing, the \u0026ldquo;Grex HPC\u0026rdquo; server endpoint does not support browser-based \u0026ldquo;Upload\u0026rdquo; functionality. The server endpoint also does not support data sharing and data publishing. The only use case for this endpoint is data transfer between two Endpoints/Connections.\nOpenOnDemand web portal on Grex now provides integration with the \u0026ldquo;UManitoba Grex HPC\u0026rdquo; endpoint: in the OOD\u0026rsquo;s File manager interface, the \u0026ldquo;Globus\u0026rdquo; button would redirect to the Globus WebApp on the endpoint, pointing to the current directory.\nHow to use Globus Web App to initiate transfer between two collections?# Once the identity is linked as shown in the previous section, you could launch Globus web application to initiate file transfer between what is called Collections. Here are the steps to follow:\n1. Launch Globus web app.\n2. Select the two panel file manager.\n3. Search for the collections you want to use and navigate through your directory.\n4. Select required objects and initiate the transfer with the START button.\nTo search for a Collection of interest, you\u0026rsquo;d need to know is name, or organization, or keywords to enter into the Globus FileManager app Search field.\nFor example, for Grex, the server Endpoint is called UManitoba Grex HPC and can be found by \u0026lsquo;grex\u0026rsquo; or \u0026lsquo;manitoba\u0026rsquo; keywords. Other collections hosted at UManitoba systems, including personal endpoints, will come up in the search for \u0026lsquo;manitoba\u0026rsquo; as well. A couple of collections/endpoints useful for Canadian users would be the Alliance\u0026rsquo;s National systems (such as Narval); the full list of the Alliance HPC Systems gives an idea on what words to search for. Another frequently used service that exposes data through Globus is UseGalaxy.ca . Finally, it is possible to use Globus to connect to the Alliance\u0026rsquo;s Object Storage on the Arbutus cloud .\nHow use personal endpoints and Globus CLI on Grex# It is possible to use personal endpoints on Grex login nodes instead of the Grex (limited) server endpoint. Each user can use Globus Connect Personal to transfer data between any Server Endpoint and Grex. The method above is also useful for the use cases (data transfer automation) that require using Globus Command Line Interface (CLI) rather than the WebApp.\nTo get access to the Globus CLI and to create their personal endpoint on Grex, under their account, the following instruction have to be followed.\n[~]$ module load globus # # Use an existing Globus identity to authenticate in the step below # [~]$ globus login --no-local-server Please authenticate with Globus here: ------------------------------------ https://auth.globus.org/v2/oauth2/authorize?[...] ------------------------------------ Enter the resulting Authorization Code here: [...] You have successfully logged in to the Globus CLI! You can check your primary identity with globus whoami For information on which of your identities are in session use globus session show Logout of the Globus CLI with globus logout [~]$ globus gcp create mapped \u0026lt;YOUR_NEW_ENDPOINT_NAME\u0026gt; Message: Endpoint created successfully Endpoint ID: abcdef00-1234-0000-4321-000000fedcba Setup Key: 12345678-aaaa-bbbb-cccc-87654321dddd [~]$ globusconnectpersonal -setup 12345678-aaaa-bbbb-cccc-87654321dddd [~]$ tmux new-session -d -s globus \u0026#39;globusconnectpersonal -start\u0026#39; ### You can now start a transfer by navigating to https://globus.alliancecan.ca/ ### and searching/choosing \u0026lt;YOUR_NEW_ENDPOINT_NAME\u0026gt; as the \u0026#34;Collection\u0026#34; Once the endpoint had been created and the personal Globus server started, the endpoint will be visible/searchable in the GlobusOnline Web interface. Now it can be used for data transfers. The module load globus command also provides Globus command line interface (CLI) that can also be used to move data as described here: Globus CLI examples Filesystems and symbolic links# Often, there is more than one filesystem on a Linux machine the personal endpoint is started on. For example, an endpoint on Grex login node would have $HOME and /project available for sharing. However, on Linux, Globus does no share everything by default, other than users $HOME ! Even when there exist symbolic links to /project or /scratch, they would not yet be navigable in the Globus Web UI or CLI. Symbolic links across the filesystems do not work in Globus, unless both filesystems are shared!\nTo enable sharing filesystems other than $HOME, the following special file has to be edited: ~/.globusonline/lta/config-paths By default Globus creates this file with only one line, ~/,0,1 that corresonds to user\u0026rsquo;s home directory. To add your project, or other filesystems, an extra line per filesystem must be added to the file. The example below shows a template ~/.globusonline/lta/config-paths file for Grex.\n# modify the file as needed. Each line is of the format Path,SharingFlag,RWFlag. # the SharingFlag must be 0 for non-Globus+ endpoints. cat ~/.globusonline/lta/config-paths ~/,0,1 /scratch/,0,1 /project/\u0026lt;YOUR_PROJECT_ID\u0026gt;/,0,1 Note that you would replace \u0026lt;YOUR_PROJECT_ID\u0026gt; above with your real path to the /project filesystem. One way to get it on Grex is to examine the output of the diskusage_report script. Another, more general way, is to use realpath command to resolve the project symlink, as in realpath /home/${USER}/projects/def-\u0026lt;YOUR_PI\u0026gt;/ .\nMore information about configuring the Paths is available at Globus Documentation on Linux Endpoints .\nManaging personal endpoints# It is a good practice to not to keep unnecessary processes running on Grex login nodes. Thus, when all data transfers are finished, user should stop their Globus server process running personal endpoint as follows: [~]$ tmux kill-session -C -t globus\nOnce an endpoint had been created, there is (usually) no need to repeat the above steps creating a new endpoint. To restart the same existing endpoint, as needed for new data transfer sessions, it will be enough to run: [~]$ tmux new-session -d -s globus \u0026#39;globusconnectpersonal -start\u0026#39;\nA more general guide on how to use Globus personal endpoint on a Linux system, can be found on the Frontenac \u0026ldquo;Data Transfers\u0026rdquo; page . This guide does not require a globus software module present on the system.\nUsing Upload button and HTTP access# Globus v.5 adds a new feature to server endpoints that allows for using HTTP protocol (instead of the traditional globus-url-copy) . This feature can be used from the Web UI (Upload button), and from CLI for accessing Globus using the HTTPS URL someone had shared with you.\nNote that the Upload feature is not available for Personal Globus endpoints. It only can be used for properly configured Server endpoints.\nExternal links# Globus official Documentation Globus CLI Documemtation DRAC Globus Wiki page Instructions on using Globus for UseGalaxy Canada Check out the ESNet website if you are curious about Globus, and why large data transfers over WAN might need specialized networks and software setups. "},{"id":"49","rootTitleIndex":"5","rootTitle":"Transferring data","rootTitleIcon":"fa-solid fa-arrow-right-arrow-left fa-lg","rootTitlePath":"/grex-docs/data-transfer/","rootTitleTitle":"Homepage / Transferring data","permalink":"/grex-docs/data-transfer/one-drive/","permalinkTitle":"Homepage / Transferring data / Connecting to OneDrive using Rclone","title":"Connecting to OneDrive using Rclone","content":"UM OneDrive# The University of Manitoba provides Microsoft OneDrive to its users as part of the Microsoft Office 365 contract. While not suited, for performance reasons, for online computation work or for storing large datasets, OneDrive can be a useful storage space to store important data of smaller size, documents, articles and such. It also offers some data preservation features like data recovery and data history.\nPlease also refer to UM Data Security Classification to learn which kinds of data can be stored where.\nrclone is a universal tool for uploading and downloading data to and from a wide variety of cloud storage systems. In particular, it is the tool for transferring data between a Linux HPC system and the Microsoft OneDrive.\nHowever, due to the nature of Web-storage like OneDrive, needs for MFA, etc. a configuration step is required to make rclone on Grex connect to the OneDrive. The configuration step requires access to a Web browser.\nConfiguring Rclone for OneDrive on Grex# Please login to Grex\u0026rsquo;s OpenOnDemand portal and request a \u0026ldquo;Simplified Desktop\u0026rdquo; session. The desktop session is required to do the Web-based part of OneDrive authentication; however, main configuration is done in the command line because rclone is a command line tool.\nWhen the Desktop session is ready, connect to it in OOD\u0026rsquo;s Desktop tab, and open a terminal. Run the following commands:\nmodule spider rclone module load rclone rclone config The last command shows a prompt and will ask a large number of interactive questions. Answer them as follows:\nQ: \u0026ldquo;No remotes found \u0026ndash; make a new one\u0026rdquo; answer \u0026ldquo;n\u0026rdquo; for a new remote Q: \u0026ldquo;name\u0026gt;\u0026rdquo; call it \u0026ldquo;OneDrive\u0026rdquo; Q: \u0026ldquo;Storage\u0026gt;\u0026rdquo; From the numbered list of storage, pick the number corresponding to \u0026ldquo;Microsoft OneDrive\u0026rdquo;. The numbers change version to version. As of now, \u0026lsquo;31\u0026rsquo;. Q: \u0026ldquo;client_id\u0026gt;\u0026rdquo; leave blank (press Enter) Q: \u0026ldquo;client_secret\u0026gt;\u0026rdquo; leave blank (press Enter) Q: \u0026ldquo;Edit advanced config?\u0026rdquo; answer \u0026ldquo;n\u0026rdquo; for No Q: \u0026ldquo;Use auto config?\u0026rdquo; answer \u0026ldquo;y\u0026rdquo; for Yes at this point, a browser window will pop up. Not every browser works, so we recommend Firefox (firefox command in the terminal) to be used as the default browser. Please inspect the browser window and URL to see if it looks like an authentic UManitoba login page! Authenticate using your UManitoba email and UManitoba credentials and MFA. Go back to your terminal when \u0026ldquo;Success\u0026rdquo; is displayed. Q: \u0026ldquo;Your Choice\u0026gt;\u0026rdquo; type a number to use your business OneDrive (as of now, \u0026lsquo;1\u0026rsquo;) Q: \u0026ldquo;Choose drive to use\u0026gt;\u0026rdquo; type \u0026lsquo;0\u0026rsquo; Q: \u0026ldquo;Is this ok y/n\u0026gt;\u0026rdquo; answer \u0026ldquo;y\u0026rdquo; to confirm the drive selection Q: \u0026ldquo;y/e/d\u0026gt;\u0026rdquo; answer \u0026ldquo;y\u0026rdquo; to confirm adding the remote to rclone If the configuration was successful, at this point an OneDrive access token has been created under your home directory, valid for a number of days (90 days). You can start using command line rclone tools from any SSH session on Grex. When the token expires, it can be regenerated with rclone config reconnect remote: command.\nUsing Rclone for data transfers# The command line rclone tool should be available, so as usual for the Grex software , its module should be loaded. Then rclone ls command can be used to explore your OneDrive, and rclone copy to copy the files to and from OneDrive as per the example below:\nmodule load rclone rclone ls OneDrive:/ rclone copy myfile.txt OneDrive:/testdir rclone ls OneDrive:/testdir rclone copy OneDrive:/testdir/myfile.txt myfile.bak Note that MS OneDrive follows Windows rather than Linux file naming conventions, thus filenames are case-insensitive, may treat special characters in the names differently, etc. Please refer to rclone Documentation for more information. Rclone is an universal data transfer tool that can be used for a variety of storages, and OneDrive is just one application for it.\nUsing Rclone in OOD# On Grex, OpenOnDemand supports Rclone connection within the File App automatically. The Rclone folder will appear for the user after he performs the above rclone config step. As with the command line access, the access token has to be periodically refreshed.\nUsing Rclone for other \u0026ldquo;remotes\u0026rdquo; like NextCloud# Actually, many remotes besides MS OneDrive would work with rclone and OpenOnDemand.\nThe following example of setting up the Alliance\u0026rsquo;s NextCloud remote with rclone Rclone and NextClod on Alliance would also work. After setting up the NextCLoud remote as described on the documentation file, the remote storage will appear in the OOD Files App.\nExternal links# OSC Documentation on using Rclone with OneDrive we are indebted to UManitoba IST OneDrive pages "},{"id":"50","rootTitleIndex":"5","rootTitle":"Transferring data","rootTitleIcon":"fa-solid fa-arrow-right-arrow-left fa-lg","rootTitlePath":"/grex-docs/data-transfer/","rootTitleTitle":"Homepage / Transferring data","permalink":"/grex-docs/data-transfer/object-storage/","permalinkTitle":"Homepage / Transferring data / Data transfers for OpenStack Object Storage","title":"Data transfers for OpenStack Object Storage","content":"What is Object Storage# Object storage is a storage technology that allows large amounts of data to be stored as “objects.” This differs from a traditional POSIX-compliant filesystem, which organizes data as randomly accessible “files” and “directories” on block-based storage. Object storage enables access to entire objects only (for loading, storing, or retrieving). Each object contains the data itself (e.g., a photo, video, or document), metadata that describes the object, and a unique identifier used to retrieve it. Access to the content inside an object is typically handled by client software.\nObject storage often offers a better cost/performance ratio than traditional HPC storage systems. However, it is entirely software-driven, and many traditional HPC applications are not compatible with Object Storage out of the box.\nA very common use case for Object Storage is simply storing large volumes of unstructured data. Another is serving web content through an API-based frontend. In particular, Object Storage can act as a basic Content Delivery Network (CDN): for example, if you store static website content (such as images, HTML pages, and JavaScript files), it can be an accessed via a web browser as a regular website. A practical example of this is the PointCloud described below.\nArbutus (and other) National DRI Object Storage instances# There is no Object Storage provided on Grex. However, the National DRI (Alliance) provides the Arbutus OpenStack Cloud, which is available on request to users with an active CCDB account. As part of the Arbutus service offering, Object Storage is available and can be requested—up to 10 TB—via the regular RAS process .\nOnce approved, you can create one or more “buckets” within your Object Storage allocation. These buckets can then be accessed using the S3 protocol.\nPlease refer to UM Data Security Classification to determine what types of data are appropriate for storage on this service. Note that Arbutus Object Storage is provided by the Alliance and the University of Victoria, and is not hosted by the University of Manitoba’s IT services. Also note that there is a single namespace across the entire system, meaning bucket names must be unique globally. Once a bucket is created, and if configured as “public” in its settings, it can be accessed via a URL: http://object-arbutus.cloud.computecanada.ca/BUCKET_NAME where BUCKET_NAME is the name of the bucket. Alternatively, buckets can be kept private, in which case a pair of access and secret keys will be required to authenticate, as described below.\nAuthentication into the OpenStack Objects Storage# Authentication is done using a Bash shell environment and the OpenStack Python client package. The easiest way to do this is from a Linux command-line environment on a trusted machine, where your credentials won’t be exposed to other users. One suitable environment is Grex.\nTwo components are required for authentication:\nAn OpenStack RC file, which can be downloaded from your Arbutus Horizon Dashboard. The OpenStack client Python package. Open the Arbutus cloud dasboard at Arbutus Cloud Link . You can navigate it to your project\u0026rsquo;s Object Storage at screenshot below, and, when your Resource Allocation Allows, create buckets there, change their permissions etc..\nNavigate to the top right corner of the OpenStack Dashboard, click on your account name ( username on the screenshot above) and use the item in the dropdown menue to Download the RC file. The RC file is a shell script; you can then copy it to Grex, or, alternatively, copy and paste its contents into a text editor on Grex and save it.\nNote: If your user account belongs to multiple projects, each project will have its own RC file, even though your dashboard login remains the same. Be sure to select the appropriate project (project-name on the screenshot above) before downloading the RC file. First, load a suitable Python module (if required on your system), and create a virtual environment:\npython -m venv OS source OS/bin/activate pip install python-openstackclient # Need to add it to this session\u0026#39;s PATH to have access to the openstack command export PATH=$PATH:`pwd`/OS/bin On Grex, openstack-client is provided as a module. So the above Python/venv steps can be replaced with convenient module load openstack-client/8.2.0 . On other HPC systems, installation of the openstack-python in a virtual environment is necessary, if the openstack command is not available from the system. Once the module is loaded or the client is installed, source the OpenStack RC file to load your credentials into the environment. It will ask for your password. Then, generate EC2-compatible credentials using the following openstack command:\n# Load your projects\u0026#39; credentials using your CCDB account/password source openstack.rc # Enter password when asked! # Create the buckets access credentials openstack ec2 credentials create The resulting output will contain various hashes (long strings of numbers and letters). There should be an \u0026ldquo;access\u0026rdquo; hash, which is your access key, and a \u0026ldquo;secret\u0026rdquo; hash, which is the password to it. This pair of credentials, \u0026ldquo;access\u0026rdquo;/\u0026ldquo;secret\u0026rdquo; is used to access OpenSTack Object Storage with any kind of client software: WinSCP, Rclone, GlobusOnline, etc. The output will contain a set of long alphanumeric strings:\nThe \u0026ldquo;Access\u0026rdquo; key (labeled access) is your access identifier. The \u0026ldquo;Secret\u0026rdquo; key (labeled secret) is your authentication password. This access/secret key pair is used to authenticate with OpenStack Object Storage using any compatible client software, such as WinSCP, Rclone, or Globus, among others.\nNote that the access/secret pair has to be treated as user\u0026rsquo;s password! Keep them in a secure place and do not share. Accessing Arbutus OpenStorage using Globus# ObjectStorage is universal and can be accessed, in general, by many tools like s3cmd or rclone or WinSCP, or by using a Python library like requests. Globus provides a fast and convenient service to organize large data transfers over WAN, and that includes Object Storage endpoints. However, this requires the storage endpoint to purchase and install a \u0026ldquo;Globus connector\u0026rdquo; and be registered in the Globus Federation.\nFortunately, Arbutus provides such a Globus endpoint, which can be accessed using your CCDB (Alliance) credentials. The official documentation is available here .\nTo access Object Storage buckets via Globus Online, you will need:\nA Globus account that is linked to your CCDB account (Grex and Alliance users typically already meet this requirement). Access credentials for a specific Object Storage bucket on Arbutus, created as per above. The following steps can be followed to set it up:\nLog in to Grex and ensure you have your OpenStack RC file ready. Follow the earlier steps to create your access/secret key pair using sourcing OpenStack RC file and running openstack ec2 credential creation. Then,\nLog in to the Globus Online portal using your CCDB credentials (Arbutus access is via CCDB). Search for the collection named \u0026ldquo;Arbutus S3 buckets\u0026rdquo;. When prompted, consent to link your accounts and allow Globus to access metadata. Globus will request this consent once for each new endpoint or collection. When prompted, enter your Access Key and Secret Key. This links your bucket credentials to the Arbutus endpoint. After successful authentication into Arbutus Endpoint and the buckets, your Object Storage buckets should appear in the Globus File Manager interface. You can now transfer files using Globus , between Arbutus buckets and HPC machines , be that National Alliance systems or Grex, as you would with any other storage collection.\nAccessing ObjectStorage using Rclone# Rclone is a versatile command-line tool for uploading and downloading data to and from a wide variety of cloud storage systems. It is especially useful for transferring data between a Linux HPC system (such as Grex) and Object Storage. Any S3-compatible storage endpoint would work, as well as other endpoints like UManitoba\u0026rsquo;s OneDrive .\nBefore you can use rclone, a one-time configuration step is required to provide it with your Object Storage credentials. This can be done entirely from the command line—no web browser access is needed. You may also configure it manually by editing rclone\u0026rsquo;s configuration file.\nIn the same SSH session on Grex, type \u0026ldquo;deactivate\u0026rdquo; to leave the virtualenv. Then, run the following commands:\nmodule load rclone rclone config On Grex, rclone is provided as a module. On other systems, rclone might be available from the system. If not available, you can install it locally. This last command will launch an interactive prompt that asks a series of configuration questions. This is where you will provide the access/secret credentials pair to the tool! Answer them as follows\nQ: \u0026ldquo;No remotes found \u0026ndash; make a new one\u0026rdquo; Answer \u0026ldquo;n\u0026rdquo; for a new remote Q: \u0026ldquo;name\u0026gt;\u0026rdquo; Call it \u0026ldquo;ObjectStorageMy\u0026rdquo; Q: \u0026ldquo;Storage\u0026gt;\u0026rdquo; From the numbered list of storage, pick the number corresponding to \u0026ldquo;Amazon S3 Compliant Storage Providers\u0026rdquo;. The numbers change version to version. As of now, \u0026lsquo;4\u0026rsquo;. Q: \u0026ldquo;Option provider. Choose your S3 provider\u0026gt;\u0026rdquo; Any other S3 compatible provider. As of now, 34. Q: \u0026ldquo;Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars)\u0026rdquo; Enter \u0026ldquo;false\u0026rdquo; to Enter AWS credentials in the next step. Q: \u0026ldquo;Option access_key_id. AWS Access Key ID.\u0026rdquo; Enter your \u0026ldquo;access\u0026rdquo; key from the openstack config ! Q: \u0026ldquo;Option secret_access_key.AWS Secret Access Key (password). \u0026quot; Enter your \u0026ldquo;secret\u0026rdquo; key from the openstack config ! Q: \u0026ldquo;Option region. Region to connect to.\u0026rdquo; Leave blank, press Enter. There is no region. Q: \u0026ldquo;Option endpoint. Endpoint for S3 API.Required when using an S3 clone\u0026rdquo; Enter \u0026ldquo;https://object-arbutus.cloud.computecanada.ca \u0026rdquo; for Arbutus Openstack here. Q: \u0026ldquo;Option location_constraint. Location constraint - must be set to match the Region.\u0026rdquo; Leave blank, press Enter. Q: \u0026ldquo;Option acl.Canned ACL used when creating buckets and storing or copying objects.\u0026rdquo; Leave blank, press Enter for the default (private) Q: \u0026ldquo;Edit advanced config?\u0026rdquo; Answer \u0026ldquo;n\u0026rdquo; for No Q: \u0026ldquo;Configuration complete. Keep this \u0026ldquo;ObjectStorageMy\u0026rdquo; remote?\u0026rdquo; Answer \u0026ldquo;y\u0026rdquo; for Yes Q: \u0026ldquo;Current remotes:\u0026rdquo; Answer \u0026ldquo;q\u0026rdquo; to quit if it lists all the remotes correctly. If the configuration was successful, you should now have an rclone \u0026ldquo;remote\u0026rdquo; destination named ObjectStorageMy. The access credentials have been stored under your home directory, and you can begin using rclone from any SSH session on Grex. To test the connection, run the following command (note the colon at the end — rclone follows a Microsoft-like naming convention for remotes, such as C: for a disk):\nrclone lsd ObjectStorageMy: This command would connect to the remote, and list the buckets available on that remote, if any exist.\nUsing Rclone for Data Transfers to the ObjectStorage# As with other Grex software , rclone\u0026rsquo;s module must be loaded before use. Once loaded, you can use commands like rclone ls to explore your configured “remote” endpoint directories, and rclone copy to transfer files to and from Object Storage, as shown in the example below:\nmodule load rclone # Working with remote endpoints. rclone list remotes rclone lsd ObjectStorageMy: # Working with files and directories. rclone ls ObjectStorageMy:/ rclone copy myfile.txt ObjectStorageMy:/mybucket/ rclone ls ObjectStorageMy:/mybucket rclone copy ObjectStorageMy:/mybucket/myfile.txt myfile.bak Please refer to rclone documentation for more information and options. Rclone is an universal data transfer tool that can be used for a variety of storages. You can also refer to our OneDrive and Nextcloud documentation for using Rclone with those services.\nUsing Rclone in OpenOnDemand# On Grex, OpenOnDemand supports Rclone connections automatically within the Files app. After completing the rclone config setup as described above, the ObjectStorageMy folder will appear as a mounted remote in the OOD interface for the user. The Rclone folder ObjectStorageMy will appear for the user after he performs the above rclone config step.\nExample: Uploading PointCloud data from Grex to Arbutus# TBD\nExternal links# Arbutus object storage Globus Rclone "},{"id":"51","rootTitleIndex":"6","rootTitle":"Storage and Data","rootTitleIcon":"fa-solid fa-database fa-lg","rootTitlePath":"/grex-docs/storage/","rootTitleTitle":"Homepage / Storage and Data","permalink":"/grex-docs/storage/","permalinkTitle":"Homepage / Storage and Data","title":"Storage and Data","content":"General Storage Information# As of now, the storage system of Grex consists of the following:\nThe /home NFSv4/RDMA filesystem is served by a very fast NVME disk server. The total size of the filesystem is 15 TB. The quota per-user is 100 GB of space and 500K files. This quota can not be increased. For larger amount of data, users should use project. The /project Lustre filesystem: as of Sep 2022, an additional storage of 1 PB was added to Grex. On Jan 2024, the /project was extended by another 1 PB. File system Type Total space Quota/User Number of files /home NFSv4/RDMA 15 TB 100 GB 500 K /project Lustre 2 PB - - The local node storage as defined by the environment variable $TMPDIR is recommended for temporary job data that is not needed after job completes. Grex nodes have SATA local disks of various capacities, leaving 323 Gb, 770 Gb, \u0026hellip; of usable space per node, depending on the kind of local disk it has. Most users would want to use /home/USERNAME for code development, source code, scripts, visualization, processed data, \u0026hellip; etc., that do not take much space and benefits for small files I/O. For production data processing, that is, massive I/O tasks from many compute or interactive jobs, the /project FS should be used. It is often beneficial to place temporary files on the local disk space of the compute nodes, if space permits, so that the jobs do not load Lustre or NFS servers extensively.\nData retention and Backup# Data retention policy as of now conforms to the corresponding of the Alliance (former Compute Canada) policy. While a user\u0026rsquo;s CCDB account is active and renewed, there will be no purges on /home nor /project filesystems. Should the CCDB account expire, the data will not be kept on Grex indefinitely! The data retention period for expired accounts is one (1) year. Data on login and compute nodes\u0026rsquo; local scratch disks gets purged regularly and automatically.\nWhen a group member or a PI leaves the group or this University, it is very important to arrange data transfer to whoever would own and continue the project before the account expiration of the leaving group member. A local tape backup is provided for /home and /project filesystems. Please refer to the Data Backup page for more information.\nData sharing# Sharing of accounts login information (like passwords or SSH keys) is strictly forbidden on Grex, as well as on most of the HPC systems. There is a mechanism of data/file sharing that does not require sharing of the accounts. To access each other\u0026rsquo;s data on Grex, the UNIX groups and permissions mechanism can be used. For more information, please refer to the section Sharing Data .\nDisclaimer of any responsibility for Data loss# Every effort is made to design and maintain Grex storage systems in a way that they are a reliable storage for researcher\u0026rsquo;s data. However, we (Grex Team, or the University) make no guarantees that any data can be recovered, regardless of where they are stored, even in backed up volumes. Accidents happen, whether caused by human error, hardware or software errors, natural disasters, or any other reason. It is the user\u0026rsquo;s responsibility that the data is protected against possible risks, as appropriate to the value of the data.\nInternal links# Data sizes and Quotas Data Backup Data sharing "},{"id":"52","rootTitleIndex":"7","rootTitle":"Running jobs on Grex","rootTitleIcon":"fa-solid fa-person-running fa-lg","rootTitlePath":"/grex-docs/running-jobs/","rootTitleTitle":"Homepage / Running jobs on Grex","permalink":"/grex-docs/running-jobs/","permalinkTitle":"Homepage / Running jobs on Grex","title":"Running jobs on Grex","content":"Why running jobs in batch mode?# There are many reasons for adopting a batch mode for running jobs on a cluster. From providing user\u0026rsquo;s computations with fairness, traffic control to prevent resource congestion and wasting, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the SLURM batch system since December 2019 with the Linux/SLURM update project .\nAccounting groups# Users belong to \u0026ldquo;accounting groups\u0026rdquo;, led by their Principal Investigators (PIs). The accounting groups on Grex match CCDB roles. By default, a user\u0026rsquo;s jobs are assigned to his primary/default accounting group. But it is possible for a user to belong to more than one accounting group; then the SLURM directive --account= can be used for sbatch or salloc to select a non-default account to run jobs under.\nFor example, a user who belongs to two accounting groups: def-sponsor1 and def-sponsor2 (sponsored by two PIs), can specify which one to use:\n#SBATCH --account=def-sponsor1 or\n#SBATCH --account=def-sponsor2 NEW : Since Jun 19, 2024, for users that have more than one Account (that is, working for more than one research group), SLURM on Grex will no longer try to assume which of the accounts is default. Instead, sbatch and salloc would ask to provide the –-account= opton explicitly, list the possible accounts, and stop. Thus, for users that are members of more than one group, specifying the account as per above is now mandatory!\nQOSs# QOS stands for Quality of Service. It is a mechanism to modify scheduler\u0026rsquo;s limits, hardware access policies and modify job priorities and job accounting/billing. Presently, QOS might be used by our Scheduler machinery internally, but not specified by the users. Jobs that specify explicit --qos= will be rejected by the SLURM job submission wrapper.\nLimits and policies# In order to prevent monopolization of the entire cluster by a single user or single accounting group, we enforce a MAXPS like limit; for non-RAC accounts it is set to 4 M CPU-minutes and 400 CPU cores. Accounts that have allocation (RAC) on Grex get higher MAXPS and max CPU cores limits in order to let them utilize their usage targets.\nTo see these limits, one could run the command:\nsacctmgr show assoc where user=$USER or with a specific format for the output:\nexport SACCTMGR_FORMAT=\u0026#34;cluster%9,user%12,account%20,share%5,qos%24,maxjobs%15,grptres%12,grptresrunmin%20\u0026#34; sacctmgr show assoc where user=$USER format=$SACCTMGR_FORMAT Partitions for preemptible jobs running on contributed hardware might be further limited, so that they cannot occupy the whole contributed hardware.\nIn cases when Grex is underutilized, but some jobs exist in the queue that can be run if not for the above-mentioned limits, we might relax the limits as a temporary \u0026ldquo;bonus\u0026rdquo;.\nSLURM commands# Naturally, SLURM provides a command line user interface. Some of the most useful commands are listed below.\nExploring the system# The SLURM command that shows the state of nodes and partitions is sinfo:\nExamples:\nsinfo to list the state of all the nodes (idle, down, allocated, mixed) and partitions. sinfo --state=idle to list all idle nodes. sinfo -p skylake to list information about a given partition (skylake in this case). sinfo -p skylake \u0026ndash;state=idle to list idle nodes on a given partition (skylake in this case). sinfo -R: to list all down nodes. sinfo -R -N -o\u0026quot;%.12N %15T [ %50E ]\u0026quot;|uniq -c: to list all down nodes and print the output in a specific format. sinfo -s \u0026ndash;format=\u0026quot;# %20P %12A %.12l %.11L %.6a %.22C %.10m\u0026quot; to show all the partitions and their characteristics. Submitting jobs# Batch jobs are submitted as follow:\nsbatch [options] myfile.job Interactive jobs are submitted in exactly same way, but they do not need the job script because they will give you an interactive session:\nsalloc [options] For more information about the usage of sbatch and salloc commands, please visit the dedicated sections: interactive and batch jobs.\nThe command sbatch returns a number called JobID and used by SLURM to identify the job in the queuing system.\nThe options are used to specify resources (wall time, tractable resources such as cores and nodes and GPUs) and accounts and QOS and partitions under which the jobs should run, and various other options like specifying whether jobs need X11 GUI (--x11), where its output should go, whether email should be sent when job changes its states and so on. Here is a list of the most frequent options to sbatch command:\nResources (tractable resources in SLURM speak) are CPU time, memory, and GPU time.\nGeneric resources can be software licenses, etc. There are also options to control job placement such as partitions and QOSs.\n--ntasks= (specifies number of tasks (MPI processes) per job) --nodes= (specifies number of nodes (servers) per job) --ntasks-per-node= (specifies number of tasks (MPI processes) per node) --cpus-per-task= (specifies number of threads per task) --mem-per-cpu= (specifies memory per task (or thread?)) --mem= (specifies the memory per node) --gpus= (specifies number of GPUs per job. There are also --gpus-per-XXX and --XXX-per-gpu) --time- (specifies wall time in format DD-HH:MM) --qos= (specifies a QOS by its name (Should not be used on Grex!)) --partition= (specifies a partition by its name (Can be very useful on Grex!)) An example of using some of these options with sbatch and salloc are listed below:\nsbatch --nodes=1 --ntasks-per-node=1 --cpus-per-task=12 --mem=40G --time=0-48:00 gaussian.job and\nsalloc --nodes=1 --ntasks-per-node=4 --mem-per-cpu=4000M --x11 --partition=skylake And so on. The options for batch jobs can be either in command line, or (perhaps better) in the special comments in the job file, like:\n#SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=12 #SBATCH --mem=40G #SBATCH --time=0-48:00 #SBATCH --partition=skylake Refer to the subsection for batch jobs and interactive jobs for more information, examples of job scripts and how to actually submit jobs.\nMonitoring jobs# Checking on the queued jobs:\nsqueue -u someuser (to see all queued jobs of the user someuser) squeue -u someuser -t R (to see all queued and running jobs of the user someuser) squeue -u someuser -t PD (to see all queued and pending jobs of the user someuser) squeue -A def-sponsor1 (to see all queued jobs for the accounting group def-sponsor1) Without the above parameters, squeue would return all the jobs in the system. There is a shortcut sq for squeue -u $USER\nCanceling jobs:\nscancel JobID (to cancel a job JobID) echo \u0026ldquo;Deleting all the jobs by $USER\u0026rdquo; \u0026amp;\u0026amp; scancel -u $USER (to cancel all your queued jobs at once). echo \u0026ldquo;Deleting all the pending jobs by $USER\u0026rdquo; \u0026amp;\u0026amp; scancel -u $USER \u0026ndash;state=pending (to cancel all your pending jobs at once). Hold and release queued jobs:\nTo put hold some one or more jobs, use:\nscontrol hold JobID (to put on hold the job JobID). scontrol hold JobID01,JobID02,JobID03 (to put on hold the jobs JobID01,JobID02,JobID03). To release them, use:\nscontrol release JobID (to release the job JobID). scontrol release JobID01,JobID02,JobID03 (to release the jobs JobID01,JobID02,JobID03). Checking job efficiency:\nThe command seff is a wrapper around the command sacct that gives a friendly output, like the actual utilization of walltime and memory:\nseff JobID seff -d JobID Note that the output from the seff command is not accurate if the job was not successful.\nChecking resource limits and usage for past and current jobs:\nsacct -j {JobID} -l sacct -u $USER -s {STARTDATE} -e {ENDDATE} -l --parsable2 Getting info on accounts and priorities# Fairshare and accounting information for the accounting group def-someprofessor:\nsshare -l -A def-someprofessor sshare -l -A def-someprofessor --format=\u0026#34;Account,EffectvUsage,LevelFS\u0026#34; sshare -a -l -A def-someprofessor sshare -a -l -A def-someprofessor --format=\u0026#34;Account,User,EffectvUsage,LevelFS\u0026#34; Fairshare and accounting information for a user:\nsshare -l -U --user $USER sshare -l -U --user $USER --format=\u0026#34;Cluster,User,EffectvUsage,FairShare,LevelFS\u0026#34; Limits and settings for an account:\nsacctmgr list assoc account=def-someprofessor format=account,user,qos Internal links# Slurm partitions Interactive jobs Batch jobs Contributed nodes Using local disks External links# SLURM documentation Running jobs on the Alliance (Compute Canada) clusters. References for migrating from PBS to SLURM: ICHEC , HPC-USC Westgrid training materials on SLURM: Scheduling Since the HPC technology is widely used by most universities and National labs, simply googling your SLURM question will likely return a few useful links to their HPC/ARC documentation. Please remember to adapt your script according to the available hardware and software as some SLURM directives and modules are specific to a given cluster. "},{"id":"53","rootTitleIndex":"8","rootTitle":"Software and Applications","rootTitleIcon":"fa-solid fa-terminal fa-lg","rootTitlePath":"/grex-docs/software/","rootTitleTitle":"Homepage / Software and Applications","permalink":"/grex-docs/software/","permalinkTitle":"Homepage / Software and Applications","title":"Software and Applications","content":"Software# In HPC, \u0026ldquo;software\u0026rdquo; typically refers to scientific and engineering codes for computation, data processing, and visualization -— rather than common business IT applications like web services or email clients. Without software to run workloads, HPC systems would be of little use. Development tools and libraries used to build HPC applications are also considered to be \u0026ldquo;software\u0026rdquo;.\nFortunately, most HPC systems, including Grex, come with a curated set of pre-installed software. This section explains how to find, access, and manage HPC software, and what to do if some software item you need is missing.\nHow software is installed and distributed# Linux software is often installed system-wide using package managers (e.g., apt on Debian and Ubuntu, yum or dnf on RedHat and its derivatives). These tools install binaries into standard locations like /usr/bin, making them immediately available via the system PATH (PATH is a variable specifying where the operating systems would look for executable code).\nWhile convenient on personal machines, this approach doesn’t scale well to HPC environments due to:\nThe need for users to have privileged (root) access, which can compromise stability and security of shared HPC systems. Package managers typically support only a single, newest version of each package at a time, which hinders reproducibility. Software must be consistently installed across all compute nodes. Public binary packages are usually compiled for generic CPUs, not optimized for a specific HPC systems hardware. Because of these limitations, HPC clusters usually do not rely on OS software repositories. They only include a minimal base OS install. Users don’t have access to system package managers. Instead, application software is built from source and installed on a shared filesystem accessible by all nodes. Multiple versions can coexist, and software dependencies are managed with Environment Modules .\nModules allow users to dynamically modify their environment (e.g., PATH, LD_LIBRARY_PATH, CPATH, MKLROOT, HDF5HOME) to load or unload specific software packages.\nLmod configuration on Grex# Grex uses Lmod , a modern, Lua-based module system developed at Texas Advanced Computing Center (TACC). Lmod supports hierarchical modules, which organize software based on dependencies like CPU architecture, compilers, and MPI implementations. Using module hierarchies helps to avoid conflicts between incompatible software toolchains.\nLmod would:\nHide software items for which their dependencies/toolchains are not yet loaded. Automatically unload conflicting modules when switching toolchains. Reload required dependencies as needed when a module is switched. Supports multiple software stacks through configurable module paths. For more information about Lmod, please refer to the software stacks available on the using modules page.\nThe following shows Grex-specific conventions on how Grex software environment is configured:\nThere is a module hierarchy. No software modules are loaded by default! There is more than one \u0026ldquo;software\u0026rdquo; stacks. A special module \u0026ldquo;environment\u0026rdquo; needs to be loaded first to switch between them. CPU architecture (a module called arch ) is often a root of a module hierarchy. Most commonly used arch/avx512 . For the local software stack, cuda module precees the arch module for GPU software. On Grex, there are two software stacks, called SBEnv and CCEnv, standing for the software built on Grex locally and the software environment from the Alliance (Compute Canada), correspondingly. In practice, all the above means that a required \u0026ldquo;software stack\u0026rdquo; module must be loaded first.\nIn the case of CCEnv, it must be followed by a StdEnv module to pick the year/version of it. This would load a number of default modules like compilers and OpenMPI.\nFor SBEnv , the local environment is the only module loaded by default. This enables the \u0026ldquo;Core\u0026rdquo; modules that do not depend on CPU architecture. Usually these are commercial and/or binary packages.\nThen, a module that picks architecture should be loaded : either cuda and arch (for GPU-based software) or just arch (for CPU-only software).\nNote that the order of loading cuda and arch modules on SBEnv matters! cuda must always be loaded first.\nIn case of CCEnv, it must be followed by loading a StdEnv module to pick the year/version of it. This would load a number of default modules like compilers and OpenMPI. It still is a good practice to pick an arch module correctly for CCEnv.\nNote that cuda modules of CCEnv won\u0026rsquo;t load on a non-GPU hardware. Use interactive jobs on GPU nodes to build cuda enabled software!\nHow to find the software with Lmod Modules# First, a software stack must be selected and loaded with module load command.\nLmod can not find modules across different software stacks!\nWhen a software stack module is loaded, the module spider command will find a specific software item (for example, GROMACS; note that all the module names are lower-case on Grex and on Alliance software stacks) if it exists under that stack:\nmodule load SBEnv module spider gromacs It might return several versions; then usually a subsequent command with the version is used to determine dependencies required for the software. In case of GROMACS on Grex, at the time of writing, it returns four versions, with gromacs/2024.1 being the latest. Let\u0026rsquo;s find its dependencies.\nmodule spider gromacs/2024.1 It will advise that there are two sets of dependencies, one for the CPU version and one for the GPU version. For the CPU version, we\u0026rsquo;d need to load the following modules: \u0026quot; arch/avx512 gcc/13.2.0 openmpi/4.1.6\u0026quot;. Note that the first module is for CPU architecture. Most CPU nodes on Grex use arch/avx512. Then, after the dependencies are loaded module load command can be used actually to load the GROMACS environment, for the CPU version.\nmodule load arch/avx512 gcc/13.2.0 openmpi/4.1.6 module load gromacs/2024.1 For more information about using Lmod modules, please refer to the Modules pages on this documentation.\nHow and when to install software in your HOME directory# Linux (Unlike some Desktop operating systems) has a concept of user permissions separation. Regular users cannot, unless explicitly permitted, access the system\u0026rsquo;s files and files of other users.\nYou can almost always install software without super-user access into your /home/$USER directory. Moreover, you can manage the software with Lmod: Lmod automatically searches for module files under $HOME/modulefiles and adds the modules it discovers there into the modules tree so they can be found by module spider, loaded by module load, etc.\nMost Linux software can be installed from sources using either Autoconf or CMake configuration tools. These will accept --prefix=/home/$USER/my-software/version or -DCMAKE_INSTALL_PREFIX=/home/$USER/my-software/version as arguments. These paths are used as installation directories where the user has full access.\nSoftware that comes as a binary archive to be unpacked can be simply unpacked into your home directory location. Then, the paths should be set for the software to be found: either by including the environment variable in $HOME/.bashrc or in $HOME/.bash_profile or by creating a specific module in $HOME/modulefiles/my-software/version following Lmod instructions for writing Modules .\nThere exist binary software environments like conda that manage their own tree of binary-everything. These can be used from your home directory as well, with some caution, because automatically pulling every software from a conda channel might conflict with the same software existing in the HPC environment (Python package paths, MPI libraries, etc.).\nNote that as of 2024, Anaconda owners have changed their licensing policy. We do not provide any system-wide conda installations on Grex. In case users want to continue using conda, they must be sure that they have a proper Anaconda license to do so. Note also that the same applies for mamba which would use the same conda software channels. However, if a software is really a part of the base OS (something like a graphics Desktop software, etc.), it can be hard to rebuild from sources due to many dependencies. If needed, it may be better if installed centrally or used in a container, see Containers documentation .\nInternal links# Linux tools Using modules Code development Containers CVMFS List of Software External links# Lmod Tcl Modules CMake Autoconf "},{"id":"54","rootTitleIndex":"9","rootTitle":"Software Specific Notes","rootTitleIcon":"fa-solid fa-glasses fa-lg","rootTitlePath":"/grex-docs/specific-soft/","rootTitleTitle":"Homepage / Software Specific Notes","permalink":"/grex-docs/specific-soft/","permalinkTitle":"Homepage / Software Specific Notes","title":"Software Specific Notes","content":"Software specific notes# This page refers to the usage of some specific programs installed on Grex, like ORCA, VASP, \u0026hellip; etc.\nSoftware / Applications# Espresso Gaussian GROMACS Julia Jupyter notebooks LAMMPS MATLAB NWChem ORCA R VASP Python for ML External links# Running jobs (on the Alliance\u0026rsquo;s clusters) SLURM documentation. "},{"id":"55","rootTitleIndex":"10","rootTitle":"OpenOnDemand, HPC Portal","rootTitleIcon":"fa-solid fa-cloud fa-lg","rootTitlePath":"/grex-docs/ood/","rootTitleTitle":"Homepage / OpenOnDemand, HPC Portal","permalink":"/grex-docs/ood/","permalinkTitle":"Homepage / OpenOnDemand, HPC Portal","title":"OpenOnDemand, HPC Portal","content":"Introduction# OpenOnDemand or OOD for short, is an open source Web portal for High-Performance computing, developed at Ohio Supercomputing Center. OOD makes it easier for beginner HPC users to access the resources via a Web interface. OOD also allows for interactive, visualization and other Linux Desktop applications to be accessed on HPC systems via a convenient Web user interface.\nSince the end of October 2021, OpenOnDemand version 2 is officially in production on Grex. Since the beginning of January 2023, OpenOnDemand version 3 is officially in production on Grex. Since the beginning of February 2025, OpenOnDemand version 4 is officially in production on Grex. For more general OOD information, see the OpenOnDemand paper OpenOndemand on Grex# Grex\u0026rsquo;s OOD instance runs on ood.hpc.umanitoba.ca and requires the Alliance\u0026rsquo;s Duo MFA to authenticate. The OOD instance is available only from UManitoba campus IP addresses \u0026ndash; that is, your computer should be on the UM Campus network to connect.\nTo connect from outside the UM network, please install and start UManitoba Virtual Private Network: UM VPN . Note that you\u0026rsquo;d need the \u0026ldquo;VPN client\u0026rdquo; installation as described there; \u0026ldquo;VPN Gateway\u0026rdquo; will likely not work.\nOOD relies on in-browser VNC sessions; so, a modern browser with HTML5 support is required; we recommend Google Chrome or Firefox or Safari, and their derivatives.\nAlso, OOD creates a state directory under users\u0026rsquo; /home (/home/$USER/ondemand) where it keeps information about running and completed OOD jobs, shells, desktop sessions and such. Deleting the ondemand directory while a job or session is running would likely cause the job or session to fail.\nIt is better to leave the /home/$USER/ondemand directory alone! Connect to OpenOndemand# Connect to OOD on campus:\n1. Point your Web browser to https://ood.hpc.umanitoba.ca . This will redirect you to our Keycloack IDP screen.\n2. Use your Alliance/CCDB username and password to log in to Grex OOD.\nUse your Alliance username and password to log in to Grex OOD. 3. Provide Alliance\u0026rsquo;s Duo second factor authentication when asked.\nConnect to OOD off-campus, using UManitoba VPN:\nMake sure UM Ivanti Secure VPN is connected. This may require using UManitoba MS Entra second factor authentication. Note that UManitoba uses a different MFA second factor than the Alliance! Perform the steps 1-3 as above. There are different options for the Alliance Duo MFA, like the 6 digits passcode generated by DUO mobile application:\nUse your 6 digits passcode generated by DUO mobile app or use the passcode generated by your YubiKey:\nUse your YubiKey: touch and hold to receive the code. Or any other option by clicking on the menu \u0026ldquo;Other options\u0026rdquo;.\nOnce connected, you will see the following screen with the current Grex Message-of-the-day (MOTD):\nOpenOndemand front page If you scroll dow, links to some applications will show up. They correspond to pinned applications and featured subset of all available applications.\nNavigating OOD Web portal interface# There are several areas of interest on the OOD main webpage: the Dashboard bar on the top of the screen, various menu items (such as Files, Clusters, Jobs, Interactive Apps and Sessions).\nOpenOnDemand: main Open OnDemand dashboard. Apps: link to the available OOD applications. Files: file browser and related operations (copy, download, delete, \u0026hellip;). Jobs: Status of queues, and a JobComposer interface to submit batch scripts. Clusters: Status of Grex system and its SLURM partitions. Interactive Apps: list of interactive applications. OpenOndemand main menus The use of the different menus is described in the following sections:\nOOD main dashboard# The OpenOnDemand main dashboard menu shows the message of the day which is similar to the message you see when connecting to Grex via SSH. It shows the url for the documentation and support email to contact in case you need help. Some other information are also added to the message of the day.\nIf you scroll down from the front page, some icons with links to pinned applications and the subset of all available applications you have used recently:\nLinks to pinned interactive applications Please note that the list of applications may change over time. The snapshots shown on this page are taken from the current list of the available applications on the OpenOnDemand portal while updating this page. Apps menu# This menu show links to the pinned applications like Grex Simplified Desktop and a link to all application.\nLink to pinned and all interactive applications Files# One of the convenient and useful features of OOD is its Files app that allows you to browse the files and directories across all Grex filesystems: /home and /project.\nFile view on OpenOndemand web portal on Grex The main features accessible vile the menu Files are:\nAccess to storage: home and project directories. Create new directories and files via the sub-menus New File and New Directory View and edit text files Upload or download files via Upload and Download sub-menus. Delete data: files or directories. Copy or move data (files and directories). Access to a path to a file or directory using the sub-menu Copy Path. Open a terminal to a selected directory. While working with the directories, you could view the content of the folder, rename the folder, delete the folder. It is also possible to download the folder as zip file. While working with files, you can edit and change the text file, rename and delet files. The Files interface also allows interaction with remote storage locations:\nA link to Globus : this sub-menu start the Globus web interface in the current directory in Files tab. For more information about globus, please have a look to the dedicated page . If you have configured any rclone remotes, such as MS OneDrive , NextCloud or an Object Storage they will appear in Files menu along with your local Home and Project directories. You can conveniently upload your data to Grex using this Web interface. However, there are limits on the size of the uploads on the Web server and there can be practical limits on download sizes as well due to internet connection speed and stability. OOD on Grex has a 10Gb limit for maximal size of files to be uploaded through the File menu. For larger amount of data, please use Globus or SSH/SCP/SFTP clients. Jobs# This menu gives access to Active Jobs; Jobs Metrics and Grex Job Composer:\nJobs menu view on OpenOndemand web portal on Grex Active Jobs:\nFrom this menu, you can access the list of current jobs on the queue. In other terms, anything you could get from running squeue from the command line. There is a field with the name Filter where you can type Queued or Running if you want to filter the queued or running jobs.\nActive Jobs view on OpenOndemand web portal on Grex Jobs Metrics:\nFrom this menu, you can access the fairshare of your group and other metrics about the efficiency of the jobs from your group for the last 7 days. Similar metrics can be obtained using sacct command with appropriate format and options.\nJobs Metrics view on OpenOndemand web portal on Grex Grex Job Composer:\nFrom this menu. it is possible to access a form with predefined or generic slurm templates to generate slurm scripts. It offers the options to customize, save and submit jobs.\nJob Composer view on OpenOndemand web portal on Grex This will be discussed in more details in another section.\nClusters# From this menu, you can access the current partitions status and the Grex Cluster Cluster Status. This later shows a summary and the overview of the reources and their state, like number of available nodes, number of available processes, numper of the GPUs available and number of jobs in running and queued state.\nGrex Cluster Cluster Status view on OpenOndemand web portal on Grex From the menu Partitions Status, one can see the state of each partition where it shows the name of the partition, number of free nodes, number of free cores and memory.\nPartitions Status view on OpenOndemand web portal on Grex A variant of the above information is available via command line by running the command partition-list from any login node.\nThere is also a link to start a terminal from OOD session. Interactive Apps# From this menu, one can access different applications that classified into 3 categories:\nDesktops like Grex Desktop and Grex Desktop Simplified. GUI Apps like Matlab, Gaussview, Ovito, Stata, \u0026hellip; etc. Servers like Jupyter, Code Server and RStudio. A lisf of ineractive applications is accessible from the top menu Interactive Apps as shown in the following screenshot:\nOpenOndemand interactive applications Some applications, like Stata and Gaussview may show up on the snapshots displayed on this page but not under your session. These applications are configured to show up only if you have access to a particular POSIX group that restricts access to the software. Customized OOD apps on Grex# The OOD Dashboard menu, Interactive Apps, shows interactive applications. This is the main feature of OOD, it allows interactive work and visualizations, all in the browser. These applications will run as SLURM Jobs on Grex compute nodes. Users can specify required SLURM resources such as time, number of cores, wall time, partition name, \u0026hellip; etc.\nOpenOndemand applications on Grex After filling all the requirements and launching the Apps, the Apps jobs are submitted via a button Launch. The corresponding jobs appear in the Interactive Sessions tab. They can be used, monitored, connected to, and terminated as needed.\nThere are numerous supported applications in OpenOnDemand on Grex. These applications fall into two broad categories: Virtual Desktop apps (the ones delivering a Linux Desktop with some GUI software via NoVNC) and Servers that are delivered through a Web Proxy. A prominent example of a Server app is Jupyter Notebook or Jupyter Lab. Some Apps such as Matlab or Rstudio exists both as a Linux Desktop GUI and a Server version.\nWe keep actively developing the OOD Web Portal, and the list below may change over time as we add more popular applications or remove less used ones!\nAs for now, the following applications are supported:\nApplication Type Availability Notes Linux Desktop NoVNC Desktop Generally available - GaussView NoVNC Desktop Licensed users only - Matlab NoVNC Desktop Generally available - Matlab Server Server Generally available - JupyterLab Server Server Generally available Comes for SBEnv and CCEnv RStudio Server Server Generally available Comes for SBEnv and CCEnv CodeServer Server Generally available - Gnuplot NoVNC Desktop Generally available - Grace NoVNC Desktop Generally available - MetaShape Pro NoVNC Desktop Licensed users only - RELION NoVNC Desktop Generally available - STATA NoVNC Desktop Licensed users only - Feko NoVNC Desktop Licensed users only Ovito NoVNC Desktop Generally available - Note that only Apps available (licensed) to your research group will be visible in your group members\u0026rsquo; OOD interface.\nAs with regular SLURM jobs, it is important to specify SLURM partitions for them to start faster. Perhaps the test partition for Desktop is the best place to start interactive Desktop jobs, so it is hardcoded in the Simplified Desktop item.\nDesktops Custom Apps Servers Job Composer Guide Lines "},{"id":"56","rootTitleIndex":"12","rootTitle":"Friendly Organizations","rootTitleIcon":"fa-solid fa-users fa-lg","rootTitlePath":"/grex-docs/friends/","rootTitleTitle":"Homepage / Friendly Organizations","permalink":"/grex-docs/friends/localit/","permalinkTitle":"Homepage / Friendly Organizations / Local IT Resources","title":"Local IT Resources","content":"Introduction# In addition to the HPC Research Computing facility, there are other IT providers that might relate to research.\nResources provided by UManitoba IST# \u0026ldquo;Information Services and Technology Service Catalogue\u0026rdquo; provides a few services related to Administrative IT, Teaching, and Research Computing as well. The page refers to all Research Computing offerings including Grex.\nResources provided by UManitoba Libraries# Libraries provide a variety of services related to Research Data Management, as well as a GIS service: UM Libraries Research Services.\nFaculties IT representatives# Several Faculties/Departments have local IT representatives that maintain servers, workstation and computing labs. They can be reached through general IST Helpdesk request.\nFor more information, please contact directly the related services:\nUManitoba IST UManitoba Libraries Research Services. "},{"id":"57","rootTitleIndex":"11","rootTitle":"Workshops and Training Material","rootTitleIcon":"fa-solid fa-chalkboard-user fa-lg","rootTitlePath":"/grex-docs/training/","rootTitleTitle":"Homepage / Workshops and Training Material","permalink":"/grex-docs/training/","permalinkTitle":"Homepage / Workshops and Training Material","title":"Workshops and Training Material","content":"Local workshops# 2025 LoRa walkthrough 2024 2023 2022 2021 WestDRI training material# The training material from WestDRI (BC DRI and Prairies DRI groups, formerly known as WestGrid) webinars are available online and classified into different categories:\nGetting started: Link Programming: Link Tools: Link Courses: Link Events: Link Training material from the Alliance and its regional partners# DRAC - Now provided via Explora ACENET CAC Calcul Quebec Compute Ontario SciNET , new link SharcNET WestDRI "},{"id":"58","rootTitleIndex":"12","rootTitle":"Friendly Organizations","rootTitleIcon":"fa-solid fa-users fa-lg","rootTitlePath":"/grex-docs/friends/","rootTitleTitle":"Homepage / Friendly Organizations","permalink":"/grex-docs/friends/","permalinkTitle":"Homepage / Friendly Organizations","title":"Friendly Organizations","content":" DRAC (Alliance) UManitoba IT resources "},{"id":"59","rootTitleIndex":"<no value>","rootTitle":"Grex changes / software and hardware updates","rootTitleIcon":"fa-solid fa-house-chimney fa-lg","rootTitlePath":"","rootTitleTitle":"Homepage / Grex changes / software and hardware updates","permalink":"/grex-docs/changes/changes-before-2020/","permalinkTitle":"Homepage / Grex changes / software and hardware updates / Linux/SLURM update project","title":"Linux/SLURM update project","content":"Grex defunded since April 2, 2018# Since being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators. The old WestGrid documentation, hosted on the WestGrid website became irrelevant after the Grex upgrade, so please visit Grex’s New Documentation . Thus, if you are an experienced user in the previous “version” of Grex, you might benefit from reading this document: Description of Grex changes .\n"},{"id":"60","rootTitleIndex":"13","rootTitle":"Getting Help","rootTitleIcon":"fa-solid fa-circle-info fa-lg","rootTitlePath":"/grex-docs/support/","rootTitleTitle":"Homepage / Getting Help","permalink":"/grex-docs/support/","permalinkTitle":"Homepage / Getting Help","title":"Getting Help","content":"The Alliance support# The single point support contact for the Alliance is support@tech.alliancecan.ca\nEmailing to this address will create a support ticket in the Alliance ticketing system (Help Desk). We support both local (Grex) and National resources through the Alliance support ticketing system. This is the main support contact for our HPC group, and it is a preferred method (as compared to contacting an HPC analyst directly). If you use your UManitoba email address (email registered in CCDB) to contact the Alliance support, it will reach us faster because the system will automatically detect it and assign your username to the generated ticket.\nYou can also open a web interface to the ticketing system OTRS . This requires actually having an Alliance account to access it, so it can be less useful for inquiries on how to get an account. To sum it all up, the main way to reach out for support is: support@tech.alliancecan.ca Secondary support contacts in the Alliance that are useful for particular services:\ncloud@tech.alliancecan.ca for Cloud support. support@frdr-dfdr.ca for Federated Data Repository FRDR . globus@tech.alliancecan.ca for issues related to Compute Canada\u0026rsquo;s Globus file transfer. More information on the Alliance (formerly known as Compute Canada) support can be found here. We support both local (Grex) and National resources through the Alliance support ticketing system.\nIST Helpdesk# We have a Research Computing group in IST\u0026rsquo;s Help Desk system support@umanitoba.ca . Tickets opened in this system will reach us, especially if you clearly state that they are HPC or Alliance related. More info on the IST Help Desk website: DRAC , Grex .\nIn person training sessions and workshops# You can book an in-person session (face to face meeting or online). We help new users for getting started on the systems we support, or for solving a specific problem that is harder to do over email, or for a consultation about Grex or the resources provided by the Alliance.\nWhere to find us on Fort Garry campus? E2-588 EITC, Fort Garry Campus\nUniversity of Manitoba\nWinnipeg, MB R3T 2N2\nHours:\nMonday - Friday\n8:30 am - 4:30 pm\nWe also provide small-group workshops on Research Computing topics when there is enough demand. Contact us if you are interested in having a workshop on a particular topic. Help with this Website# The documentation is hosted on Github and is open-source. If you see errors, inconsistencies or have other feedback, or want to add to this documentation, please do not hesitate to open an issue or create a pull request at the Github repository!\nInternal links# Workshops and Training Material External links# the Alliance support IST Help Desk WestDRI/Simon Fraser University Training Material the Alliance National Training Calendar: Explora "},{"id":"61","rootTitleIndex":"14","rootTitle":"Disclaimer","rootTitleIcon":"fa-solid fa-triangle-exclamation fa-lg","rootTitlePath":"/grex-docs/disclaimer/","rootTitleTitle":"Homepage / Disclaimer","permalink":"/grex-docs/disclaimer/","permalinkTitle":"Homepage / Disclaimer","title":"Disclaimer","content":" This website is a place for technical information related to certain Research Computing resources, maintained for the benefit of the researchers at the University of Manitoba and their external collaborators. The information, which is technical in nature, represents advice on best practices for using the Research Computing resources (HPC). Parts of the website are preliminary/draft texts released provisionally, in order to speed up the documentation process, and may contain inaccuracies and errors.\nWe advise users to exercise reason when following the advice from these pages. We disclaim responsibility for any harm, loss of data, loss of good-will or whatever negative effects might happen due to reading this documentation.\nThis website does not represent official views, policies, or standards of University of Manitoba , BC DRI group, Prairies DRI group or the Digital Research Alliance of Canada (formerly known as Compute Canada). Please refer to the official sites of the above-mentioned institutions and organizations for more information.\nThis website does not represent official views of any hardware, software or services vendors that might be mentioned on the website\u0026rsquo;s pages.\nWestGrid ceased operations on April 1st, 2022. The former WestGrid institutions are now re-organized into two consortia: BC DRI group and Prairies DRI group. Compute Canada ceased operations on April 1st, 2022. The Digital Research Alliance of Canada , is coordinating the national advanced research computing ARC services. "},{"id":"62","rootTitleIndex":"15","rootTitle":"Glossary","rootTitleIcon":"fa-solid fa-book-atlas fa-lg","rootTitlePath":"/grex-docs/glossary/","rootTitleTitle":"Homepage / Glossary","permalink":"/grex-docs/glossary/","permalinkTitle":"Homepage / Glossary","title":"Glossary","content":"A# ACL :: Access Control List AI :: Artificial Intelligence AOCC :: AMD Compilers Collection ARC :: Advanced Research Computing AVX :: Advanced Vector eXtensions on x86_64 CPUs B# BLAS :: Basic Linear Algebra Subprograms C# CPU :: Central Processing Unit, a computer\u0026rsquo;s processor CC :: Compute Canada, a former organization replaced by DRAC CCDB :: CC User Database, a user accounts portal maintained by DRAC CVMFS :: Cern Virtual Machines File System CUDA :: Compute Unified Device Architecture, an NVidia library for GPU calculations CY:: Core Year, the unit of CPU and GPU resources used by CCDB and RAC D# DFT :: 1. Discrete Fourier Transform, 2. Density Functional Theory DRAC :: Digital Research Alliance of Canada, same as The Alliance E# F# FFT :: Fast Fourier Transform G# GCC :: GNU Compiler Collection GPU :: Graphics Processing Unit GROMACS :: GROningen MAchine for Chemical Simulations GUI :: Graphical User Interface H# HPC :: High-Performance Computing HTTP :: Hypertext Transfer Protocol HTTPS :: Hypertext Transfer Protocol Secure HDF5 :: Hierarchical Data file Format version 5 I# I/O :: Input/Output IOPS :: I/O Operations Per Second IST :: Information Services and Technologies at UM J# JuPyteR :: an interactive debugging and visualization tool for Python, Julia, R and other programming languages. K# L# LAMMPS :: Large-scale Atomic/Molecular Massively Parallel Simulator LAPACK :: Linear Algebra PACKage, used together with BLAS LDAP :: Lightweight Directory Access Protocol, an user authenticaton service LLM :: Large Language Model, a flavour of ML LMod :: a software environment module system written in Lua at TACC Lustre :: a parallel filesystem for HPC M# MDS :: Metadata Server MDT :: Metadata Target ML :: Machine Learning MPI :: Message-Passing Interface N# NetCDF :: Network Common Data Form, a set of machine independed data formats NFS :: Network File System for Linux NVMe :: Non-Volatile Memory express, a form of SSD disk O# OCI :: Open Containers Initiative, a container image format specification OMP , OpenMP :: Open Multi-Processing is a program interface for shared-memory multi-processing OOD :: Open OnDemand , a web portal for HPC OSS :: Object Storage Server OST :: Object Storage Target P# PAICE :: Pan-Canadian AI Compute Environment PI :: Principal Investigator PMIx :: Process Management Interface for Exascale, a process kickstart system for MPI and SLURM Q# R# RAC :: Resource Allocation Competition RAM :: Random Access Memory RAS :: Rapid Access Service RDM :: Research Data Management S# SLURM :: Simple Linux Utility for Resource Management SSH :: Secure SHell SSD :: Solid State Disk SSL :: Secure Socket Layers, same as TLS T# TLS :: Transport Layer Security U# UM :: University of Manitoba V# VASP :: Vienna Ab initio Simulation Package VM :: Virtual Machine W# WAN :: A Wide Area Network (WAN) is a network that spans several geographically distributed locations. X# X11 :: A GUI for UNIX and Linux based systems Y# Z# "},{"id":"63","rootTitleIndex":"16","rootTitle":"Grex Documentation - Sitemap","rootTitleIcon":"fa-solid fa-sitemap fa-lg","rootTitlePath":"/grex-docs/sitemap/","rootTitleTitle":"Homepage / Grex Documentation - Sitemap","permalink":"/grex-docs/sitemap/","permalinkTitle":"Homepage / Grex Documentation - Sitemap","title":"Grex Documentation - Sitemap","content":" Grex HPC User Guide Grex Quick Start Guide Access and usage conditions Connecting to Grex Multi-Factor Authentication Connecting with SSH Connect with OOD Transferring Data Globus on Grex Rclone and OneDrive Object Storage on OpenStack Storage and Data Data sizes and Quotas Data Backup Data sharing Running Jobs Slurm partitions Interactive jobs Batch jobs Contributed nodes Using local disks Software / Applications Linux tools Using modules Code development Containers CVMFS List of Software Software Specific Notes Espresso Gaussian GROMACS Julia Jupyter notebooks LAMMPS MATLAB NWChem ORCA R VASP Python for ML OpenOnDemand Desktops Custom Apps Servers Job Composer Guide Lines Workshops 2025 LoRa walkthrough 2024 2023 2022 2021 Friendly Organizations DRAC (Alliance) UManitoba IT resources Getting Help Disclaimer Glossary Sitemap "}]