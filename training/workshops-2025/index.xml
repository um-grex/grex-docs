<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Workshops - 2025 on Grex</title><link>https://um-grex.github.io/grex-docs/training/workshops-2025/</link><description>Recent content in Workshops - 2025 on Grex</description><generator>Hugo</generator><language>en</language><copyright>The MIT License (MIT) Copyright Â© 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-docs/training/workshops-2025/index.xml" rel="self" type="application/rss+xml"/><item><title>LoRa walkthrough</title><link>https://um-grex.github.io/grex-docs/training/workshops-2025/lora-huggingface/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/training/workshops-2025/lora-huggingface/</guid><description>&lt;h2 id='high-performance-computing-workshop---may-21-23-2025'>High Performance Computing Workshop - May 21-23, 2025&lt;a href='#high-performance-computing-workshop---may-21-23-2025' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>This is an example of training LoRa in a batch job.
The example uses a script from &lt;a
 href="https://huggingface.co"
 class="is-pretty-link">Huggingface&lt;/a
>
 diffusers package.&lt;/p>
&lt;h3 id='pull-huggingface-diffusers-copy-data'>Pull Huggingface diffusers, copy data&lt;a href='#pull-huggingface-diffusers-copy-data' class='anchor'>#&lt;/a>
&lt;/h3>&lt;hr>
&lt;p>We will downolad Huggingface source code from their Github using Git.
This is needed to use their Python example training script rather than developing ours from scrath.
We will also copy the datasets for training from &lt;strong>pythonai&lt;/strong> subdirectory of our Workshop materials.&lt;/p></description></item></channel></rss>