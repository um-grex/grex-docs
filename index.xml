<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Unofficial Grex User Guide on Grex</title><link>https://um-grex.github.io/grex-docs/</link><description>Recent content in Unofficial Grex User Guide on Grex</description><generator>Hugo</generator><language>en</language><copyright>The MIT License (MIT) Copyright Â© 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Connecting to OneDrive using Rclone</title><link>https://um-grex.github.io/grex-docs/connecting/data-transfer/one-drive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/connecting/data-transfer/one-drive/</guid><description>UM OneDrive# The University of Manitoba provides Microsoft OneDrive to its users as part of the Microsoft Office 365 contract. While not suited, for performance reasons, for online computation work or for storing large datasets, OneDrive can be a useful storage space to store important data of smaller size, documents, articles and such. It also offers some data preservation features like data recovery and data history.
Please also refer to UM Data Security Classification to learn which kinds of data can be stored where.</description></item><item><title>Linux/SLURM update project</title><link>https://um-grex.github.io/grex-docs/changes/linux-slurm-update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/changes/linux-slurm-update/</guid><description>Grex changes: Linux/SLURM update project.# December 10-11, 2019
Introduction / Motivation# Grex runs an old version of CentOS 6, which gets unsupported in 2020. The 2.6.x Linux kernel that is shipped with CentOS 6 does not support containerized workloads that require recent kernel features. The Lustre parallel filesystem client had some troubles that we were unable to resolve with the CentOS 6 kernel version as well. Finally, the original Grex resource management software, Torque 2.</description></item><item><title>Running Gaussian on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/gaussian/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/gaussian/</guid><description>Introduction# Gaussian 16 is a comprehensive suite for electronic structure modeling using ab initio, DFT and semi-empirical methods. A list of Gaussian 16 features can be found here .
User Responsibilities and Access# University of Manitoba has a site license for Gaussian 16 and GaussView. However, it comes with certain license limitations, so access to the code is subject to some license conditions.
Since, as of now, Compute Canada accounts are a superset of Grex accounts, users will want to initiate getting access by sending an email agreeing to Gaussian conditions to support@tech.</description></item><item><title>Running Julia on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/julia/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/julia/</guid><description>Introduction# Julia is a programming language that was designed for performance, ease of use and portability. It is available as a module on Grex.
Available Julia versions# Presently, binary Julia versions 1.3.0, 1.5.4 and 1.6.1 are available. Use module spider julia to find out other versions.
Installing packages# We do not maintain centralized versions of Julia packages. Users should install Julia modules in their home directory.
The command is (in Julia REPL):</description></item><item><title>Running LAMMPS on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/lammps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/lammps/</guid><description>Introduction# LAMMPS is a classical molecular dynamics code. The name stands for Large-scale Atomic / Molecular Massively Parallel Simulator.
Modules# Multiple versions of LAMMPS were installed on Grex. To see all the available versions, use module spider lammps and follow the instructions.
Available CPU versions:# Version Module Name Supported Packages 29 Sep 21 lammps/29Sep21 * 05 Jun 19 lammps/5Jun19 * 11 Aug 17 lammps/11Aug17 * 05 Nov 16 lammps/5Nov16 * 30 Jul 16 lammps/30jul16 * Available GPU versions:# As for the time when writing this page, there is only one version of LAMMPS with GPU support.</description></item><item><title>General Linux tools</title><link>https://um-grex.github.io/grex-docs/software/general-linux/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/software/general-linux/</guid><description>Linux tools on Grex# There are a number of general and distro-specific tools on Grex that are worth mentioning here. Such tools are: text editors, image viewers, file managers, &amp;hellip; etc.
Command line Text editors# Command line text editors allow you to edit files right on Grex in any terminal session (such as SSH session or an X terminal under X2Go):
The (arguably) most popular editor is vi, or vim. It is very powerful, but requires some experience to use it.</description></item><item><title>Running MATLAB on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/matlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/matlab/</guid><description>Introduction# MATLAB is a general-purpose high-level programming package for numerical work such as linear algebra, signal processing and other calculations involving matrices or vectors of data. We have a campus license for MATLAB which is used on Grex and other local computing resources. MATLAB is available only for UManitoba users.
As with most of the Grex software, MATLAB is available as a module. The following command will load the latest version available on Grex:</description></item><item><title>Running NWChem on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/nwchem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/nwchem/</guid><description>Introduction# NWChem is a Scalable open-source solution for large scale molecular simulations. NWChem is actively developed by a consortium of developers and maintained by the EMSL located at the Pacific Northwest National Laboratory (PNNL) in Washington State. The code is distributed as open-source under the terms of the Educational Community License version 2.0 (ECL 2.0).
System specific notes# On the Grex software stack, NWChem is using OpenMPI 3.1 with Intel compilers toolchains.</description></item><item><title>Running ORCA on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/orca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/orca/</guid><description>Introduction# ORCA is a flexible, efficient and easy-to-use general purpose tool for quantum chemistry with specific emphasis on spectroscopic properties of open-shell molecules. It features a wide variety of standard quantum chemical methods ranging from semi-empirical methods to DFT to single - and multi-reference correlated ab initio methods. It can also treat environmental and relativistic effects.
User Responsibilities and Access# ORCA is a proprietary software, even if it is free it still requires you to agree to the ORCA license conditions.</description></item><item><title>Slurm partitions</title><link>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</guid><description>Partitions# The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a &amp;ldquo;partition&amp;rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who &amp;ldquo;owns&amp;rdquo; the hardware as well.
There is no fully automatic selection of partitions, other than the default skylake for most of the users, and compute for the short jobs.</description></item><item><title>Running Priroda on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/priroda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/priroda/</guid><description>Introduction# Priroda is a fast parallel relativistic DFT and ab-initio code for molecular modeling, developed by Dr. Dimitri N. Laikov. The code originally implemented fast resolution-of-identity GGA DFT for coulomb and exchange integrals. Later it was extended to provide RI-DFT with hybrid functional, RI-HF and RI-MP2, and parallel high-level coupled-cluster methods. All these levels of theory can be used together with an efficient all-electron scalar-relativistic method, with small-component bases supplied for all the elements of the Periodic Table.</description></item><item><title>Running VASP on Grex</title><link>https://um-grex.github.io/grex-docs/specific-soft/vasp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/specific-soft/vasp/</guid><description>Introduction# VASP is a massively parallel plane-wave solid state DFT code. On Grex it is available only for the research groups that hold VASP licenses. To get access, PIs would need to send us a confirmation email from the VASP vendor, detailing the status of their license and a list of users allowed to use it.
System specific notes# On the Grex local software stack, we have VASP 5 and VASP 6 using Intel compiler and OpenMPI 3.</description></item><item><title>Using Multi-Factor Authentication (MFA) on Grex</title><link>https://um-grex.github.io/grex-docs/connecting/mfa/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/connecting/mfa/</guid><description>Important Notices# As of January 9, 2024 MFA is enforced for all UManitoba/Grex users. We appreciate your attention to this matter and encourage you to enroll at your earliest convenience to ensure a smooth transition. Should you have any questions or require further assistance, please don&amp;rsquo;t hesitate to reach out to your local HPC team. If some of your work relies on unattended connections or automations that may be disrupted by MFA, we ask that you do not enroll yet, and instead contact us for technical support, so that we can work with you on a solution.</description></item><item><title>Connecting to Grex with SSH</title><link>https://um-grex.github.io/grex-docs/connecting/ssh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/connecting/ssh/</guid><description>SSH# Most of the work on shared HPC computing systems is done via Linux command line / Shell. To connect, in a secure manner, to a remote Linux system, you would like to use SSH protocol. You will need to have:
access to the Internet that lets SSH ports open. a user account on Grex (presently, it is a Compute Canada (an Alliance) Account). and an SSH client for your operating system.</description></item><item><title>How to run interactive jobs on Grex?</title><link>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</guid><description>Interactive work# The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:
edit files compile codes and run short interactive calculations. configure and build programs (limit the number of threads to 4: make -j4) submit and monitor jobs transfer and/or download data run short tests, &amp;hellip; etc. In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].</description></item><item><title>Modules and software stacks</title><link>https://um-grex.github.io/grex-docs/software/using-modules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/software/using-modules/</guid><description>Introduction# External links# The Alliance documentation about using module</description></item><item><title>Running batch jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</guid><description>Batch jobs# HPC systems usually are clusters of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users&amp;rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called &amp;ldquo;jobs&amp;rdquo; to a &amp;ldquo;batch queue&amp;rdquo;.</description></item><item><title>Code Development on Grex</title><link>https://um-grex.github.io/grex-docs/software/code-development/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/software/code-development/</guid><description>Introduction# Grex comes with a sizable software stack that contains most of the software development environment for typical HPC applications. This section of the documentation covers the best practices for compiling and building your own software on Grex.
The login nodes of Grex can be used to compile codes and to run short interactive and/or test runs. All other jobs must be submitted to the batch system. We do not do as heavy resource limiting on Grex login nodes as, for example, Compute Canada does; so, code development on login nodes is entirely possible.</description></item><item><title>Connecting to Grex via X2Go</title><link>https://um-grex.github.io/grex-docs/connecting/x2go/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/connecting/x2go/</guid><description>Graphical user interface access with X2Go# Linux (and UNIX) have a graphical user interface framework, which is called X11, X-Window system . It is possible to use remote X11 applications with the combination of SSH client with X11-tunneling enabled and a local X11-server running. However, this way is often quite slow and painful, especially over WAN networks, where latencies of the network really impair user experience.
Luckily, there is a solution for this, which is called NX.</description></item><item><title>Connect and Transfer data with OOD</title><link>https://um-grex.github.io/grex-docs/connecting/ood/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/connecting/ood/</guid><description>OSC OpenOnDemand on Grex# Grex&amp;rsquo;s current OOD v.3 instance runs on zebu.hpc.umanitoba.ca (the earlier OOD v.2 instance on aurochs.hpc.umanitoba.ca is depreciated and no longer available) . It is available only from UManitoba IP addresses &amp;ndash; that is, your computer should be on the UM Campus network or the UM VPN to connect.
To connect from outside the UM network, please install and start UManitoba Virtual Private Network . OOD relies on in-browser VNC sessions; so, a modern browser with HTML5 support is required; we recommend Google Chrome or Firefox and its derivatives (Waterfox, for example).</description></item><item><title>Containers for Software</title><link>https://um-grex.github.io/grex-docs/software/containers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/software/containers/</guid><description>Introduction# Linux Containers are means to isolate software dependencies from the base Linux operating system. On Grex, we support the Singularity container system, developed by a company called SyLabs. There was a fork of the Singularity project, that produced a new project called Apptainer . As of now (early 2024), Singularity-CE by Sylabs and Apptainer by Linux Foundations are using the same SIF image format and thus are largely same with respect to their usage and main features.</description></item><item><title>Using local disks: $SLURM_TMPDIR</title><link>https://um-grex.github.io/grex-docs/running-jobs/using-localdisks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/using-localdisks/</guid><description>Introduction#</description></item><item><title>Workshops - 2023</title><link>https://um-grex.github.io/grex-docs/training/workshops-2023/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/training/workshops-2023/</guid><description>Autumn workshop, November 2023# Below are the slides from the Grex workshop that was held on November 6-8, 2023:
Updates on Research Computing Resources: Slides Getting accounts, connecting, using Duo MFA: Slides Basics of Linux Shell: Slides Beginner running HPC jobs with SLURM: Slides Using HPC with OpenOnDemand Web portal: Slides Beginner HPC software overview: Slides Using containers in HPC (Apptainer, SingularityCE): Slides Basics of using OpenStack cloud: Slides Getting most of HPC system: Slides Fall DATA-4010 seminars, September 2023# Below are the slides from the DATA 4010 seminars that were held on September 18 and 25, 2023:</description></item><item><title>Workshops - 2022</title><link>https://um-grex.github.io/grex-docs/training/workshops-2022/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/training/workshops-2022/</guid><description>Autumn workshop, October 2022# Below are the slides from the Grex workshop that was held on October 26-27, 2022:
Program and updates: Slides High Performance Computing: Start Guide: Slides High Performance Computing: software environments: Slides OSC OpenOnDemand portal on Grex: Slides Spring workshop, May 2022# Below are the slides from the Grex workshop that was held on May 4-5, 2022:
Introduction to local and National HPC at UManitoba: How to use available software and run jobs efficiently: Slides Introduction to High Performance Computing step by step: From getting an account to running jobs: Slides Using GP GPU compute on Grex: Slides High Performance Computing and software environments: Slides OSC OpenOnDemand portal on Grex: Slides</description></item><item><title>Scheduling policies and running jobs on Contributed nodes</title><link>https://um-grex.github.io/grex-docs/running-jobs/contributed-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/contributed-systems/</guid><description>Scheduling policies for contributed systems# Grex has a few user-contributed nodes. The owners of the hardware have preferred access to them. The current mechanism for the &amp;ldquo;preferred access&amp;rdquo; is preemption.
On the definition of preferential access to HPC systems# Preferential access is when you have non-exclusive access to your hardware, in a sense that others can share in its usage over large enough periods. There are the following technical possibilities that rely on the HPC batch queueing technology we have.</description></item><item><title>CVMFS and the Alliance software stack</title><link>https://um-grex.github.io/grex-docs/software/cern-vmfs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/software/cern-vmfs/</guid><description>CC CernVMFS on Grex# CVMFS or CernVM FS stands for CernVM File System. It provides a scalable, reliable and low-maintenance software distribution service. CVMFS was originally developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications. Since then it got a use as a generic way of distributing software.
Presently, we use CernVMFS (CVMFS) to provide the Alliance&amp;rsquo;s (or Compute Canada&amp;rsquo;s) software stack.</description></item><item><title>Workshops - 2021</title><link>https://um-grex.github.io/grex-docs/training/workshops-2021/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/training/workshops-2021/</guid><description>Autumn workshop, November 2021# Below are the slides from the Grex workshop that was held on November 01-02, 2021:
Introduction to HPC: Basics: Steps from getting an account to submitting jobs: Slides Linux (Unix) shell basics: Slides Software on HPC clusters: Slides Scheduling jobs on HPC clusters: How to optimize your jobs and get more from the resources available?: Slides Spring workshop, April 2021# Below are the slides from the Grex workshop that was held on April 21-22, 2021:</description></item><item><title>Data sizes and quotas</title><link>https://um-grex.github.io/grex-docs/storage/data-sizes-and-quota/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/storage/data-sizes-and-quota/</guid><description>Data size and quotas# This section explains how to find the actual space and inode usage of /home/ and /project allocations on Grex. We limit the size of the data and the number of files that can be stored on these filesystems. The table provides &amp;ldquo;defaut&amp;rdquo; storage quota on Grex. Larger quota can be obtained on /project via UM local RAC process.
File system Type Total space Bulk Quota Files Quota /home NFSv4/RDMA 15 TB 100 GB / user 0.</description></item><item><title>How to use jupyter notebooks on Grex?</title><link>https://um-grex.github.io/grex-docs/software/jupyter-notebook/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/software/jupyter-notebook/</guid><description>Jupyter on Grex# Jupyter is a Web-interface aimed to support interactive data science and scientific computing. Jupyter supports several dynamic languages, most notably Python, R and Julia. Jupyter offers a metaphor of &amp;ldquo;computational document&amp;rdquo; that combines code, data and visualizations, and can be published or shared with collaborators.
Jupyter can be used either as a simple, individual notebook or as a multi-user Web server/Interactive Development Environment (IDE), such as JupyterHub/JupyterLab. The JupyterHub servers can use a variety of computational back-end configurations: from free-for-all shared workstation to job spawning interfaces to HPC schedulers like SLURM or container workflow systems like Kubernetes.</description></item><item><title>Data Backup</title><link>https://um-grex.github.io/grex-docs/storage/data-backup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/storage/data-backup/</guid><description>Backup policies# Since late 2023, there has been a tape backup for user data stored on main Grex filesystems, /home and /project. Our limited resources and large amounts of data do put some limitations on what and how fast can be backed up and restored. All backup is done to tape in the same HPCC datacentre.
The /home filesystem is backed up daily using incremental backup. A new full backup is done quarterly.</description></item><item><title>Data sharing</title><link>https://um-grex.github.io/grex-docs/storage/data-sharing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/storage/data-sharing/</guid><description>Data sharing PPPPP# Sharing of accounts login information (like passwords or SSH keys) is strictly forbidden on Grex, as well as on most of the HPC systems. There is a mechanism of data/file sharing that does not require sharing of the accounts. To access each other&amp;rsquo;s data on Grex, the UNIX groups and permissions mechanism can be used as explained below. Also, Access Control Lists (ACLs) should be used for a more fine-grained control of permissions.</description></item><item><title>Linux/SLURM update project</title><link>https://um-grex.github.io/grex-docs/changes/changes-before-2020/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/changes/changes-before-2020/</guid><description>Grex defunded since April 2, 2018# Since being defunded by WestGrid (on April 2, 2018), Grex is now available only to the users affiliated with University of Manitoba and their collaborators. The old WestGrid documentation, hosted on the WestGrid website became irrelevant after the Grex upgrade, so please visit Grexâs New Documentation . Thus, if you are an experienced user in the previous âversionâ of Grex, you might benefit from reading this document: Description of Grex changes .</description></item></channel></rss>