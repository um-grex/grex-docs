<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SLURM on Grex</title><link>https://um-grex.github.io/grex-docs/tags/slurm/</link><description>Recent content in SLURM on Grex</description><generator>Hugo</generator><language>en</language><copyright>The MIT License (MIT) Copyright Â© 2023 UM-Grex</copyright><atom:link href="https://um-grex.github.io/grex-docs/tags/slurm/index.xml" rel="self" type="application/rss+xml"/><item><title>Slurm partitions</title><link>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/slurm-partitions/</guid><description>&lt;h2 id='partitions'>Partitions&lt;a href='#partitions' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM, as a scheduler, this requires partitioning: a &amp;ldquo;partition&amp;rdquo; is a set of compute nodes, grouped by a characteristic, usually by the kind of hardware the nodes have, and sometimes by who &amp;ldquo;owns&amp;rdquo; the hardware as well.&lt;/p>
&lt;p>There is no fully automatic selection of partitions, other than the default &lt;strong>skylake&lt;/strong> for most of the users for the short jobs. For the contributors&amp;rsquo; group members, the default partition will be their contributed nodes. &lt;strong>Thus, in many cases users have to specify the partition manually when submitting their jobs!&lt;/strong>&lt;/p></description></item><item><title>How to run interactive jobs on Grex?</title><link>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/interactive-jobs/</guid><description>&lt;h2 id='interactive-work'>Interactive work&lt;a href='#interactive-work' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>The login nodes of Grex are shared resources and should be used for basic operations such as (but not limited) to:&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>edit files&lt;/li>
&lt;li>compile codes and run short interactive calculations.&lt;/li>
&lt;li>configure and build programs (limit the number of threads to 4: make -j4)&lt;/li>
&lt;li>submit and monitor jobs&lt;/li>
&lt;li>transfer and/or download data&lt;/li>
&lt;li>run short tests, &amp;hellip; etc.&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>In other terms, anything that is not CPU, nor memory intensive [for example, a test with up to 4 CPUs, less than 2 Gb per core for 30 minutes or less].&lt;/p></description></item><item><title>Running batch jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/batch-jobs/</guid><description>&lt;h2 id='batch-jobs'>Batch jobs&lt;a href='#batch-jobs' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>HPC systems usually are &lt;strong>clusters&lt;/strong> of many compute nodes, which are joined by an interconnect (like InfiniBand (IB) or Omni-PATH (OPA)), and under control of a resource management software (for example SLURM). From the users&amp;rsquo; point of view, the HPC system is a unity, a single large machine rather than a network of individual computers. Most of the time, HPC systems are used in batch mode: users would submit so-called &amp;ldquo;jobs&amp;rdquo; to a &amp;ldquo;batch queue&amp;rdquo;. A subset of the available resources of the HPC machine is allocated to each of the users&amp;rsquo; batch jobs, and they run without any need for user intervention as soon as the resources become available.&lt;/p></description></item><item><title>Running jobs on Grex</title><link>https://um-grex.github.io/grex-docs/running-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://um-grex.github.io/grex-docs/running-jobs/</guid><description>&lt;h2 id='why-running-jobs-in-batch-mode'>Why running jobs in batch mode?&lt;a href='#why-running-jobs-in-batch-mode' class='anchor'>#&lt;/a>
&lt;/h2>&lt;hr>
&lt;p>There are many reasons for adopting a batch mode for running jobs on a cluster. From providing user&amp;rsquo;s computations with fairness, traffic control to prevent resource congestion and wasting, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the &lt;a
 href="https://slurm.schedmd.com/documentation.html"
 class="is-pretty-link">SLURM&lt;/a
>
 batch system since &lt;strong>December 2019&lt;/strong> with the &lt;strong>Linux/SLURM update&lt;/strong> &lt;a
 href="https://um-grex.github.io/grex-docs/changes/linux-slurm-update/"
 class="is-pretty-link">project&lt;/a
>
.&lt;/p></description></item></channel></rss>