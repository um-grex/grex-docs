<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="CVMFS and ComputeCanada"><meta property="og:title" content="CVMFS and ComputeCanada"><meta property="og:description" content="Cern VMFS on Grex CVMFS or CernVM stands for CernVM File System. It provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications.
Presently, we use CernVMFS (CVMFS) to provide Compute Canada&rsquo;s software stack. We plan to add more publically available CVMFS software repositories such as the one from OpenScienceGrid , in a near future."><meta property="og:type" content="article"><meta property="og:url" content="http://um-grex.github.io/grex-docs/docs/grex/software/cern-vmfs/"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2022-02-24T19:45:39-06:00"><title>CVMFS and ComputeCanada | Unofficial Grex User Guide</title><link rel=icon href=/grex-docs/favicon.png type=image/x-icon><link rel=stylesheet href=/grex-docs/book.min.06cbd313f49ddc884804421299d5dc11b1cd097fdcfed7f054a79a137890a2d7.css integrity="sha256-BsvTE/Sd3IhIBEISmdXcEbHNCX/c/tfwVKeaE3iQotc="><script defer src=/grex-docs/en.search.min.0698a0af23c7558f008b273fc6213934a3f35e45907b7ce3bfeefd15788363e0.js integrity="sha256-BpigryPHVY8Aiyc/xiE5NKPzXkWQe3zjv+79FXiDY+A="></script></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><style>hr{border:0;height:3px;background-image:linear-gradient(to right,transparent,#095484,transparent)}</style><h2 class=book-brand><a href=http://um-grex.github.io/grex-docs><img src=/grex-docs/logo/um_logo_email_signature.png style=width:auto;height:auto alt=Logo><br><hr><span>Unofficial Grex User Guide</span><hr></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64><div class="book-search-spinner spinner hidden"></div><ul id=book-search-results></ul></div><hr><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/>Notes of Grex Changes</a><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/training/>Training Materials and Presentations</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/grex-docs/docs/computecanada/>Accessing Compute Canada resources</a><ul></ul></li><li class=book-section-flat><a href=/grex-docs/docs/grex/>Grex HPC QuickStart</a><ul><li><a href=/grex-docs/docs/grex/access/>Access and Usage conditions</a></li><li><a href=/grex-docs/docs/grex/connecting/>Connecting / Transferring data</a></li><li><a href=/grex-docs/docs/grex/data/>Storage and Data</a></li><li><a href=/grex-docs/docs/grex/running/>Running Jobs</a></li><li><a href=/grex-docs/docs/grex/ood/>Grex's OpenOnDemand Web Portal</a></li><li><a href=/grex-docs/docs/grex/software/>Software</a><ul><li><a href=/grex-docs/docs/grex/software/cern-vmfs/ class=active>CVMFS and ComputeCanada</a></li><li><a href=/grex-docs/docs/grex/software/general-linux/>General Linux tools</a></li><li><a href=/grex-docs/docs/grex/software/containers/>Containers for Software</a></li><li><a href=/grex-docs/docs/grex/software/code-development/>Code Development on Grex</a></li><li><a href=/grex-docs/docs/grex/software/jupyter-notebook/>Using JuPyTer Notebooks</a></li><li><a href=/grex-docs/docs/grex/software/specific/>Software-specific notes</a></li></ul></li></ul></li><li><a href=/grex-docs/docs/faq/>Frequently Asked Questions</a></li><li><a href=/grex-docs/docs/localit/>Local IT Resources</a></li><li><a href=/grex-docs/docs/support-contacts/>Support and Training</a></li><li><a href=/grex-docs/docs/disclaimer/>Disclaimer</a></li></ul></nav><script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></aside><div class=book-page><article class=markdown><h1 id=cern-vmfs-on-grex>Cern VMFS on Grex</h1><p><a href=https://cernvm.cern.ch/portal/filesystem title=CVMFS target=_blank rel=noopener>CVMFS or CernVM</a>
stands for CernVM File System. It provides a scalable, reliable and low-maintenance software distribution service. It was developed to assist High Energy Physics (HEP) collaborations to deploy software on the worldwide-distributed computing infrastructure used to run data processing applications.</p><p>Presently, we use CernVMFS (CVMFS) to provide Compute Canada&rsquo;s software stack. We plan to add more publically available CVMFS software repositories such as the one from <a href=https://opensciencegrid.org/ title=OpenScienceGrid target=_blank rel=noopener>OpenScienceGrid</a>
, in a near future. Note that we can only &ldquo;pull&rdquo; software from these repositories. To actually add or change software, datasets etc., the respective orgaizations controlling CVMFS repositories should be contacted directly.</p><p>Access to the CVMFS should be transparent to the Grex users: no action is needed other than loading a software module or setting a path.</p><p>Grex does not have a local CVMFS stratum server. All we do is to cache the software items as they get requested. Thus there can be a delay associated with pulling a software item from <strong>Stratum 1 (Replica Server)</strong> for the first time. It usually does not matter for serial progams but parallel codes, that rely on simultaneus process spawning across many nodes, it might cause timeout errors. Thus, it is probably a good idea to first access the codes in a small interactive job to warm up the Grex&rsquo;s local CVMFS cache.</p><h2 id=compute-canada-software-stack>Compute Canada software stack</h2><p>The main reason for having CVMFS supported on Grex is to provide Grex users with the software environment as similar as possible with the environment existig on National Compute Canada HPC machines. On Grex, the module tree from Compute Canada software stack is not set as default, but has to be loaded with the following commands:</p><blockquote class="book-hint info"><p><code>module purge</code></p><p><code>module load CCEnv</code></p></blockquote><p>After the above command, use <strong>module spider</strong> to search for any software that might be available in the CC software stack. Note that &ldquo;default&rdquo; environment (the <em>StdEnv</em> and <em>nixpkgs</em> modules of the CC stack) are not loaded automatically, unlike on Compute Canada general purpose (GP) clusters. Therefore, it is a good practice to load these modules right away after the CCEnv. The example below loads the Nix package layer that forms the base layer of CC software stack, and then one of the &ldquo;standard environments&rdquo;, in this case based on Intel 2018 and GCC 7.3 compilers, MKL and OpenMPI.</p><p>There is more than one StdEnv version to chose from.</p><blockquote class="book-hint info"><p><code>module load nixpkgs/16.09</code></p><p><code>module load StdEnv/2018.3</code></p></blockquote><p>Note that there are several CPU architectures in the CC software stack. They differ in the CPU instruction set used by the compilers, to generate the binary code. The default for legacy systems like Grex is the lowest SSE3 architecture <em>arch/sse3</em>. It ensures that there is no failure on the legacy Grex nodes (which are of NEHALEM, SSE4.2 architecture) due to more recent instructions like AVX, AVX2 and AVX512 that were added by Intel afterwards.</p><p>For running on Contributed Nodes, that may be of much newer CPU generation, it is better to use the <em>arch/avx512</em> module and setting RSNT_ARCH=avx512 environment variable in the job scripts.</p><p>Some of the software items on CC software stack might assume certain environment variables set that are not present on Grex; one example is SLURM_TMPDIR. In case your script fails for this reason, the following line could be added to the job script:</p><blockquote class="book-hint info"><code>export SLURM_TMPDIR=$TMPDIR</code></blockquote><p>While a majority of CC software stack is built using OpenMPI, some items might be based on IntelMPI. These will require following additional environment variables to be able to integrate with SLURM on Grex:</p><blockquote class="book-hint info"><p><code>export I_MPI_PMI_LIBRARY=/opt/slurm/lib/libpmi.so</code></p><p><code>export I_MPI_FABRICS_LIST=shm:dapl</code></p></blockquote><p>If a script assumes, or relies on using the <em>mpiexec.hydra</em> launcher, the later might have to be provided with <em>-bootstrap slurm</em> option.</p><h3 id=how-to-find-software-on-cc-cvmfs>How to find software on CC CVMFS</h3><p>Compute Canada&rsquo;s software building system automatically generates documentation for each item, which is available at the <a href=https://docs.computecanada.ca/wiki/Available_software target=_blank rel=noopener>Available Software</a>
page. So the first destination to look for a software item is probably to browse this page. Note that this page covers the default CPU arhitectures (AVX2, AVX512) of the National systems, and legacy architecturs (SSE3, AVX) might not necessary have each of the software versions and items compiled for them. It is possible to request such versions to be added.</p><p>The Lmod, <strong>module spider</strong> command can be used on Grex to search for modules that are actually available. Note that the <em>CCEnv</em> software stack is not loaded by default; you would have to load it first to enable the spider command to search through the CC software stack:</p><blockquote class="book-hint info"><p><code>module purge; module load CCEnv</code></p><p><code>module spider mysoftware</code></p></blockquote><p>Then, when finding available software versions and their dependencies, <em>module load</em> command can be used, as descibed <a href=https://docs.computecanada.ca/wiki/Utiliser_des_modules/en target=_blank rel=noopener>here</a></p><h3 id=how-to-request-software-added-to-cc-cvmfs>How to request software added to CC CVMFS</h3><p>Compute Canada maintains and distributes the software stack as part of its mandate to maintain the National HPC systems. To request a software item installed, the requestor should be part of Compute Canada system (that is, have an account in <a href=https://ccdb.computecanada.ca target=_blank rel=noopener>CCDB</a>
, which is also a prerequisite to have access to Grex. Any CC user can submit such request to <a href=mailto:support@computecanada.ca>support@computecanada.ca</a>
and notify if a version for non-default CPU architecture such as SSE3 is also necessary to build.</p><h3 id=an-example-r-code-with-dependencies-from-cc-cvmfs-stack>An example, R code with dependencies from CC CVMFS stack</h3><p>A real world example of using R on Grex, with several dependencies required for the R packages.</p><p>For the dynamic languages like R and Python, Compute Canada does not, in general, provide or manage pre-installed packages. Rather, users are expected to load the base R (Python, Perl, Julia) module and then proceed for the loacl installation of the required R (or Python, Perl, Julia etc.) packages in their home directories. Check the <a href=https://docs.computecanada.ca/wiki/R target=_blank rel=noopener>CC R documentation</a>
and <a href=https://docs.computecanada.ca/wiki/Python target=_blank rel=noopener>CC Python documentation</a>
.</p><blockquote class="book-hint slurm"><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:teal>#!/bin/bash
</span><span style=color:teal></span><span style=color:#080;font-style:italic>#SBATCH --ntasks=1</span>
<span style=color:#080;font-style:italic>#SBATCH --mem-per-cpu=4000M</span>
<span style=color:#080;font-style:italic>#SBATCH --time=0-72:00:00</span>
<span style=color:#080;font-style:italic>#SBATCH --job-name=&#34;R-gdal-jags-bench&#34;</span>
<span style=color:#080;font-style:italic># Load the modules:</span>
module load CCEnv
module load nixpkgs/16.09 gcc/5.4.0
module load r/3.5.2 jags/4.3.0 geos/3.6.1 gdal/2.2.1
export MKL_NUM_THREADS=<span style=color:#00f>1</span>
echo <span style=color:#00f>&#34;Starting run at: `date`&#34;</span>
R --vanilla &lt; Benchmark.R &amp;&gt; benchmark.<span style=color:#00f>${</span>SLURM_JOBID<span style=color:#00f>}</span>.txt
echo <span style=color:#00f>&#34;Program finished with exit code </span>$?<span style=color:#00f> at: `date`&#34;</span></code></pre></div></blockquote><h3 id=notes-on-mpi-based-software-from-cc-stack>Notes on MPI based software from CC Stack</h3><p>We recommend to use a recent ComputeCanada environment/toolchain that provides OpenMPI 3.1.x or later, which has a recent PMIx process management interface and supports UCX interconnect libraries that are used on Grex.
Earlier versions of OpenMPI might or might not work. With OpenMPI 3.1.x or 4.0.x, <code>srun</code> command should be used in SLURM job scripts on Grex.
Below is an example of MPI job (Intel benchmark) using the StdEnv/2018.3 toolchain (Intel 2018 / GCC 7.3.0 and OpenMPI 3.1.2).</p><blockquote class="book-hint slurm"><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:teal>#!/bin/bash
</span><span style=color:teal></span><span style=color:#080;font-style:italic>#SBATCH --ntasks-per-node=2 --nodes=2</span>
<span style=color:#080;font-style:italic>#SBATCH --mem-per-cpu=4000M</span>
<span style=color:#080;font-style:italic>#SBATCH --time=0-1:00:00</span>
<span style=color:#080;font-style:italic>#SBATCH --job-name=&#34;IMB-MPI1-4&#34;</span>
<span style=color:#080;font-style:italic># Load the modules:</span>
module load CCEnv
module load StdEnv/2018.3
module load imb/2019.3
module list
echo <span style=color:#00f>&#34;Starting run at: `date`&#34;</span>
srun IMB-MPI1 &gt; imb-ompi312-2x2.txt
echo <span style=color:#00f>&#34;Program finished with exit code </span>$?<span style=color:#00f> at: `date`&#34;</span></code></pre></div></blockquote><p>If the script above is saved into <em>imb.slurm</em>, it can be submitted as follows:</p><blockquote class="book-hint info"><code>sbatch imb.slurm</code></blockquote><h3 id=notes-on-code-development-with-cc-stack>Notes on Code development with CC Stack</h3><p>Because Compute Canada software stack can only distribute open source software to non-CC systems like Grex, proprietary/restricted software items are omitted. This means that Intel compiler modules, while providing their redistributable parts necessary to run the code compiled with them, will not work to compile new code on Grex. Thus, only GCC compilers and GCC-based toolchains from CC Stack are useful for the local code development on Grex.</p><h2 id=opensciencegrid>OpenScienceGrid</h2><p>On Grex, we mount OSG reporsitories, mainly for Singularity containers provided through OSG. Pointing the singularuty to the desired path under /cvmfs/singularity.opensciencegrid.org/ will automatically mount and fetch the required software items. Discovering them is up the the users. See more in our <a href=../containers/>Containers</a>
documentation page.</p></article><div class="book-footer justify-between"><div><a class="flex align-center" href=https://github.com/um-grex/grex-docs/commit/79e691641d6f80c9a601dd3cfe912635785b99af title="Last modified by Dr. Ali Kerrache | Feb 25, 2022" target=_blank><img src=/grex-docs/svg/calendar.svg class=book-icon alt=Calendar>
<span>Feb 25, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/um-grex/grex-docs/blob/master/content/docs/grex/software/cern-vmfs.md target=_blank><img src=/grex-docs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><br><hr></div><aside class="book-toc levels-4 fixed"><nav id=TableOfContents><ul><li><a href=#cern-vmfs-on-grex>Cern VMFS on Grex</a><ul><li><a href=#compute-canada-software-stack>Compute Canada software stack</a><ul><li><a href=#how-to-find-software-on-cc-cvmfs>How to find software on CC CVMFS</a></li><li><a href=#how-to-request-software-added-to-cc-cvmfs>How to request software added to CC CVMFS</a></li><li><a href=#an-example-r-code-with-dependencies-from-cc-cvmfs-stack>An example, R code with dependencies from CC CVMFS stack</a></li><li><a href=#notes-on-mpi-based-software-from-cc-stack>Notes on MPI based software from CC Stack</a></li><li><a href=#notes-on-code-development-with-cc-stack>Notes on Code development with CC Stack</a></li></ul></li><li><a href=#opensciencegrid>OpenScienceGrid</a></li></ul></li></ul></nav></aside></main></body></html>