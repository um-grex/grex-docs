<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Code Development on Grex"><meta property="og:title" content="Code Development on Grex"><meta property="og:description" content="Code Developing on Grex Grex comes with a sizeable software stack that contains most of the software development environment for typical HPC applications. This section of the documentation covers the best practices for compiling and building your own software on Grex.
The login nodes of Grex can be used to compile code and to run short interactive and/or test runs. All other jobs must be submitted to the batch system. We do not do as heavy resource limiting on Grex login nodes as, for example, Compute Canada does; so code development on login nodes is entirely possible."><meta property="og:type" content="article"><meta property="og:url" content="http://um-grex.github.io/grex-docs/docs/grex/software/code-development/"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2021-11-24T11:47:49-06:00"><title>Code Development on Grex | Unofficial Grex User Guide</title><link rel=icon href=/grex-docs/favicon.png type=image/x-icon><link rel=stylesheet href=/grex-docs/book.min.9e24c4795dd4fdd0e504b497cfd7198ef04d9fc59147fe183e68ca7912ee2901.css integrity="sha256-niTEeV3U/dDlBLSXz9cZjvBNn8WRR/4YPmjKeRLuKQE="><script defer src=/grex-docs/en.search.min.161f9cf51adf16bcba439124f9bb1f1cfbf7d5e0187b3fe400e258941d4be10b.js integrity="sha256-Fh+c9RrfFry6Q5Ek+bsfHPv31eAYez/kAOJYlB1L4Qs="></script></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><h2 class=book-brand><a href=http://um-grex.github.io/grex-docs><img src=/grex-docs/um_logo_email_signature.png alt=Logo><span>Unofficial Grex User Guide</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64><div class="book-search-spinner spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/>Notes of Grex Changes</a><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/training/>Training Materials and Presentations</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/grex-docs/docs/computecanada/>Accessing Compute Canada resources</a><ul></ul></li><li class=book-section-flat><a href=/grex-docs/docs/grex/>Grex HPC Documentation</a><ul><li><a href=/grex-docs/docs/grex/access/>Access and Usage conditions</a></li><li><a href=/grex-docs/docs/grex/connecting/>Connecting / Transferring data</a></li><li><a href=/grex-docs/docs/grex/data/>Storage and Data</a></li><li><a href=/grex-docs/docs/grex/running/>Running Jobs</a></li><li><a href=/grex-docs/docs/grex/ood/>Grex's OpenOnDemand Web Portal</a></li><li><a href=/grex-docs/docs/grex/software/>Software</a><ul><li><a href=/grex-docs/docs/grex/software/cern-vmfs/>CVMFS and ComputeCanada</a></li><li><a href=/grex-docs/docs/grex/software/general-linux/>General Linux tools</a></li><li><a href=/grex-docs/docs/grex/software/containers/>Containers for Software</a></li><li><a href=/grex-docs/docs/grex/software/code-development/ class=active>Code Development on Grex</a></li><li><span>Software-specific notes</span><ul><li><a href=/grex-docs/docs/grex/software/specific/gaussian/>Gaussian</a></li><li><a href=/grex-docs/docs/grex/software/specific/julia/>Julia</a></li><li><a href=/grex-docs/docs/grex/software/specific/matlab/>Matlab</a></li><li><a href=/grex-docs/docs/grex/software/specific/nwchem/>Nwchem</a></li><li><a href=/grex-docs/docs/grex/software/specific/orca/>Orca</a></li><li><a href=/grex-docs/docs/grex/software/specific/priroda/>Priroda</a></li><li><a href=/grex-docs/docs/grex/software/specific/vasp/>Vasp</a></li></ul></li></ul></li></ul></li><li><a href=/grex-docs/docs/faq/>Frequently Asked Questions</a></li><li><a href=/grex-docs/docs/localit/>Local IT Resources</a></li><li><a href=/grex-docs/docs/support-contacts/>Support and Training</a></li><li><a href=/grex-docs/docs/disclaimer/>Disclaimer</a></li></ul></nav><script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></aside><div class=book-page><header class="flex align-center justify-between book-header"><label for=menu-control><img src=/grex-docs/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Code Development on Grex</strong></header><article class=markdown><h1 id=code-developing-on-grex>Code Developing on Grex</h1><p>Grex comes with a sizeable software stack that contains most of the software development environment for typical HPC applications. This section of the documentation covers the best practices for compiling and building your own software on Grex.</p><p>The login nodes of Grex can be used to compile code and to run short interactive and/or test runs. All other jobs must be submitted to the batch system. We do not do as heavy resource limiting on Grex login nodes as, for example, Compute Canada does; so code development on login nodes is entirely possible. However, it might still make sense to perform some of the code development in interactive jobs, in cases of a) the build process and/or tests requires heavy, many-core computations and/or b) you need access to specific hardware not present on the login nodes, such as GPUs and AVX512 CPUs.</p><p>Most of the software on Grex is available through environmental modules. To find a software development tool or a library to build your code against, the <em>module spider</em> command is a good start. The applications software is usually installed by us from sources, into sub-directories under <em>/global/software/cent7</em></p><p>It is almost always better to use communication libraries (MPI) provided on Grex rather than building your own, because ensuring tight
integration of these libraries with our SLURM scheduler and low-level, interconnect-specific libraries might be tricky.</p><h2 id=general-centos-7-notes>General CentOS-7 notes</h2><p>The base operating system on Grex is CentOS 7.x that comes with its set of development tools. However, due to the philosophy of CentOS, the tools are usually rather old. For example, <em>cmake</em> and <em>git</em> are of ancient versions of 2.8, 1.7 correspondingly. Therefore, even for these basic tools you more likely want to load a module with newest verstions of these tools:</p><p><code>module load git</code></p><p><code>module load cmake</code></p><p>CentOS also has its system versions of Python, Perl, and GCC compilers. When no modules loaded, the binaries of these will be available in the PATH. The purpose of these is to make some systems scripts possible, to compile OS packages, drivers and so on.</p><p>We do not install many packages for the dynamic languages system-wide, because it makes maintaining different versions of them complicated. The same adivce applies: use the <strong>module spider</strong> command to find a version of Perl, Python, R, etc. to suit you. The same applies to compiler suites like GCC and Intel.</p><p>We do install CentOS packages with OS that are a) base OS things necessary for functioning, b) graphical libraries that have many dependencies, c) never change versions that are not critical for performance and/or security. Here are some examples: FLTK, libjpeg, PCRE, Qt4 and Gtk. Login nodes of Grex have many &lsquo;'-devel&rsquo;' packages installed, while compute nodes do not because we want them lean and quickly reinstallable. Therefore, compiling codes that requires &lsquo;'-devel&rsquo;' base OS packages might fail on compute nodes. Contact us us if something like taht happens when compiling or running your applications.</p><p>Finally, because HPC machines are shared systems and users do not have &lsquo;&lsquo;sudo&rsquo;&rsquo; access, following some instructions from a Web page that asks for &lsquo;&lsquo;apt-get install this '&rsquo; or &lsquo;&lsquo;yum install that&rsquo;&rsquo; will fail. Rather, <em>module spider</em> should be used to see if the package you want is already installed and available as a module.</p><h2 id=compilers-and-toolchains>Compilers and Toolchains</h2><p>Due to the hierarchical nature of our <strong>Lmod</strong> modules system, compilers and certain core libraries (MPI and CUDA) form toolchains. Normally, you would need to chose a compiler suite (Intel or GCC) and, in case of parallel applications, a MPI library (OpenMPI or IntelMPI). These come in different versions. Also, you&rsquo;d want to know if you want CUDA should your applications be able to utilize GPUs. A combination of compiler/version, MPI/version and possibly CUDA makes a toolchain. Toolchains are mutually exclusive; you cannot mix software items compiled with different toolchains!</p><p>See Modules for more information.</p><p><em>There is no modules loaded by default</em> ! There will be only system&rsquo;s GCC-4.8 and no MPI whatsoever. To get started, load a compiler/version. Then, if necessary, an MPI (ompi or impi) and if necessary, CUDA (for which 10.2 is the current version, there is no known reason to use another).</p><p><code>module load intel/2019.5</code></p><p><code>module load ompi/3.1.4</code></p><p>The above loads Intel compilers and OpenMPI 3.1.4. The example below is for GCC 7 and openmpi 4.1.2.</p><p><code>module load gcc/7.4</code></p><p><code>module load ompi/4.1.2</code></p><p>The MPI wrappers (<em>mpicc</em>, <em>mpicxx</em>, <em>mpif90</em>, &mldr; etc.) will be set correctly by <em>ompi</em> modules to point to the right compiler.</p><h3 id=intel-compilers-suite>Intel compilers suite</h3><p>At the moment of writing this documentation, the following Intel Compilers Suites are available on Grex:</p><ul><li>Intel 2019.5 : a recent Intel Parallel Studio suite. Most software is compiled with it, so use this one if unsure.</li><li>Intel 2020.4 : a recent Intel Parallel Studio suite.</li><li>Intel 2017.8 : a somewhat less recent Intel Parallel studio suite.</li><li>Intel 15.0 : Legacy, for maintenance of older Grex software. Do not use for anything new, unless absolutely must</li><li>Intel 12.1 : a very old one, for maintenance of a very old PETSc version. Do not use.</li><li>Intel 14.1 : a very old one, for maintenance of older Grex software, and broken. Do not use. It will be removed soon.</li></ul><p>The name for the Intel suite modules is <em>intel</em>; <code>module spider intel</code> is the command to find available Intel versions. The later is left for compatibility with legacy codes. It does not work with systems C++ standard libraries well, so <em>icpc</em> for Intel 12.1 might be dysfunctional. So the intel/12.1 toolchain is actually using GCC 4.8&rsquo;s C++ and C compilers.</p><p>If unsure, or do not have a special reason otherwise, use Intel 15.0 compilers (icc, icpc, ifort). Intel 15.0 is probably the first Intel compiler to support AVX512 if you are going to use the contributed nodes that have AVX512 architecture.</p><p>The Intel compilers suite also provides tools and libraries such as <a href=https://software.intel.com/en-us/mkl>MKL</a> (Linear Algebra, FFT, etc.), Intel Performance Primitives (IPP), Intel <a href=https://software.intel.com/en-us/tbb>Threads Building Blocks</a> (TBB), and <a href=https://software.intel.com/en-us/vtune>VTune</a>. Intel MPI as well as MKL for GCC compilers are available as separate modules, should they be needed for use separately.</p><h3 id=gcc-compilers-suite>GCC compilers suite</h3><p>At the moment of writing this page, the following GCC compilers are available:</p><ul><li>GCC 9.2</li><li>GCC 7.4</li><li>GCC 5.2</li><li>GCC 4.8</li></ul><p>The name for GCC is <em>gcc</em>, as in <code>module spider gcc</code>. The GCC 4.8 is a place holder module; its use is to unload any other modules of the compiler family, to avoid toolchains conflicts. Also, GCC 4.8 is the only multilib GCC compiler around; all the others are strictly <strong>64-bit</strong>, and thus unable to compile legacy <strong>32-bit</strong> programs.</p><p>For utilizing of the AVX512 instructions, probably the best way is to go with the latest GCC compilers (9.2 and 7.4) and latest MKL. GCC 4.8 does not handle AVX512. Generally Intel compilers outperform GCC, but GCC might have better support for the recent C++11,14,17 standards.</p><h2 id=mpi-and-interconnect-libraries>MPI and Interconnect libraries</h2><p>The standard distribution of MPI on Grex is <a href=https://www.open-mpi.org/>OpenMPI</a>. We build most of the software with it. To keep compatibility with the old Grex software stack, we name the modules <em>ompi</em>. MPI modules depend on the compiler they were built with, which means, that a compiler module should be loaded first; then the dependent MPI modules will become available as well. Changing the compiler module will trigger automatic MPI module reload. This is how Lmod hierarchy works now.</p><p>For a long time Grex was using the interconnect drivers with ibverbs packages from the IB hardware vendor, Mellanox. It is no longer the case: for CentOS-7, we have switched to the vanilla Linux infiniband drivers, the open source RDMA-core package, and OpenUCX libraries. The current version of UCX on Grex is 1.6.1. Recent versions of OpenMPI (3.1.x and 4.0.x) do support UCX. Also, our OpenMPI is built with process management interface versions PMI1, PMIx2 and 3, for tight integration with the SLURM scheduler.</p><p>The current default and recommended version of MPI is OpenMPI 4.1.1. OpenMPI 4.1 works well for new codes but could break old ones
There is an older version, OpenMPI 3.1.4 or 3.1.6 that is more compatible. A very old OpenMPI 1.6.5 exists for compatibily with older software.</p><p><code>module load ompi/3.1.4</code></p><p>There is also IntelMPI, for which the modules are named <em>impi</em>. See the notes on running MPI applications under SLURM <a href=../running/batch>here</a>.</p><p>All MPI modules, be that OpenMPI or Intel, will set MPI compiler wrappers such as <em>mpicc</em>, <em>mpicxx</em>, <em>mpif90</em> to the compiler suite they were built with. The typical workflow for building parallel programs with MPI would be to first load a compiler module, then an MPI module, and then use the wrapper of C, C++ or Fortran in your makefile or build script.</p><p>In case a build or configure script does not want to use the wrapper and needs explicit compiler and link options for MPI, OpenMPI wrappers provide the <em>&ndash;show</em> option that list the required command line options. Try for example:</p><p><code>mpicc --show</code></p><p>To print include and library flags to the C compiler to be linked against currently loaded OpenMPI.</p><h2 id=linear-algebra-blaslapack>Linear Algebra BLAS/LAPACK</h2><p>It is always a bad idea to use the reference BLAS/LAPACK/CLAPACK from Netlib (or the generic -lblas, -llapack from CentOS or EPEL which also likely is the reference BLAS/LAPACK from Netlib). The physical Computer architecture has much evolved, and is now way different from the logical Computer the human programmer is presented with. Todays, it takes careful, manual assembly coding optimization to implement BLAS/LAPACK that performs fast on modern CPUs with their memory hierarchies, instruction prefetching and speculative execution. A vendor-optimized BLAS/LAPACK implementation should always be used. For the Intel/AMD architectures, it is Intel MKL, OpenBLAS, and BLIS.</p><p>ALso, it is worth noting that the linear algebra libraries might come with two versions: one 32 bit array indexes, another of full 64 bit. Users must pay attention and link against the proper version for their software (that is, a Fortran codes with -i8 or -fdefault-integer-8 would link against 64 bit pointers BLAS).</p><h3 id=mkl>MKL</h3><p>The fastest BLAS/LAPACK implementation from Intel. With Intel compilers, it can be used as a convenient compiler flag, <em>-mkl</em> or if threaded version is not needed, <em>-mkl=sequential</em>.</p><p>With both Intel and GCC compilers, the MKL libraries can be linked explicitly with compiler/linker options. The base path for MKL includes and libraries is defined as the <em>MKLROOT</em> environment variable. For GCC compilers, <code>module load mkl</code> is needed to add <em>MKLROOT</em> to the environment. There is a command line <a href=https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor>advisor</a> Website to pick the correct order and libraries. Libraries with the _ilp64 suffix are for 64 bit indexes while _lp64 are for the default, 32 bit indexes.</p><p>Note that when Threaded MKL is used, the number of threads is controlled with <em>MKL_NUM_THREADS</em> environment variable. On Grex software stack, it is set by the MKL module to 1 to prevent accidental CPU over-subscription. Redefine it in your SLURM job scripts if you really need threaded MKL execution as follows:</p><p><code>export MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK</code></p><p>We use the MKL&rsquo;s BLAS and LAPACK for compiling R and Python&rsquo;s NumPY package on Grex, and thats one example when threaded MKL can speed up computations if the code spends significant time in linear algebra routines by using SMP.</p><p>Note that MKL also provides ScaLAPACK and FFTW libraries.</p><h3 id=openblas>OpenBLAS</h3><p>The successor and continuation of the famous GotoBLAS2 library. It contains both BLAS and LAPACK in a sigle library, <em>libopenblas.a</em> . Use &lsquo;&lsquo;module spider openblas&rsquo;&rsquo; to find available versions for a given compiler suite. We provide both 32 bit and 64 bit indexes versions (and reflect ot in the version names, like <em>openblas/0.3.7-i32</em>). The <a href=https://software.intel.com/en-us/articles/performance-comparison-of-openblas-and-intel-math-kernel-library-in-r>performance of OpenBLAS</a> is close to that of MKL.</p><h3 id=blis>BLIS</h3><p><a href=https://en.wikipedia.org/wiki/BLIS_(software)>Blis</a> is a recent, C++ template based implementation of linear algebra that contains a BLAS interface. On Grex, only 32 bit indexes BLIS is available at the moment. Use &lsquo;&lsquo;module spider blis&rsquo;&rsquo; to see how to load it.</p><h3 id=scalapack>ScaLAPACK</h3><p>MKL has ScaLAPACK included. Note that it depends on BLACS which in turn depends on an MPI version. MKL comes with support of OpenMPI and IntelMPI for the BLACS layer; it is necessary to pick the right library of them to link against.</p><p>The <a href=https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor>command line advisor</a> is helpful for that.</p><h2 id=fourier-transoform-fftw>Fourier Transoform FFTW</h2><p>FFTW3 is the standard and well performing implementation of FFT. <code>module spider fftw</code> should find it. There is parallel version of the FFTW3 that depends on MPI it uses, thus to load the <em>fftw</em> module, compiler and MPI modules would have to be loaded first. MKL also provides FFTW bindings, which can be used as follows:</p><p>Either of Intel or GCC MKL modules would set the <em>MKLROOT</em> environment variable, and add necessary directories to <em>LD_LIBRARY_PATH</em>. The <em>MKLROOT</em> is handy when using explicit linking against libraries. It can be useful if you want to select a particular compiler (Intel or GCC), pointer width (the corresponding libraries have suffix _lp64 for 32 bit pointers and_ilp64 for 64 bit ones; the later is needed for, for example, Fortran codes with INTEGER*8 array indexes, explicit or set by -i8 compiler option) and kind of MPI library to be used in BLACS (OpenMPI or IntelMPI which both are available on Grex). An example of the linker options to link against sequential, 64 bit pointed vesion of BLAS, LAPACK for an Intel Fortran code is:</p><p><code>ifort -O2 -i8 main.f -L$MKLROOT/lib/intel64 -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core -lpthread -lm</code></p><p>MKL also has FFTW bindings. They have to be enabled separately from the general Intel compilers installation; and therefore details of the usage might be different between different clusters. On Grex, these libraries are pesent in two versions: 32 bit pointers (libfftw3xf_intel_lp64) and 64 bit pointers (fftw3xf_intel_ilp64). To link against these FFT libraries, the following include and library options to the compilers can be used (for the _lp64 case):</p><p><code>-I$MKLROOT/include/fftw -I$MKLROOT/interfaces/fftw3xf -L$MKLROOT/interfaces/fftw3xf -lfftw3xf_intel_lp64</code></p><p>The above line is, admittedly, rather elaborate but give the benefit of compiling and building all of the code with MKL, without the need for maintaining a separtate library such as FFTW3.</p><h2 id=hdf5-and-netcdf>HDF5 and NetCDF</h2><p>Popular hierarchical data formats. Two versions exist on the Grex software stack, one serial and another MPI-dependent version. Which one you load depends whether MPI is loaded.
"
<code>module spider hdf5</code></p><p><code>module spider netcdf</code></p><h2 id=python>Python</h2><p>There are Conda python modules and the Python built from sources with variety of the compilers. The conda based modules can be distinguished by the module name. Note that the base OS python should in most cases not be used; rather use a module:</p><p><code>module spider python</code></p><p>We do install certain most popular python modules (such as Numpy, Scipy, matplotlib) centrally. <em>pip list</em> and <em>conda list</em> would show the instaled modules</p><h2 id=r>R</h2><p>We build R from sources and link against MKL. There are Intel 15 versions of R; however we find that some packages would only work with GCC-compiled versions of R because they assume GCC or rely on some C++11 features that the older Intel C++ might be lacking. To find available modules for R, use:</p><p><code>module spider "r"</code></p><p>Several R packages are installed with the R modules on Grex. Note that it is often the case that R packages ar bindings for some other software (JAGS, GEOS, etc.) and require the software or its dynamic libraries to be available at runtime. This means, the modules for the dependencies (JAGS, GEOS) are also to be loaded when R is loaded.</p></article><div class="book-footer justify-between"><div><a class="flex align-center" href=https://github.com/um-grex/grex-docs/commit/a6ed2f08882a068415cc2bf575b5bfa0ce90d72b title="Last modified by Shamov | Nov 24, 2021" target=_blank><img src=/grex-docs/svg/calendar.svg class=book-icon alt=Calendar>
<span>Nov 24, 2021</span></a></div><div><a class="flex align-center" href=https://github.com/um-grex/grex-docs/blob/master/content/docs/grex/software/code-development.md target=_blank><img src=/grex-docs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div></div><aside class="book-toc levels-4 fixed"><nav id=TableOfContents><ul><li><a href=#code-developing-on-grex>Code Developing on Grex</a><ul><li><a href=#general-centos-7-notes>General CentOS-7 notes</a></li><li><a href=#compilers-and-toolchains>Compilers and Toolchains</a><ul><li><a href=#intel-compilers-suite>Intel compilers suite</a></li><li><a href=#gcc-compilers-suite>GCC compilers suite</a></li></ul></li><li><a href=#mpi-and-interconnect-libraries>MPI and Interconnect libraries</a></li><li><a href=#linear-algebra-blaslapack>Linear Algebra BLAS/LAPACK</a><ul><li><a href=#mkl>MKL</a></li><li><a href=#openblas>OpenBLAS</a></li><li><a href=#blis>BLIS</a></li><li><a href=#scalapack>ScaLAPACK</a></li></ul></li><li><a href=#fourier-transoform-fftw>Fourier Transoform FFTW</a></li><li><a href=#hdf5-and-netcdf>HDF5 and NetCDF</a></li><li><a href=#python>Python</a></li><li><a href=#r>R</a></li></ul></li></ul></nav></aside></main></body></html>