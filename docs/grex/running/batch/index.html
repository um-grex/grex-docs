<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Batch jobs"><meta property="og:title" content="Batch jobs"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="http://um-grex.github.io/grex-docs/docs/grex/running/batch/"><title>Batch jobs | Unofficial Grex User Guide</title><link rel=icon href=/grex-docs/favicon.png type=image/x-icon><link rel=stylesheet href=/grex-docs/book.min.9e24c4795dd4fdd0e504b497cfd7198ef04d9fc59147fe183e68ca7912ee2901.css integrity="sha256-niTEeV3U/dDlBLSXz9cZjvBNn8WRR/4YPmjKeRLuKQE="><script defer src=/grex-docs/en.search.min.7c8d83c4c9aea6d1a57f52aaaf5ca7ca861c8d28672422202b98ce959e36811c.js integrity="sha256-fI2DxMmuptGlf1Kqr1ynyoYcjShnJCIgK5jOlZ42gRw="></script><link rel=alternate type=application/rss+xml href=http://um-grex.github.io/grex-docs/docs/grex/running/batch/index.xml title="Unofficial Grex User Guide"></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><h2 class=book-brand><a href=http://um-grex.github.io/grex-docs><img src=/grex-docs/um_logo_email_signature.png alt=Logo><span>Unofficial Grex User Guide</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64><div class="book-search-spinner spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/>Notes of Grex Changes</a><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/training/>Training Materials and Presentations</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/grex-docs/docs/computecanada/>Accessing Compute Canada resources</a><ul></ul></li><li class=book-section-flat><a href=/grex-docs/docs/grex/>Grex HPC Documentation</a><ul><li><a href=/grex-docs/docs/grex/access/>Access and Usage conditions</a></li><li><a href=/grex-docs/docs/grex/connecting/>Connecting / Transferring data</a></li><li><a href=/grex-docs/docs/grex/data/>Storage and Data</a></li><li><a href=/grex-docs/docs/grex/running/>Running Jobs</a><ul><li><a href=/grex-docs/docs/grex/running/batch/ class=active>Batch jobs</a><ul></ul></li><li><a href=/grex-docs/docs/grex/running/interactive/>Interactive work</a></li><li><a href=/grex-docs/docs/grex/running/contributed-systems/>Contributed systems</a></li></ul></li><li><a href=/grex-docs/docs/grex/software/>Software</a></li></ul></li><li><a href=/grex-docs/docs/faq/>Frequently Asked Questions</a></li><li><a href=/grex-docs/docs/localit/>Local IT Resources</a></li><li><a href=/grex-docs/docs/support-contacts/>Support and Training</a></li><li><a href=/grex-docs/docs/disclaimer/>Disclaimer</a></li></ul></nav><script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></aside><div class=book-page><header class="flex align-center justify-between book-header"><label for=menu-control><img src=/grex-docs/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Batch jobs</strong></header><article class=markdown><h1 id=batch-jobs>Batch jobs</h1><p>There are several good reasons for adopting batch mode for running jobs on any HPC facility. Traditional HPC continues using the mainframes tradition: despite an HPC system being a cluster of several compute nodes joined by an interconnect, the users' view of the system is that of a unity, a single large machine. A subset of the available resources of the HPC machine is provided to the users batch jobs.</p><p>The resource placements, usage monitoring and accounting are done via a special software, the HPC scheduler. This is an often under-appreciated automation that makes usage efficient and saves a lot of work on part of the user. However, using HPC is hard in a sense that users have to make effort in order to figure out what are the available resources on an HPC cluster and what is the efficient way of requesting them for their jobs to optimize time to soluition.</p><p>Resources (tractable resources in SLURM speak) are CPU time, memory, and GPU time. Generic resources can be software licenses, etc. Requesting resources is done via command line options to job submission commands <strong>sbatch</strong> and <strong>salloc</strong>, or via special comment lines (starting with #SBATCH) in job scripts. There are also options to control job placement such as partitions and QOSs.</p><p>There are default values for the resources which are taken when you do not specify the resource limit. Note that the default values are, as a rule, quite small: <strong>3 hours</strong> of walltime, <strong>256mb</strong> of memory per CPU. In most of the cases it is better to have an explicit request of an appropriate resource limit rather than using the default.</p><p>Asking for too many resources might be wasteful both in preventing others of using them and in making a longer queuing time. We ask our users to be fair and considerate and do not allow for deliberate waste (running serial jobs on more than one core).</p><p>There are certain scheduling policies in place to prevent the cluster from being swamped by a single user. See below:</p><p>In particular, the MAXPS / GrpRunMins limit disfavours asking for many CPU cores for long walltimes.</p><p>Notes and examples on typical kinds of jobs, resources they need and the syntax of requesting them in SLURM are listed below:</p><h2 id=batch-job-policies>Batch job policies</h2><p>The following policies are implemented on Grex:</p><ul><li>The default walltime is 3 hours.</li><li>The default amount of memory per processor (mem-per-cpu) is 256 mb. Memory limits are enforced, so an accurate estimate of memory resource (either in form of mem or mem-per-cpu) should be provided.</li><li>The maximum walltime is 14 days.</li><li>The maximum number of processor-minutes for all currently running jobs of a group without a RAC is 4M.</li><li>The maximum number of jobs that a user may have queued to run is 4000. The maximum size of an array job is 2000.</li><li>Users without a RAC award are allowed to simultaneously use up to 400 CPU cores per accounting group.</li><li>There are limits on number of GPUs that can be used on contributed hardware. TODO</li></ul><h2 id=batch-jobs-use-cases>Batch jobs use cases</h2><p>Any batch job is submitted with <strong>sbatch</strong> command. Batch jobs are usually shell (BASH, etc.) scripts wrapping around the invocation of a code. The comments on top of the script that start with <em>#SBATCH</em> are interpreted by the SLURM scheduler as options for resource requests:</p><ul><li><em>--ntasks=</em> : specifies number of tasks (MPI processes) per job.</li><li><em>--nodes=</em> : specifies number of nodes (servers) per job.</li><li><em>--ntasks-per-node=</em> : works with</li><li><em>--cpus-per-task=</em> : specifies number of threads per task.</li><li><em>--mem-per-cpu=</em> : specifies memory per task (or thread?)</li><li><em>--mem=</em> : specifies memory per node</li><li><em>--gpus=</em> : specifies number of GPUs per job. There are also <em>--gpus-per-XXX</em> and <em>--XXX-per-gpu</em></li><li><em>--time-</em> : specifies walltime in format DD-hh:mm</li><li><em>--qos=</em> : specifies a QOS by its name</li><li><em>--partition=</em> : specifies a partiton by its name.</li></ul><p>Assuming the name of myfile.slurm (the name or the extension does not matter, it can can be afile.job, otherjob.sh, etc.), a job is submitted with the command:</p><p><code>sbatch myfile.slurm</code></p><p>Refer to the official SLURM <a href=https://slurm.schedmd.com/documentation.html>documentation</a> and/or <strong>man sbatch</strong> for the available options. Below we provide examples for typical cases of SLURM jobs.</p><h2 id=serial-jobs>Serial Jobs</h2><p>The simplest kind of job is a serial job when one compute process runs in a sequential fashion. Naturally, this job can utilize only a sigle CPU core: large parallel supercomputers do not parallelize binary codes automatically. So the resources can be wall time and memory. SLURM has two ways of specifying the later: memory per cpu core (<em>--mem-per-cpu=</em>) and total memory per node (<em>--mem=</em>). It is more logical to use per-core memory always; except in case of the whole-node jobs when special value --mem=0 gives all the available memory per each node. If you have used Torque before, <em>--mem-per-cpu</em> is the same as <em>pmem</em>.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --time=0-0:30:00</span>
<span style=color:#75715e>#SBATCH --mem=2500M</span>
<span style=color:#75715e>#SBATCH --job-name=&#34;Serial-Job-Test&#34;</span>

<span style=color:#75715e># Script for running serial program: your_program</span>

echo <span style=color:#e6db74>&#34;Current working directory is `pwd`&#34;</span>

<span style=color:#75715e># Load modules if needed:</span>

echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>

./your_program

echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span></code></pre></div><p>An important special case of serial jobs is high-throughput computing: jobs are serial because they are too short to parallelize them, however there are many such jobs per research project. The case of embarassingly parallel computations like some of the Monte Carlo simulations are often High Throughput Computing (HTC).</p><ul><li>Serial jobs that have regularely named inputs and run more than a few minutes each best be specified as Array Jobs (see below).</li><li>Serial jobs that are great in numbers and run less than a few minutes each better be joined into a task farm running within a single larger job using tools like GLOST, GNU Parallel or a workflow engine like QDO.</li></ul><p>An example of GLOST job is under MPI jobs below.</p><h2 id=smp--threaded--single-node-jobs>SMP / threaded / Single Node jobs</h2><p>Another kind of jobis are threaded jobs or single-node parallel jobs. Often Symmetric Multiprocessing jobs, but not necessary as many kinds of concurency/parallelism exist and can be employed.</p><p>Popular examples are OpenMP, pthreads, Java codes, etc. Gaussian and PSI4 are SMP codes; threaded BLAS/LAPACK routines from MKL (inside NumPY) can utilize threads etc.</p><p>These jobs run concurrently and can utilize multiple CPU cores to give better computation speeds. Often these use some form of shared memory as an inter-process communication which limit them to use of the cpu cores within a single server/compute node.</p><p>Thus from the point of the job script and resources request, two parameters are important:</p><ul><li>asking always only a single node but several CPU cores on it per job.</li><li>configuring the code to use exactly the number of CPU cores allocated to the job to prevent waste or congestion of the resources.</li></ul><p>In SLURM (unlike the old Torque) it makes difference whether you ask for &lsquo;parallel tasks&rsquo; or &lsquo;threads (--cpus-per-task)&rsquo; ; threads should not be isolated from each other!</p><p>An enviromental variable <strong>$SLURM_CPUS_PER_TASK</strong> is set in the job, so you can set an appropriate parameter of your code to the same value.</p><p>For OpenMP, it would be done like:
<code>export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK</code></p><p>For MKL it is <em>MKL_NUM_THREADS</em>, for Julia -- JULIA_NUM_THREADS , for Java -Xfixme parameter.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span>
<span style=color:#75715e>#SBATCH --time=0-8:00:00</span>
<span style=color:#75715e>#SBATCH --mem=0</span>
<span style=color:#75715e>#SBATCH --nodes=1</span>
<span style=color:#75715e>#SBATCH --ntask-per-node=1</span>
<span style=color:#75715e>#SBATCH --cpus-per-task=12</span>
<span style=color:#75715e>#SBATCH --job-name=&#34;OMP-Job-Test&#34;</span>

<span style=color:#75715e># An example of an OpenMP threaded job that takes a whole Grex node for 8 hours. </span>

export OMP_NUM_THREADS<span style=color:#f92672>=</span>$SLURM_CPUS_PER_TASK

echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>

./your-openmp.x input.dat &gt; output.log <span style=color:#75715e># OpenMP codes are usually homegrown.</span>

echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span></code></pre></div><p>Note that the above example request whole node&rsquo;s memory with <em>--mem=0</em> because the node is allocated for it fully because of the CPUs anyways.</p><p>It is easier to use <em>--mem</em> for SMP jobs because the memory is shared between threads (i.e., the memory amount used does not change with the number of threads).</p><h2 id=gpu-jobs>GPU jobs</h2><p>The GPU jobs would usually be similar to SMP/threaded jobs, with the following differences:</p><ul><li>The GPU jobs should run on the nodes that have GPU hardware, which means you&rsquo;d want always to specify <strong>&ndash;partition=gpu</strong> or <strong>&ndash;partition=stamps-b</strong> .</li><li>SLURM on Grex uses so called &ldquo;GTRES&rdquo; plugin for scheduling GPU jobs, which means that a request in the formof <strong>&ndash;gpus=N</strong> or <strong>&ndash;gpus-per-node=N</strong> or <strong>&ndash;gpus-per-task=N</strong> is required. Note that both partitions have up to four GPU per node, so asking more than 4 GPUs per node, or per task, is nonsensical.</li></ul><p>How many GPUs to ask? Grex at the moment does not have GPU-direct MPI enabled, which means that most of the jobs would be single-node. The GPU nodes in either <strong>gpu</strong> (two nodes there, 32GB V100s) or <strong>stamps-b</strong> (three nodes, 16GB V100s) partition have 4 V100 GPUs, 32 Intel 52xx CPUs and 192GB of CPU memory. So, asking 1 to 4 GPUs, one node, and 6-8 CPUs per GPU with an appropriate amount of RAM (4-8GB) per job would be a good starting point.</p><p>Note that V100 is a fairly large GPU for most of the jobs, and to best utilize the GPU resources available on Grex it is best to start with single GPU and then try if the code actually is able to saturate it with load! Many codes cannot scale to utilize more than one GPU, and few codes can utilize more than two of them.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --ntasks=1 --cpus-per-task=6</span>
<span style=color:#75715e>#SBATCH --time=0-12:00:00 --mem-per-cpu=6000mb</span>
<span style=color:#75715e>#SBATCH --job-name=genomics-test</span>
<span style=color:#75715e>#SBATCH --gpus=1 --partition=stamps-b</span>
<span style=color:#75715e># adjust the resource requests above to your needs</span>

<span style=color:#75715e>#example of loading modules, CUDA</span>
module load gcc/4.8
module load cuda/10.2

export OMP_NUM_THREADS<span style=color:#f92672>=</span>$SLURM_CPUS_PER_TASK

nvidia-smi

guppy_basecaller -x auto --gpu_runners_per_device <span style=color:#ae81ff>6</span> -i Fast5 -s GuppyFast5 -c dna_r9.4.1_450bps_hac.cfg

echo <span style=color:#e6db74>&#34;all done&#34;</span> </code></pre></div><p>The above script (if called say, gpu.job) can be submitted with the usual command:</p><p><code>sbatch gpu.job</code></p><h2 id=distributed-massively-parallel-jobs>Distributed, massively parallel jobs</h2><p>Parallel jobs that can spawn multiple servers are the most scalable ones. GAMESS-US, SIESTA, NWChem, are the examples. Running many parallel tasks across more than one node requires some inter-node communication which as a rule is slower than shared memory within one server.</p><p>In HPC, high speed interconnect like Infiniband (IB) used on Grex and specialized RDMA-aware communication libraries make distributed parallel computational very scalable.</p><p>Most often (but not always), parallel programs build upon a low-level message passing library called MPI. Refer to the Software section for the informations of the parallel libraries on Grex.</p><p>The jobs must specify required layout of nodes/tasks (or nodes/tasks/threads, or even nodes/tasks/GPUs since hybrid MPI+OpenMP and MPI+GPU programs exist), amount of memory and walltime.</p><p>The layout to consider is a tradeoff between making program to work correctly and making scheduler&rsquo;s work easier.</p><p>A well written MPI software theoretically should not care how the tasks are distributed across how many physical compute nodes. Thus, SLURMs <em>--ntasks=</em> request (similar to the old Torque <em>procs=</em>) specified without <em>--nodes</em> would work and make scheduling easier.</p><p>A note on process starting: since MPI jobs are distributed, there should be a mechanism to start the compute processes across many nodes. And, come to think of it, the mechanism should know which nodes to use, and how many. Most modern MPI implementations &ldquo;tightly integrate&rdquo; with SLURM, so they will get this informaton automatically via a Process Management Interface (PMI).</p><p>OpenMPI on Grex is compiled against PMIx (3.x, 4.x) or PMI1 (1.6.5). So it is preferrable to use <em>srun</em> instead of <em>mpiexec</em> to kickstart the MPI proecesses, because <em>srun</em> would use PMI.</p><p>For Intel MPI, <em>srun</em> may not work, but PMI1 can be used with <em>mpiexec.hydra</em> by setting the following environmemt variable:</p><p><code>export I_PMI_LIBRARY=/opt/slurm/lib/libpmi.so</code></p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span>
<span style=color:#75715e>#SBATCH --time=0-8:00:00</span>
<span style=color:#75715e>#SBATCH --mem-per-cpu=2500M</span>
<span style=color:#75715e>#SBATCH --ntasks=32</span>
<span style=color:#75715e>#SBATCH --job-name=&#34;Espresso-Job&#34;</span>

<span style=color:#75715e># A example of an MPI parallel that takes 32 cores on Grex for 8 hours. </span>

<span style=color:#75715e># Load the modules:</span>

module load intel/15.0.5.223 ompi/3.1.4 espresso/6.3.1

echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>

srun pw.x -in MyFile.scf.in  &gt; Myfile.scf.log 

echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span></code></pre></div><p>However, in practice there are cases when layout should be more restrictive. If the software or script it runs form assumes equal distribution of processes per node, the request should be <em>&ndash;nodes=N &ndash;ntasks-per-node=M</em> (similar to old Torque&rsquo;s nodes=N:ppn=M). A similar case is MPMD codes (Like NWCHem or GAMESS or OpenMolcas) that have some of the processes doing computation and some communication functions, and therefore requiring at least two tasks running per each node.</p><p>For some codes, especially for large parallel jobs with intensive communication between tasks there can be performance differences due to memory and interconnect bandwidths, depending on whether the same number of parallel tasks is compacted on few nodes or spread across many of them. Find an example of the job below.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --time=0-8:00:00</span>
<span style=color:#75715e>#SBATCH --mem-per-cpu=4000M</span>
<span style=color:#75715e>#SBATCH --ntasks-per-node=8 --nodes=4</span>
<span style=color:#75715e>#SBATCH --job-name=&#34;NWchem-Job&#34;</span>

<span style=color:#75715e># An example of an MPI parallel that takes 32 cores </span>
<span style=color:#75715e># across 4 grex Grex nodes for 8 hours. </span>

module load intel/15.0.5.223 ompi/3.1.4 nwchem/6.8.1

<span style=color:#75715e># Uncomment/Change these in case you want to use custom basis sets</span>
NWCHEMROOT<span style=color:#f92672>=</span>/global/software/cent7/nwchem/6.8.1-intel15-ompi314
export NWCHEM_NWPW_LIBRARY<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>NWCHEMROOT<span style=color:#e6db74>}</span>/data/libraryps
export NWCHEM_BASIS_LIBRARY<span style=color:#f92672>={</span>NWCHEMROOT<span style=color:#f92672>}</span>/data/libraries

<span style=color:#75715e># In most cases SCRATCH_DIR would  be on local nodes scratch</span>
<span style=color:#75715e># While results are in the same directory</span>
export NWCHEM_SCRATCH_DIR<span style=color:#f92672>=</span>$TMPDIR
export NWCHEM_PERMANENT_DIR<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>pwd<span style=color:#e6db74>`</span>

<span style=color:#75715e># Optional memory setting; note that this one or the one in your code</span>
<span style=color:#75715e># must match the #SBATCH -l mem= value !</span>
export NWCHEM_MEMORY_TOTAL<span style=color:#f92672>=</span><span style=color:#ae81ff>1000000000</span> <span style=color:#75715e># 12000 MB, double precision words only</span>
export MKL_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>

echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>
srun nwchem  dft_feco5.nw &gt; dft_feco5.$SLURM_JOBID.log
echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span></code></pre></div><h3 id=intel-mpi>Intel MPI</h3><p>For applications using IntelMPI (impi modules on Grex, or Intel-MPI based software from Compute Canada CVMFS software stack), a few environment variables have to be set. The following link explains it: <a href=https://software.intel.com/en-us/articles/how-to-use-slurm-pmi-with-the-intel-mpi-library-for-linux>Using SLURM with PMI</a>.</p><p>The JLab documentation example shows an <a href=https://scicomp.jlab.org/docs/intelMPIJobs>example of SLURM script with IntelMPI</a>.</p><h3 id=other-mpis>Other MPIs</h3><p>Finally, some canned codes like ANSYS or StatCCM+ would use a vendor-specific MPI implementation that would not tightly integrate with our scheduler&rsquo;s process to CPU core placement. In that case, several whole nodes (that is, with number of tasks equal to the node&rsquo;s number of CPU cores) should be requested to prevent the impact on other jobs with resource congestion.</p><p>Such codes will also require a nodelist (machinefile) file obtained from SLURM and provided to them in their own format.</p><p>ComputeCanada&rsquo;s <strong>slurm_hl2hl.py</strong> script makes this easier (see <a href=https://docs.computecanada.ca/wiki/Star-CCM%2B>CC StarCCM+</a> or <a href=https://docs.computecanada.ca/wiki/ANSYS#Cluster_Batch_Job_Submission>CC ANSYS documentation</a>).</p><p><code>slurm_hl2hl.py --format STAR-CCM+ > machinefile</code></p><h2 id=array-jobs>Array jobs</h2><p>Array jobs allow for submitting many similar jobs &ldquo;in one blow&rdquo;. It saves users work on job sumbission, and also makes SLURM scheduler more efficient in scheduling the array jobs because it would know they are the same with respect to size, expected walltime etc.</p><p>Array jobs work most naturally when a single code has to be applied to a large number of input files that are reguraly named, for example as: test1.in, test2.in, &mldr; test99.in</p><p>Then, a single job script with <em>#SBATCH --array=1,99</em> can be used to submit the 99 jobs.</p><p>In order to distinguish between the input files, within each of the jobs at run time, you would have to obtain a value for the array index. Which is set by SLURM as $SLURM_ARRAY_TASK_ID environment variable. The call to the code on a particular input will then be like:</p><p><code>./my_code test${SLURM_ARRAY_TASK_ID}.in</code></p><p>This way each of the array element jobs can distinguish their own portion of the work to do. A real life example is below; it attempts to run all of the Gaussian standard tests which have names of the format test0001.com, test0002.com, .. test1204.com, etc. Note the <em>printf</em> trick to deal with trailing zeroes in the input names.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --cpus-per-task=1</span>
<span style=color:#75715e>#SBATCH --mem=1000MB</span>
<span style=color:#75715e>#SBATCH --job-name=&#34;G16-tests&#34;</span>
<span style=color:#75715e>#SBATCH --array=1-1204</span>

echo <span style=color:#e6db74>&#34;Current working directory is `pwd`&#34;</span>
echo <span style=color:#e6db74>&#34;Running on `hostname`&#34;</span>
echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>

<span style=color:#75715e>## Set up the Gaussian environment using the module command:</span>

module load gaussian/g16.c01

<span style=color:#75715e>## Run g16 on an array job element</span>

id<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>printf <span style=color:#e6db74>&#34;%04d&#34;</span> $SLURM_ARRAY_TASK_ID<span style=color:#e6db74>`</span>
v<span style=color:#f92672>=</span>test<span style=color:#e6db74>${</span>id<span style=color:#e6db74>}</span>.com

w<span style=color:#f92672>=</span><span style=color:#e6db74>`</span>basename $v .com<span style=color:#e6db74>`</span>

g16 &lt; $v &gt; <span style=color:#e6db74>${</span>w<span style=color:#e6db74>}</span>.<span style=color:#e6db74>${</span>SLURM_JOBID<span style=color:#e6db74>}</span>.out

echo <span style=color:#e6db74>&#34;Job finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span></code></pre></div><p>There are limits on how large array jobs can be (see our scheduling policies): the maximal number of elements in job array, as well as the maximal number of jobs that can be submitted by a user.</p><h2 id=using-cc-cvmfs-software>Using CC CVMFS software</h2><p>As explained in more detail in the software/Modules documentation, we provide Compute Canada&rsquo;s software environment. Most of it can run out of the box by just specifying corresponding module.</p><p>There are some <em>caveats</em>.</p><ul><li><p>Some of the Compute Canada software might have hardcoded environmental variables that exists only on these systems. An example being SLURM_TMPDIR. On Grex, add <em>export SLURM_TMPDIR=$TMPDIR</em> to your job scripts.</p></li><li><p>In general, it is hard to containerize HPC. So the software that requires low-lewel hardware/device drivers access (OpenMPI, CUDA) may have problems when running on non-CC systems. Newer version of OpenMPI (3.1.x) seems to be more portable for using the PMIx job starting mechanism.</p></li><li><p>&ldquo;Restricted&rdquo; (commercial) software&rsquo;s binaries are not distributed by Compute Canada CVMFS due to the obvious licensing issues. It has to be installed locally on Grex.</p></li></ul><p>Having said that, <strong>module load CCEnv</strong> gives the right software environment to be run on Grex for a vast majority of threaded and serial software items from CC softvare stack. See discussion about MPI-parallel jobs below.</p><p>Because of the distributed nature of CVMFS, it might take time to download a program or library or data file. It would probably make sense to first access it interactively or from an interactive job to warm the CVMFS local cache, to avoid job failures due to the delay.</p><p>Below is an example of an R serial job that uses quite a few packages from Compute Canada software stack.</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --ntasks=1</span>
<span style=color:#75715e>#SBATCH --mem-per-cpu=4000M</span>
<span style=color:#75715e>#SBATCH --time=0-72:00:00</span>
<span style=color:#75715e>#SBATCH --job-name=&#34;R-gdal-jags-bench&#34;</span>
 
cd $SLURM_SUBMIT_DIR
               
<span style=color:#75715e># Load the modules:</span>
               
module load CCEnv
module load nixpkgs/16.09 gcc/5.4.0
module load r/3.5.2 jags/4.3.0 geos/3.6.1 gdal/2.2.1
  
export MKL_NUM_THREADS<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> 

echo <span style=color:#e6db74>&#34;Starting run at: `date`&#34;</span>

R --vanilla &lt; Benchmark.R &amp;&gt; benchmark.<span style=color:#e6db74>${</span>SLURM_JOBID<span style=color:#e6db74>}</span>.txt

echo <span style=color:#e6db74>&#34;Program finished with exit code </span>$?<span style=color:#e6db74> at: `date`&#34;</span></code></pre></div><p>Users of contributed systems which are newer than the original Grex nodes might want to switch to <em>arch/avx2</em> or <em>arch/avx512</em> from the default <em>arch/sse3</em>.</p><h3 id=using-cc-cvmfs-software-that-is-mpi-based>Using CC CVMFS software that is MPI-based.</h3><p>We have found that the recent ComputeCanada toolchains that use OpenMPI 3.1.x work on Grex without any changes (that is, with <code>srun</code>).
Therefore for OpenMPI based applications, we recommended to load ComputeCanada&rsquo;s software that depends on the recent toolchains, 2018.3 or later (Intelr 2018 compilers, GCC 7.3 compilers and openmpi/3.1.2) .
For example, the module commands below would load the Intel/OpenMPI 3.1.2 toochain-based environment:</p><p><code>module load CCEnv</code></p><p><code>module load StdEnv/2018.3</code></p><p>Below is an arbitrarily chosen IMB benchmark result for MPI1 on Grex, the <em>sendrecv</em> tests using two
processes on two nodes with several MPI implementations (CC means MPI coming from the ComputeCanada stack, Grex means compiled locally on Grex).</p><p><img src=/doc/mpis-on-grex.png alt></p><p>You can see that differences in performance between OpenMPI 3.1.x from CC stack and Grex are minor for this benchmark, even vithout attemting
at any local tuning for the CC OpenMPI.</p></article><div class="book-footer justify-between"><div><a class="flex align-center" href=https://github.com/gshamov/site/commit/f42cd442db68ea019509b0aa26c742457dd13f71 title="Last modified by Shamov | Jun 29, 2021" target=_blank><img src=/grex-docs/svg/calendar.svg class=book-icon alt=Calendar>
<span>Jun 29, 2021</span></a></div><div><a class="flex align-center" href=https://github.com/gshamov/site/blob/master/content/docs/grex/running/batch/_index.md target=_blank><img src=/grex-docs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div></div><aside class="book-toc levels-4 fixed"><nav id=TableOfContents><ul><li><a href=#batch-jobs>Batch jobs</a><ul><li><a href=#batch-job-policies>Batch job policies</a></li><li><a href=#batch-jobs-use-cases>Batch jobs use cases</a></li><li><a href=#serial-jobs>Serial Jobs</a></li><li><a href=#smp--threaded--single-node-jobs>SMP / threaded / Single Node jobs</a></li><li><a href=#gpu-jobs>GPU jobs</a></li><li><a href=#distributed-massively-parallel-jobs>Distributed, massively parallel jobs</a><ul><li><a href=#intel-mpi>Intel MPI</a></li><li><a href=#other-mpis>Other MPIs</a></li></ul></li><li><a href=#array-jobs>Array jobs</a></li><li><a href=#using-cc-cvmfs-software>Using CC CVMFS software</a><ul><li><a href=#using-cc-cvmfs-software-that-is-mpi-based>Using CC CVMFS software that is MPI-based.</a></li></ul></li></ul></li></ul></nav></aside></main></body></html>