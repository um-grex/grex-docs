<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Running Jobs"><meta property="og:title" content="Running Jobs"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="http://um-grex.github.io/grex-docs/docs/grex/running/"><title>Running Jobs | Unofficial Grex User Guide</title><link rel=icon href=/grex-docs/favicon.png type=image/x-icon><link rel=stylesheet href=/grex-docs/book.min.06cbd313f49ddc884804421299d5dc11b1cd097fdcfed7f054a79a137890a2d7.css integrity="sha256-BsvTE/Sd3IhIBEISmdXcEbHNCX/c/tfwVKeaE3iQotc="><script defer src=/grex-docs/en.search.min.e84623165fc23d56e58be97ca54226e75a1ca592d71e4ec8e22a2575fc82bd02.js integrity="sha256-6EYjFl/CPVbli+l8pUIm51ocpZLXHk7I4ioldfyCvQI="></script><link rel=alternate type=application/rss+xml href=http://um-grex.github.io/grex-docs/docs/grex/running/index.xml title="Unofficial Grex User Guide"></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><style>hr{border:0;height:3px;background-image:linear-gradient(to right,transparent,#095484,transparent)}</style><h2 class=book-brand><a href=http://um-grex.github.io/grex-docs><img src=/grex-docs/logo/um_logo_email_signature.png style=width:auto;height:auto alt=Logo><br><hr><span>Unofficial Grex User Guide</span><hr></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64><div class="book-search-spinner spinner hidden"></div><ul id=book-search-results></ul></div><hr><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/>Notes of Grex Changes</a><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/training/>Training Materials and Presentations</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/grex-docs/docs/computecanada/>Accessing Compute Canada resources</a><ul></ul></li><li class=book-section-flat><a href=/grex-docs/docs/grex/>Grex HPC QuickStart</a><ul><li><a href=/grex-docs/docs/grex/access/>Access and Usage conditions</a></li><li><a href=/grex-docs/docs/grex/connecting/>Connecting / Transferring data</a></li><li><a href=/grex-docs/docs/grex/data/>Storage and Data</a></li><li><a href=/grex-docs/docs/grex/running/ class=active>Running Jobs</a><ul><li><a href=/grex-docs/docs/grex/running/batch/>Batch jobs</a></li><li><a href=/grex-docs/docs/grex/running/interactive/>Interactive work</a></li><li><a href=/grex-docs/docs/grex/running/contributed-systems/>Contributed systems</a></li></ul></li><li><a href=/grex-docs/docs/grex/ood/>Grex's OpenOnDemand Web Portal</a></li><li><a href=/grex-docs/docs/grex/software/>Software</a></li></ul></li><li><a href=/grex-docs/docs/faq/>Frequently Asked Questions</a></li><li><a href=/grex-docs/docs/localit/>Local IT Resources</a></li><li><a href=/grex-docs/docs/support-contacts/>Support and Training</a></li><li><a href=/grex-docs/docs/disclaimer/>Disclaimer</a></li></ul></nav><script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></aside><div class=book-page><article class=markdown><h1 id=running-jobs>Running Jobs</h1><h2 id=why-running-jobs-in-batch-mode>Why running jobs in batch mode?</h2><p>There is a number of reasons for adopting a batch mode for running jobs on a cluster. From providing user&rsquo;s computations with fairness, traffic control to prevent resource congestion and resource trashing, enforcing organizational priorities, to better understanding the workload, utilization and resource needs for future capacity planning; the scheduler provides it all. After being long-time PBS/Moab users, we have switched to the SLURM batch system since <strong>December 2019</strong>.</p><h2 id=partitions>Partitions</h2><p>The current Grex system that has contributed nodes, large memory nodes and contributed GPU nodes is (and getting more and more) heterogeneous. With SLURM as a scheduler, this requires partitioning: a &ldquo;partition&rdquo; is a set of compute nodes, groupped by a characteristic, usually by kind of hardware the nodes have, and sometimes by who &ldquo;owns&rdquo; the hardware as well.</p><p>There is no fully automatic selection of partitions, other than the default <strong>skylake</strong> for most of the users, and <strong>compute</strong> for the short jobs. For the contributors' group members, the default partition will be their contributed nodes. <strong>Thus in many cases users have to specify the partition manually when submitting their jobs!</strong></p><p>Currently, the following partitions are available on Grex:</p><h3 id=-general-purpose-partitions>* <strong>General purpose partitions:</strong></h3><blockquote class="book-hint slurm"><ul><li><strong>skylake</strong> : the new <strong>52-core</strong>, CascadeLakeRefresh compute nodes, 96 Gb/node (set as the default partition). <strong>NEW</strong></li><li><strong>largemem</strong> : the new <strong>40-core</strong>, CascadeLake compute nodes, 384 Gb/node. <strong>NEW</strong></li><li><strong>compute</strong> : the original SSE4.2 <strong>12-core</strong> Grex nodes, RAM 48 Gb/node (no longer set as the default partition for jobs over 30 minutes).</li><li><strong>gpu</strong> : two GPU <strong>V100/32 GB</strong> AVX512 nodes, RAM 192 GB/node. <strong>NEW</strong></li><li><strong>test</strong> : a <strong>24-core</strong> Skylake CPU Dell large memory (512 GB), NVMe workstation for interactive work and visualizations. <strong>NEW</strong></li></ul></blockquote><h3 id=-contributed-partitions>* <strong>Contributed partitions:</strong></h3><blockquote class="book-hint slurm"><ul><li><strong>stamps</strong> : three <strong>4 x GPU v100/16GB</strong> AVX512 nodes contributed by Prof. R. Stamps (Department of Physics and Astronomy).</li><li><strong>livi</strong> : a <strong>HGX-2 16xGPU V100/32GB</strong>, NVSwitch server contributed by Prof. L. Livi (Department of Computer Science).</li><li><strong>agro</strong> : two <strong>24-core</strong> AMD Zen, RAM 256 GB/node, two NVIDIA A30 GPUs per node, contributed by Faculty of Agriculture.</li></ul></blockquote><h3 id=-preemptible-partitions>* <strong>Preemptible partitions:</strong></h3><blockquote class="book-hint slurm"><ul><li><strong>stamps-b</strong> : Preemptible partition for general use of the above nodes contributed by Prof. R. Stamps.</li><li><strong>livi-b</strong> : Preemptible partition for general use of the above nodes contributed by Prof. L. Livi.</li><li><strong>agro-b</strong> : Preemptible partition for general use of the above nodes contributed by Faculty of Agriculture.</li></ul></blockquote><p>The former five partitions (<strong>skyake</strong>, <strong>compute</strong>, <strong>largemem</strong>, <strong>test</strong> and <strong>gpu</strong>) are generally accessible. The next three are open only to the contributor&rsquo;s groups.</p><p>On the contributed partitions, the owner&rsquo;s group has preferencial access. However, users belonging to other groups can submit jobs to one of the preemptible partitions (ending with <strong>-b</strong>) to run on the contributed hardware as long as it is unused, on the condition that their jobs can be preempted (that is, killed) should owners jobs need the hardware. There is a minimum runtime guaranteed to preemptible jobs, which is as of now 1 hour. The maximum walltime for the preemptible partition is set per partition (and can be seen in the output of the <strong>sinfo</strong> command). To have a global overview of all partitions on Grex, run the custom script <strong>partition-list</strong> from your terminal.</p><p>On the special partition <strong>test</strong>, oversubcsription is enabled in SLURM, to facilitate better turnaround of interactive jobs.</p><p>Jobs cannot run on several partitions at the same time; but it is possible to specify more than one partiton, like in <strong>--partition=compute,skylake</strong>, so that the job will be directed by the scheduler to the first partiton available.</p><p>Jobs will be rejected by the SLURM scheduler if partition&rsquo;s hardware and requested resources do not match (that is, asking for GPUs on compute, largeme or skylake partitions is not possible). So in some cases, explicitly adding <strong>--partition=</strong> flag to SLURM job submission is needed.</p><p>Jobs that require <strong>stamps-b</strong> or <strong>gpu</strong> partitons have to use GPUs, otherwise they will be rejected; this is to prevent of bogging up the precious GPU nodes with CPU-only jobs!</p><h2 id=accounts>Accounts</h2><p>Users belong to &ldquo;accounting groups&rdquo;, led by their principal investigators (PIs). The accounting grops on Grex match CCDB roles. By default, a user&rsquo;s jobs are assigned to his primary/default accounting group. But it is possible for a user to belong to more than one group; then the <strong>--account=</strong> parameter can be used for <strong>sbatch</strong> or <strong>salloc</strong> to select non-default account to run jobs under. For example, a use who belongs to two accounting groups: <strong>def-sponsor1</strong> and <strong>def-sponsor2</strong>, can specify which one to use:</p><blockquote class="book-hint slurm">#SBATCH &ndash;account=def-sponsor1</blockquote><p>or<blockquote class="book-hint slurm">#SBATCH &ndash;account=def-sponsor2</blockquote></p><h2 id=qoss>QOSs</h2><p>QOS stands for Quality of Service. It is a mechanism to modify scheduler&rsquo;s limits, hardware access policies and modify job priorities and job accounting/billing. Presently, QOS might be used by our Scheduler machinery internally, but not specified by the users. Jobs that specify explicit <strong>--qos=</strong> will be rejected by the SLURM job submission wrapper.</p><h2 id=limits-and-policies>Limits and policies</h2><p>In order to prevent monopolization of the entire cluster by a single user or single accountig group, we enforce a MAXPS like limit; for non-RAC accounts it is set to 4 M CPU-minutes and 400 CPU cores. Accounts that have allocation (RAC) on Grex get higher MAXPS and max CPU cores limits in order to let them to utilize their usage targets.</p><p>Partitions for preemptible jobs running on contributed hardware might be further limited, so that they can not occupy the whole contributed hardware.</p><p>In cases when Grex is underutilized, but some jobs exist in the queue that can be run if not for the above mentioned limits, we might relax the limits as a temporary &ldquo;bonus&rdquo;.</p><h2 id=slurm-commands>SLURM commands</h2><p>Naturally, SLURM provides a command line user interface. Some of the most useful commands are listed below.</p><h3 id=exploring-the-system>Exploring the system</h3><p>The SLURM command that shows state of nodes and partitions is <strong>sinfo</strong>:</p><p><strong>Examples:</strong></p><blockquote class="book-hint slurm"><ul><li><strong>sinfo</strong>: to list all the nodes (idle, down, allocated, mixed) and partitions.</li><li><strong>sinfo &ndash;state=idle</strong>: to lisl all idle nodes.</li><li><strong>sinfo -p compute</strong>: to list information about partition (<strong>compute</strong> in this case).</li><li><strong>sinfo -p skylake &ndash;state=idle</strong>: to list idle nodes on a given partition (<strong>skylake</strong> in this case).</li></ul></blockquote><h3 id=submitting-jobs>Submitting jobs</h3><p>Batch jobs are submitted as follow:</p><blockquote class="book-hint slurm"><code>sbatch [options] myfile.job</code></blockquote><p>Interactive jobs are submitted in exactly same way, but they do not need the job script because they will give you an interactive session:</p><p><code>salloc [options]</code></p><blockquote class="book-hint info">The command <em>sbatch</em> returns a number called JobID and used by SLURM to identify the job in the queuing system.</blockquote><p>The options are used to specify resources (wall time, tractable resources such as cores and nodes and GPUs) and accounts and QOS and partitions under which the jobs should run, and various other options like specifying whether jobs need X11 GUI (<strong>--x11</strong>), where its output should go, whether email should be sent when job changes its states and so on. Here is a list of the most frequent options to <strong>sbatch</strong> command:</p><p>Resources (tractable resources in SLURMspeak) are CPU time, memory, and GPU time.</p><p>Generic resources can be software licenses, etc. There are also options to control job placement such as partitions and QOSs.</p><blockquote class="book-hint info"><ul><li><strong>--ntasks=</strong>: specifies number of tasks (MPI processes) per job.</li><li><strong>--nodes=</strong>: specifies number of nodes (servers) per job.</li><li><strong>--ntasks-per-node=</strong>: specifies number of tasks (MPI processes) per node.</li><li><strong>--cpus-per-task=</strong>: specifies number of threads per task.</li><li><strong>--mem-per-cpu=</strong>: specifies memory per task (or thread?)</li><li><strong>--mem=</strong>: specifies the memory per node.</li><li><strong>--gpus=</strong>: specifies number of GPUs per job. There are also <strong>--gpus-per-XXX</strong> and <strong>--XXX-per-gpu</strong></li><li><strong>--time-</strong>: specifies walltime in format DD-HH:MM</li><li><strong>--qos=</strong>: specifies a QOS by its name (<strong>Should not be used on Grex!</strong>)</li><li><strong>--partition=</strong>: specifies a partiton by its name (<strong>Can be very useful on Grex!</strong>)</li></ul></blockquote><p>An example of using some of these options with <em>sbatch</em> and <em>salloc</em> are listed below:</p><blockquote class="book-hint info"><ul><li><p><code>sbatch --nodes=1 --ntasks-per-node=1 --cpus-per-task=12 --mem=40gb --time=0-48:00 gaussian.job</code></p></li><li><p><code>salloc --nodes=1 --ntasks-per-node=4 --mem-per-cpu=4000mb --x11 --partition=compute</code></p></li></ul></blockquote><p>And so on. The options for batch jobs can be either in command line, or (perhaps better) in the special comments in the job file, like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic>#SBATCH -\-mem=40gb</span></code></pre></div><p>Refer to the subsection for <a href=/grex-docs/docs/grex/running/batch/>Batch jobs</a>
and <a href=/grex-docs/docs/grex/running/interactive/>Interacive jobs</a>
for more information, examples of job scripts and how to actually sumbit jobs.</p><h3 id=monitoring-jobs>Monitoring jobs</h3><p><strong>Checking on the queued jobs:</strong></p><blockquote class="book-hint info"><ul><li><code>squeue -u someuser</code> (to see all queued jobs of the user <strong>someuser</strong>)</li><li><code>squeue -u someuser -t R</code> (to see all queued and running jobs of the user <strong>someuser</strong>)</li><li><code>squeue -u someuser -t PD</code> (to see all queued and pending jobs of the user <strong>someuser</strong>)</li><li><code>squeue -A def-sponsor1</code> (to see all queued jobs for the accounting group <strong>def-sponsor1</strong>)</li></ul></blockquote><p>Without the above parameters, squeue would return all the jobs in the system. There is a shortcut &lsquo;&lsquo;sq&rsquo;&rsquo; for &lsquo;&lsquo;squeue -u $USER&rsquo;&rsquo;</p><p><strong>Cancelling jobs:</strong></p><blockquote class="book-hint info"><ul><li><code>scancel JobID</code> (to cancel a job JobID)</li><li><code>echo "Deleting all the jobs by $USER" && scancel -u $USER</code> (to cancel all your queued jobs at once).</li><li><code>echo "Deleting all the pending jobs by $USER" && scancel -u $USER --state=pending</code> (to cancel all your pending jobs at once).</li></ul></blockquote><p><strong>Hold and release queued jobs:</strong></p><p>To put hold some one or more jobs, use:</p><blockquote class="book-hint info"><ul><li><code>scontrol hold JobID</code> (to put on hold the job JobID).</li><li><code>scontrol hold JobID01,JobID02,JobID03</code> (to put on hold the jobs JobID01,JobID02,JobID03).</li></ul></blockquote><p>To release them, use:</p><blockquote class="book-hint info"><ul><li><code>scontrol release JobID</code> (to release the job JobID).</li><li><code>scontrol release JobID01,JobID02,JobID03</code> (to release the jobs JobID01,JobID02,JobID03).</li></ul></blockquote><p><strong>Checking job efficiency:</strong></p><p>The command <strong>seff</strong> is a wrapper around the command <strong>sacct</strong> ang gives a friendly output, like the actual utilization of walltime and memory:</p><blockquote class="book-hint info"><ul><li><code>seff JobID</code></li><li><code>seff -d JobID</code></li></ul></blockquote><p>Note that the output from seff command is not accurate if the job was not successful.</p><p><strong>Checking resource limits and usage for past and current jobs:</strong></p><blockquote class="book-hint info"><ul><li><code>sacct -j {JobID} -l</code></li><li><code>sacct -u $USER -s {STARTDATE} -e {ENDDATE} -l --parsable2</code></li></ul></blockquote><h3 id=getting-info-on-accounts-and-priorities>Getting info on accounts and priorities</h3><p>Fairshare and accounting information for the accounting group <strong>abc-12-aa</strong>:</p><blockquote class="book-hint slurm"><ul><li><code>sshare -l -A abc-12-aa</code></li><li><code>sshare -l -A abc-12-aa --format="Account,EffectvUsage,LevelFS"</code></li><li><code>sshare -a -l -A abc-12-aa</code></li><li><code>sshare -a -l -A abc-12-aa --format="Account,User,EffectvUsage,LevelFS"</code></li></ul></blockquote><p>Fairshare and accounting information for a user:</p><blockquote class="book-hint slurm"><code>sshare -l -U -u $USER</code></blockquote><p>Limits and settings for an account:</p><blockquote class="book-hint slurm"><code>sacctmgr list assoc account=abc-12-aa format=account,user,qos</code></blockquote><h2 id=useful-links>Useful links</h2><ul><li>SLURM <a href=https://westgrid.github.io/trainingMaterials/tools/scheduling/ target=_blank rel=noopener>documentation</a></li><li><a href=https://docs.computecanada.ca/wiki/Running_jobs target=_blank rel=noopener>Running jobs</a>
on Compute Canada clusters.</li><li>References for migrating from PBS to SLURM: <a href=https://www.ichec.ie/academic/national-hpc/kay-documentation/pbs-slurm target=_blank rel=noopener>ICHEC</a>
, <a href=https://hpcc.usc.edu/support/documentation/pbs-to-slurm/ target=_blank rel=noopener>HPC-USC</a></li><li>Westgrid training materials on SLURM: <a href=https://westgrid.github.io/trainingMaterials/tools/scheduling/ target=_blank rel=noopener>Scheduling</a></li></ul><p>Since the HPC technology is widely used by most of universities and National labs, simple googling your SLURM question will likely return a few useful links to their HPC/ARC documentation.</p></article><div class="book-footer justify-between"><div><a class="flex align-center" href=https://github.com/um-grex/grex-docs/commit/4a07d74120d95d4386c1c274f941df41cb1cc57e title="Last modified by Shamov | Feb 16, 2022" target=_blank><img src=/grex-docs/svg/calendar.svg class=book-icon alt=Calendar>
<span>Feb 16, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/um-grex/grex-docs/blob/master/content/docs/grex/running/_index.md target=_blank><img src=/grex-docs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><br><hr></div><aside class="book-toc levels-4 fixed"><nav id=TableOfContents><ul><li><a href=#running-jobs>Running Jobs</a><ul><li><a href=#why-running-jobs-in-batch-mode>Why running jobs in batch mode?</a></li><li><a href=#partitions>Partitions</a><ul><li><a href=#-general-purpose-partitions>* <strong>General purpose partitions:</strong></a></li><li><a href=#-contributed-partitions>* <strong>Contributed partitions:</strong></a></li><li><a href=#-preemptible-partitions>* <strong>Preemptible partitions:</strong></a></li></ul></li><li><a href=#accounts>Accounts</a></li><li><a href=#qoss>QOSs</a></li><li><a href=#limits-and-policies>Limits and policies</a></li><li><a href=#slurm-commands>SLURM commands</a><ul><li><a href=#exploring-the-system>Exploring the system</a></li><li><a href=#submitting-jobs>Submitting jobs</a></li><li><a href=#monitoring-jobs>Monitoring jobs</a></li><li><a href=#getting-info-on-accounts-and-priorities>Getting info on accounts and priorities</a></li></ul></li><li><a href=#useful-links>Useful links</a></li></ul></li></ul></nav></aside></main></body></html>