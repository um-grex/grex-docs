<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes of Grex Changes"><meta property="og:title" content="Notes of Grex Changes"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="http://um-grex.github.io/grex-docs/docs/longread/"><title>Notes of Grex Changes | Unofficial Grex User Guide</title><link rel=icon href=/grex-docs/favicon.png type=image/x-icon><link rel=stylesheet href=/grex-docs/book.min.9e24c4795dd4fdd0e504b497cfd7198ef04d9fc59147fe183e68ca7912ee2901.css integrity="sha256-niTEeV3U/dDlBLSXz9cZjvBNn8WRR/4YPmjKeRLuKQE="><script defer src=/grex-docs/en.search.min.5e2a58bb0cfe70d26350958981a578b6db10d26bdebfeb5943e205de71c3e2e2.js integrity="sha256-XipYuwz+cNJjUJWJgaV4ttsQ0mvev+tZQ+IF3nHD4uI="></script><link rel=alternate type=application/rss+xml href=http://um-grex.github.io/grex-docs/docs/longread/index.xml title="Unofficial Grex User Guide"></head><body><input type=checkbox class=hidden id=menu-control><main class="flex container"><aside class="book-menu fixed"><nav><h2 class=book-brand><a href=http://um-grex.github.io/grex-docs><img src=/grex-docs/um_logo_email_signature.png alt=Logo><span>Unofficial Grex User Guide</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64><div class="book-search-spinner spinner hidden"></div><ul id=book-search-results></ul></div><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/ class=active>Notes of Grex Changes</a><ul><li class=book-section-flat><a href=/grex-docs/docs/longread/training/>Training Materials and Presentations</a><ul></ul></li></ul></li><li class=book-section-flat><a href=/grex-docs/docs/computecanada/>Accessing Compute Canada resources</a><ul></ul></li><li class=book-section-flat><a href=/grex-docs/docs/grex/>Grex HPC Documentation</a><ul><li><a href=/grex-docs/docs/grex/access/>Access and Usage conditions</a></li><li><a href=/grex-docs/docs/grex/connecting/>Connecting / Transferring data</a></li><li><a href=/grex-docs/docs/grex/data/>Storage and Data</a></li><li><a href=/grex-docs/docs/grex/running/>Running Jobs</a></li><li><a href=/grex-docs/docs/grex/software/>Software</a></li></ul></li><li><a href=/grex-docs/docs/faq/>Frequently Asked Questions</a></li><li><a href=/grex-docs/docs/localit/>Local IT Resources</a></li><li><a href=/grex-docs/docs/support-contacts/>Support and Training</a></li><li><a href=/grex-docs/docs/disclaimer/>Disclaimer</a></li></ul></nav><script>(function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></aside><div class=book-page><header class="flex align-center justify-between book-header"><label for=menu-control><img src=/grex-docs/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Notes of Grex Changes</strong></header><article class=markdown><h1 id=grex-changes-related-to-the-linuxslurm-update-project>Grex changes related to the Linux/SLURM update project.</h1><p><strong>December 10-11, 2019</strong></p><h2 id=introduction-motivation>Introduction / Motivation</h2><p><strong>Grex</strong> runs an old version of CentOS 6, which gets unsupported in 2020. The 2.6.x Linux kernel that is shipped with CentOS 6 does not support containerized workloads that require recent kernel features. The Lustre parallel filesystem client had some troubles that we were unable to resolve with CentOS 6 kernel version as well. Finally, the original Grex resource management software, Torque 2.5 and Moab7 are unable to properly schedule jobs that use newer MPI implementations (OpenMPI 2 and 3), which are increasingly common amongst HPC users. Therefore, using the power outages of October and December 2019, we have embarked on a rather ambitious project of updating entire Grex OS and software stack and scheduling to CentOS 7 and SLURM. This document outlines the changes and how they will affect Grex users.</p><h2 id=connecting-to-grex>Connecting to Grex</h2><p>Grex is still using Westgrid accounting system (Portal/LDAP). To connect to Grex, one needs an active Compute Canada account and a Westgrid consortium account linked to it. You are likely to have one.</p><h3 id=hosts-to-connect-to>Hosts to connect to</h3><ol><li><p>During the outage, most of the Grex compute nodes, and all the contributed systems have been reprovisioned to CentOS 7. Public access login nodes, <strong>bison.westgrid.ca</strong> and <strong>tatanka.westgrid.ca</strong> (and their DNS alias, <strong>grex.westgrid.ca</strong> ) give the new CentOS 7.6 / SLURM / LMOD environment.</p></li><li><p>A test node, <strong>aurochs.westgrid.ca</strong> was preserved in the original CentOS 6 state, as well as about 600 cores of the Grex compute nodes. Logging in to aurochs for now allows users access to the original Grex environment (Torque, Moab, Tcl-Modules). We plan to eventually decommission the CentOS 6 partition when the CentOS 7 is debugged and fully in production.</p></li></ol><h3 id=command-line-access-via-ssh-clients>Command line access via SSH clients</h3><p>Because Grex login nodes were reinstalled, SSH clients might give either a warning or an error for not recognizing the Grex host keys. Remove the offending keys from the file <strong>~/.ssh/known_hosts</strong> mentioned in the error message.</p><h3 id=graphical-access-with-x2go>Graphical access with X2go</h3><p>The new CentOS 7 supports X2Go connections to use GUI applications. However, the <strong><del>GNOME Desktop environment</del></strong> that was default in CentOS 6 is no longer available! Please use either <strong>ICEWM</strong> or <strong>OPENBOX</strong> Desktop environment in the X2Go client to connect.</p><p>Because X2Go is using SSH under the hood to establish the connection, the advice above about <strong>~/.ssh/known_hosts</strong> holds: delete the old SSH keys from it if you have connection problems. On Windows, it is often located under C:\Users\<strong>username</strong>\.ssh\known_hosts.</p><h2 id=storage>Storage</h2><p>Lustre storage (/global/scratch) was updated to Lustre 2.10.8 on both server and client sides. We have ran our extensive tests and observed an increase of the write throughputs for large parallel I/O up to 3x. The change should be transparent to Grex users.</p><h2 id=software-interconnect-lmod-cclocal-stacks>Software, Interconnect, LMOD, CC/local stacks</h2><h3 id=interconnect-and-communications-libraries>Interconnect and communications libraries</h3><p>Grex&rsquo;s HPC interconnect hardware is no longer officially supported by commercial MLNX IB Verbs drivers. At the same time, open source projects like RDMA-Core and the new universal interconnect, UCX almost reached maturity and superior performance. Therefore, for the Grex software update, we have opted for vanilla Kernel drivers for our Infiniband, RDMA-Core for verbs userland libraries and UCX for the communication layer for the new OpenMPI versions, of which we support <strong>OpeMPI 3.1.4 (the new default)</strong> and 4.0.2 (An experimental, bleeding edge MPI v3 standard implementation that obsoletes many old MPI features). The previous default version <strong>OpenMPI 1.6.5</strong> is still supported, and still uses IB Verbs from RDMA-core.</p><p>Users that have codes directly linked against any IBVerbs or other low level MLNX libraries, or having fixed RPATH to the old OpenMPI binaries will have to recompile their codes!</p><h3 id=software-modules-lmod>Software Modules: LMOD</h3><p>This is a most major change! We have made obsolete the tried and tested flat, TCL-based software Modules system in favour of Lmod. Lmod is a new software Module system developed by Robert McLay at TACC. The main difference between Lmod and TCL-mod is that Lmod is built to have a hierarchical module structure: it ensures that no modules of the same “kind” can be loaded simultaneously; that there be no deep module paths like <em>“intel/ompi/1.6.5</em>” or “<em>netcdf/pnetcdf-ompi165-nc433</em>” . Rather, users will load “root” modules first and dependent modules second. That is, instead of TCL-mode’s way on the old system (loading OpenMPI for Intel-14 compilers:</p><p><code>module load intel/14.0.2.144</code></p><p><code>module load intel/ompi/1.6.5</code></p><p>The new Lmod way would be:</p><p><code>module load intel/14.0.2.144</code></p><p><code>module load ompi/1.6.5</code></p><p>The hierarchy ensures that only a good <strong>ompi/1.6.5</strong> module that corresponds to the previously loaded Intel-14 compilers gets loaded. Note that swapping the compiler modules (Intel to GCC or Intel 14 to Intel 15) results in automatic reload of the dependent modules, if possible. Loading two versions of the same modules simultaneously is no longer possible! This largely removes the need for “<em>module purge”</em> command.</p><p>In the hierarchical module system, dependent modules are not visible for “<em>module avail</em>” unless their dependencies are loaded. To figure what modules are available use now “<em>module spider</em>” instead.</p><p>For more information and features, visit the official documentation of <a href=https://lmod.readthedocs.io/en/latest/010_user.html>Lmod</a> and/or Compute Canada documentation for <a href=https://docs.computecanada.ca/wiki/Utiliser_des_modules/en>modules</a>.</p><p>We have tried to preserve the module names and paths closest to the original TCL-modules on Grex, whenever that was possible with the new hierarchy format. Note that the hierarchy is not “complete”: not every combination of software, compilers, and MPI exists on Grex, for practical reasons. Use &ldquo;<em>module spider <name of the software>/<version></em>&rdquo; to check what built variants are there. Send us a request to <a href=mailto:support@computecanada.ca>support@computecanada.ca</a> if there is any missing software/toolchain combination you may want to use.</p><h4 id=software-modules-stacks--cvmfs-defaults>Software Modules: Stacks , CVMFS, Defaults</h4><p>One of the nice features of Lmod is its ability to maintain several software stacks at once. We have used it and now provide the following software stacks modules. The modules are “sticky” which means, one of them is always loaded. (Use module avail to see them). <em>GrexEnv</em> (default), <em>OldEnv</em> and <em>CCEnv</em> .</p><ol><li><p><em>GrexEnv</em> is the current Grex default software environment, (mostly) recompiled to CentOS-7. Note that <strong>we do not load Intel compilers and OpenMPI modules by default</strong> anymore, in contrast to the old environment of Grex. The recompilation is mostly done, but not completely: should you miss a software item, please contact us!</p></li><li><p><em>OldEnv</em> is an Lmod-ized version of the old CentOS-6 modules, should you need it for compatibility reasons. The software (except OpenMPI) is as it was before (not recompiled). It <em>may</em> run on CentOS-7.</p></li><li><p><em>CCEnv</em> is the full Compute Canada software stack, brought to Grex via Cern Virtual Filesystem (CVMFS). We use the “sse3” software stack of Compute Canada because this is the one that suits our older hardware. Note that the <strong>default CC environment (<em>StdEnv</em>, <em>nixpkgs</em>) are NOT loaded by default</strong> on Grex! In order to access the CC software, they have to be loaded after the CCEnv as follows. (Note that first load of the CC modules and software items might take a few seconds! It is probably a good practice to first access a CC software binary in a small interactive job to warm the local cache).</p><p><code>module load CCEnv</code></p><p><code>module load StdEnv/2016.4 nixpkgs/16.09 arch/sse3</code></p><p><code>module avail</code></p></li></ol><p>The CC software stack is documented at Compute Canada wiki: <a href=https://docs.computecanada.ca/wiki/Available_software>Available_software</a> page. A caveat: it is in general impossible to isolate and containerize high-performance software completely, so not all CC CVMFS software might work on Grex: the most troublesome parts are MPI and CUDA software that rely on low level hardware drivers and direct memory access. Threaded SMP software and serial software we expect to run without issues.</p><p>Note that for Contributed systems it might be beneficial to use different architecture than the default SSE3, which is available at loading corresponding <em>arch/</em> module.</p><h2 id=running-jobs-migration-to-slurm>Running jobs, migration to SLURM</h2><p>CentOS-7 provides a different method of process isolation (cgroups). It also allows for better support of containers such as Singularity which (hopefully) can now be run in user namespaces. Incidentally, the cgroups are not supported well by any Torque/Moab scheduler version that was compatible with our current Moab software license. Torque is not known to support the new process management interface (PMIX) that increasingly becoming a standard for MPI libraries either. Therefore, we had little choice but to migrate our batch workload management from Torque to SLURM. It wil also make Grex more similar to Compute Canada machines (which are documented here: <a href=https://docs.computecanada.ca/wiki/Running_jobs>Running_jobs</a>).</p><p>Unlike Torque which is a monolithic, well engineered piece of software that we knew well, SLURM is a very modular, plugin-based ecosystem which is new to us. Therefore, <strong>initially we will enable only short walltimes to test our SLURM configuration (48h)</strong> before going to full production.</p><h3 id=torque-wrappers>Torque wrappers</h3><p>To make life easier for PBS/Torque users, SLURM developers provided most of the Torque commands as wrappers over SLURM commands. So with some luck, you can continue using qsub, qstat, pbsnodes etc. keeping in mind correspondence between Torque’s queues and SLURM partitions. For example:</p><p><code>sbatch --allocation=abc-668-aa --partition=compute my.job</code></p><p>Can be called using Torque syntax as:</p><p><code>qsub -A abc-668-aa -q compute my.job</code></p><p>Presently, and unlike old Grex, there is no automatic queue/partition/QOS assignment based on jobs resource request. The option <em>--partition</em> must be selected explicitly for using High Memory and contributed nodes, if you have access to the later. By default, jobs go into “<em>compute</em>” partition comprised of the original 48 GB, 12 Nehalem CPUs nodes. The command _qstat -q _now actually lists partitions (which it thinks being queues).</p><h3 id=interactive-jobs>Interactive Jobs</h3><p>As before, and as usual for HPC systems, we ask users to limit their work on login nodes to code development and small, infrequent test runs and to use interactive jobs for <strong>longer and/or heavier</strong> interactive workloads.</p><p>Interactive jobs can be done using SLURM <em>salloc</em> command as well as using_ qsub -I_ . The one limitation for the later is there for graphical jobs: the latest and greatest SLURM 19.05 supports _&ndash;x11_ flags natively for salloc, but does not support yet corresponding _qsub -I -X_ flags for the Torque wrapper. So graphical interactive jobs are only possible with _salloc_.</p><p>Another limitation of qsub is the syntax: SLURM does distinguish between <em>&ndash;ntasks=</em> for MPI parallel jobs and <em>&ndash;cpus-per-task=</em> for SMP threaded jobs; while for qsub it is all the same for its <em>-l nodes=1:ppn=</em> syntax. Therefore, the SMP threaded applications might not be placed correctly with the jobs submitted with qsub.</p><h3 id=batch-jobs>Batch Jobs.</h3><p>There are two changes: need to specify partitions explicitly and short maximal walltimes during initial Grex testing. Resource allocations for Grex RAC 2019-2020 will be implemented in the SLURM scheduler. Two useful commands: <em>sshare</em> (shows your groups fairshare parameters) and <em>seff</em> (shows efficiency of your jobs: CPU and memory usage) might help with effective usage of your allocation.</p><p>In general, Compute Canada documentation on running SLURM jobs can be followed, obviously with the exception of different core count on Grex nodes. Presently, we schedule on Grex by CPU core (as opposed to by-node) so whole-node jobs do not get any particular prioritization.</p><p>For the local scratch directories, <em>$TMPDIR</em> should be used in batch scripts on Grex (as opposed to Compute Canada where $SLURM_TMPDIR is defined). Thus, for using software from Compute Canada stack that has references to SLURM_TMPDIR hardcoded in some scripts it relies on (an example being GAMESS-US)
the following line should be added on Grex to your job scripts:</p><p><code>export SLURM_TMPDIR=$TMPDIR</code></p></article><div class="book-footer justify-between"><div><a class="flex align-center" href=https://github.com/gshamov/site/commit/f42cd442db68ea019509b0aa26c742457dd13f71 title="Last modified by Shamov | Jun 29, 2021" target=_blank><img src=/grex-docs/svg/calendar.svg class=book-icon alt=Calendar>
<span>Jun 29, 2021</span></a></div><div><a class="flex align-center" href=https://github.com/gshamov/site/blob/master/content/docs/longread/_index.md target=_blank><img src=/grex-docs/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div></div><aside class="book-toc levels-4 fixed"><nav id=TableOfContents><ul><li><a href=#grex-changes-related-to-the-linuxslurm-update-project>Grex changes related to the Linux/SLURM update project.</a><ul><li><a href=#introduction-motivation>Introduction / Motivation</a></li><li><a href=#connecting-to-grex>Connecting to Grex</a><ul><li><a href=#hosts-to-connect-to>Hosts to connect to</a></li><li><a href=#command-line-access-via-ssh-clients>Command line access via SSH clients</a></li><li><a href=#graphical-access-with-x2go>Graphical access with X2go</a></li></ul></li><li><a href=#storage>Storage</a></li><li><a href=#software-interconnect-lmod-cclocal-stacks>Software, Interconnect, LMOD, CC/local stacks</a><ul><li><a href=#interconnect-and-communications-libraries>Interconnect and communications libraries</a></li><li><a href=#software-modules-lmod>Software Modules: LMOD</a></li></ul></li><li><a href=#running-jobs-migration-to-slurm>Running jobs, migration to SLURM</a><ul><li><a href=#torque-wrappers>Torque wrappers</a></li><li><a href=#interactive-jobs>Interactive Jobs</a></li><li><a href=#batch-jobs>Batch Jobs.</a></li></ul></li></ul></li></ul></nav></aside></main></body></html>